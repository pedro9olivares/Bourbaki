{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Supervised Fine-tuning (SFT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "El proceso de entrenamiento de un LLM como ChatGPT, Gemini o Deepseek consiste en, a grandes rasgos, tres fases:\n",
        "\n",
        "- **Pre-entrenamiento**: entrenamiento de predicción del siguiente token (dataset enorme, del orden de toda la web).\n",
        "- **Supervised Fine-tuning (SFT)**: fine-tuning del modelo preentrenado, usando un conjunto de datos de conversaciones. Esto permite que el modelo adquiera la capacidad de seguir instrucciones y de conversar con el usuario.\n",
        "- **Reinforcement Learning with Human Feedback (RLHF)**: se busca alinear al modelo con las preferencias humanas (por ejemplo, no groserías)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En este notebook, crearemos un GPT desde cero (https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf), y haremos las fases de **pre-entrenamiento** y **SFT**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Diferencias entre pre-entrenamiento y SFT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "SFT suena similar al pre-entrenamiento, pero hay diferencias clave:\n",
        "\n",
        "- Plantilla de chat: mientras que en el pre-entrenamiento el modelo predice el siguiente token, en la etapa de SFT el modelo necesita diferenciar quién dijo qué. Por ello, la muestra de entrenamiento debe estar estructurada y contener un delimitador que indique quién es el hablante.\n",
        "\n",
        "- Función de pérdida enmascarada: el SFT solo calcula la pérdida sobre los tokens de la respuesta del asistente, sin considerar el prompt del usuario. Esto permite que el modelo aprenda si la respuesta del asistente es buena o no."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Por ejemplo, OpenAI usa el siguiente template:\n",
        "\n",
        "```\n",
        "<|im_start|>user\n",
        "Hello, how are you?<|im_end|>\n",
        "<|im_start|>assistant\n",
        "I'm fine, thank you!<|im_end|>\n",
        "<|im_start|>user\n",
        "What is the capital of France?<|im_end|>\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "El SFT solo calculará la pérdida sobre los tokens de la respuesta del asistente sin considerar el prompt del usuario."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "(Por ejemplo, se puede usar CrossEntropyLoss de PyTorch. Según la documentación, si el valor de la etiqueta es –100, la pérdida correspondiente a ese token será ignorada)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Construcción de un GPT y pre-entrenamiento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La arquitectura que implementaremos es:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"./imgs/1.png\" width=500>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Siguiendo de cerca al artículo mencionado."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "El objetivo del preentrenamiento es: obtener un modelo que pueda predecir de forma confiable el siguiente token dado los $k$ tokens anteriores de una secuencia.\n",
        "El resultado final del preentrenamiento es un modelo de deep learning que recibe $k$ tokens y produce una distribución de probabilidad discreta sobre cuál debería ser el token $k+1$. Queremos que esta distribución asigne un valor alto al token correcto y valores bajos a los incorrectos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"./imgs/2.png\" width=500>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Empezaremos con un conjunto de datos grande de texto sin procesar. Este texto puede provenir de libros, blogs, wikis, artículos de investigación y otras fuentes. \n",
        "\n",
        "Después de compilar este gran conjunto de datos, lo dividimos en “chunks” de tokens, donde cada chunk contiene una cierta cantidad de tokens (512 en GPT, 1024 en GPT-2, 16 385 en GPT-3). Al tamaño de chunk se le conoce como la “ventana de contexto”.\n",
        "\n",
        "El modelo tomará esa cantidad de tokens y producirá el siguiente token más probable (token por token, no chunk por chunk)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Definición de token:** Unidad más pequeña de texto que un modelo puede analizar. Esto es similar a una búsqueda en un diccionario: cada palabra/token tiene un “índice” numérico en la tabla de búsqueda. Este índice es lo que realmente se introduce en la red para ser analizado."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"./imgs/3.png\" width=500>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Formato de datos:** Cada ejemplo del conjunto de datos de preentrenamiento es un chunk de tokens. El mismo fragmento se usa como entrada y como salida, pero la salida está desplazada 1 token hacia el “futuro”. La razón de esto tiene que ver con las capacidades de procesamiento paralelo del transformer, algo que explicaremos con más detalle en la sección del transformer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"./imgs/4.png\" width=500>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En primera instancia, usaremos un dataset pequeño que cabe en memoria. Usaremos el dataset [Salesforce wikitext](https://huggingface.co/datasets/EleutherAI/wikitext_document_level) que continene un extracto de artículos de Wikipedia.   \n",
        "\n",
        "Cargaremos los datasets desde el [hub de datasets de Hugging Face](https://huggingface.co/docs/datasets/en/load_hub). El paquete datasets de Hugging Face proporciona una manera sencilla de cargar, preprocesar y utilizar una variedad de datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "! pip install --upgrade datasets\n",
        "! pip install tiktoken\n",
        "! pip install transformers\n",
        "! pip install torch\n",
        "! pip install matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import warnings\n",
        "import torch\n",
        "import math\n",
        "import time\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from itertools import cycle\n",
        "from datasets import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoTokenizer\n",
        "from torch.optim.lr_scheduler import _LRScheduler\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"EleutherAI/wikitext_document_level\", \"wikitext-2-raw-v1\", split=\"train\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tokenización y chunking del dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Un enfoque sencillo para tokenizar y dividir el texto en fragmentos es el siguiente:\n",
        "\n",
        "- Concatenar todo el texto en un único “bloque” gigante (cadena de texto muy larga).\n",
        "\n",
        "- Tokenizar todo el bloque en una única lista de tokens (gran arreglo de enteros).\n",
        "\n",
        "- Dividir los tokens en bloques de tamaño fijo (1024, 2048, más grandes…) (“ventana de contexto”) (múltiples arreglos de enteros)\n",
        "\n",
        "Este proceso cambiará ligeramente cuando se usen conjuntos de datos demasiado grandes para caber en memoria.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En nuestro caso, usaremos tiktoken. En particular, es la implementación de OpenAI de un tokenizador para BPE (Byte Pair Encoding)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary size: 50257\n",
            "End of text token: 50256\n",
            "Example tokenization: [15496, 995, 0]\n",
            "Input Shape torch.Size([2, 4])\n",
            "Output Shape torch.Size([2, 4])\n",
            "Input Example:\n",
            "tensor([[   27,  7700,    29,   220],\n",
            "        [  569, 18354,  7496, 17740]], device='cuda:0')\n",
            "Output Example:\n",
            "tensor([[ 7700,    29,   220,   796],\n",
            "        [18354,  7496, 17740,  6711]], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "import tiktoken\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\") # Get the same tokenizer used for GPT-2\n",
        "\n",
        "\n",
        "print(\"Vocabulary size:\", tokenizer.n_vocab) # Vocabulary size is how many unique tokens the tokenizer can encode\n",
        "print(\"End of text token:\", tokenizer.eot_token) # End of text token is used to indicate the end of a text sequence\n",
        "print(\"Example tokenization:\", tokenizer.encode(\"Hello world!\"))\n",
        "\n",
        "# Convert entire dataset into a single string\n",
        "# This dataset is small enough to fit into memory\n",
        "# For larger datasets, you may need to use more \n",
        "# sophisticated methods to process the data.\n",
        "all_text = \"\"\n",
        "all_data = dataset[\"page\"]\n",
        "for example in all_data:\n",
        "    all_text += \"<page> \"+ example + \" </page>\"\n",
        "\n",
        "# Tokenize the entire text at once\n",
        "tokenized_text = tokenizer.encode(all_text)\n",
        "\n",
        "# We will create a function that generates a dataset of examples\n",
        "# for the language model.\n",
        "def get_dataset(num_examples, context_window_length, test_split=0.1):\n",
        "    input_blocks = [] # List to store input sequences\n",
        "    target_blocks = [] # List to store target sequences\n",
        "\n",
        "    # Use a sliding window to create input/target sequences\n",
        "    for i in range(0, len(tokenized_text), context_window_length + 1):\n",
        "        block = tokenized_text[i:i+context_window_length+ 1]\n",
        "        \n",
        "        # Skip blocks that are too short\n",
        "        if len(block) < context_window_length + 1:\n",
        "            continue\n",
        "\n",
        "        input_seq = block[:-1]  \n",
        "        target_seq = block[1:]  \n",
        "\n",
        "        input_blocks.append(input_seq)\n",
        "        target_blocks.append(target_seq)\n",
        "        \n",
        "        # Stop if we have enough examples\n",
        "        if len(input_blocks) >= num_examples:\n",
        "            break\n",
        "\n",
        "    # Convert to tensors for pytorch and move to gpu\n",
        "    inputs = torch.tensor(input_blocks, dtype=torch.long).to(device)\n",
        "    targets = torch.tensor(target_blocks, dtype=torch.long).to(device)\n",
        "\n",
        "    # Calculate train/test split point\n",
        "    split_idx = int(num_examples * (1 - test_split))\n",
        "\n",
        "    # Split into train/test\n",
        "    train_inputs = inputs[:split_idx]\n",
        "    train_targets = targets[:split_idx]\n",
        "    test_inputs = inputs[split_idx:]\n",
        "    test_targets = targets[split_idx:]\n",
        "    return train_inputs, train_targets, test_inputs, test_targets\n",
        "\n",
        "# Get a small dataset\n",
        "i, o, _, _ = get_dataset(2, 4, 0)\n",
        "print(\"Input Shape\", i.shape)\n",
        "print(\"Output Shape\", o.shape)\n",
        "print(\"Input Example:\")\n",
        "print(i)\n",
        "print(\"Output Example:\")\n",
        "print(o)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Construcción del LLM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Primero, construiremos un objeto de “configuración” que almacenará nuestros parámetros para la red. Más adelante revisaremos cada parámetro en detalle dentro de la red."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# A simple configuration container\n",
        "class GPTConfig:\n",
        "    def __init__(\n",
        "        self, \n",
        "        vocab_size,  # size of the vocabulary, from tokenizer, for gpt2 tokenizer it is 50257\n",
        "        n_layer,   # number of transformer blocks\n",
        "        n_head,    # number of attention heads for each transformer block\n",
        "        n_embd,  # embedding dimension for each token\n",
        "        seq_len,  # sequence length for the model - e.g. the \"context window\" \n",
        "    \n",
        "    ):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.n_layer = n_layer\n",
        "        self.n_head = n_head\n",
        "        self.n_embd = n_embd\n",
        "        self.seq_len = seq_len\n",
        "     \n",
        "test_config = GPTConfig(\n",
        "    vocab_size=tokenizer.n_vocab,\n",
        "    n_layer=2,  \n",
        "    n_head=3,\n",
        "    n_embd=6,\n",
        "    seq_len=4,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ahora sí, vayamos bloque por bloque."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Token Embedding Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"./imgs/5.png\" width=500>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Es esencialmente una tabla de búsqueda que devuelve un “vector de embedding” para un índice entero dado. \n",
        "\n",
        "El objetivo de esta capa es convertir tokens en vectores. Estos vectores se tunean a medida que la red se entrena, de modo que su posición en el espacio, en relación con los demás vectores, refleja las relaciones estadísticas entre los tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Después de esta capa, el modelo logra una comprensión semántica de los tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para nuestro dataset \"dummy\", la entrada a esta capa será una matriz de tamaño 2×4.\n",
        "La salida será 2×4×6."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"./imgs/6.png\" width=500>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch shape: torch.Size([2, 4]) Batch x Seq Len\n",
            "After embedding: torch.Size([2, 4, 6]) Batch x Seq Len x Embedding Dim\n",
            "\n",
            "Before embedding\n",
            "tensor([[   27,  7700,    29,   220],\n",
            "        [  569, 18354,  7496, 17740]], device='cuda:0')\n",
            "After embedding\n",
            "tensor([[[-0.0850, -0.5393, -0.4814, -0.5201,  0.8528,  1.5614],\n",
            "         [ 1.7124,  0.9409,  0.5425, -0.2307,  1.0325,  0.6966],\n",
            "         [ 1.8344, -0.6808,  0.1047,  0.9956,  0.6336,  0.1757],\n",
            "         [-0.3960, -1.9960, -0.7287,  0.4127,  0.1946, -0.3331]],\n",
            "\n",
            "        [[ 2.4173, -1.0166,  0.2398, -0.3766, -0.5538,  0.2580],\n",
            "         [-1.2367, -0.7099, -0.0859, -0.8818, -0.9876,  0.7716],\n",
            "         [ 1.1070, -0.5770,  0.2477,  2.5843,  0.5314,  1.3516],\n",
            "         [ 1.0932, -0.0932, -1.0295,  0.9911, -0.2331,  2.5532]]],\n",
            "       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ],
      "source": [
        "token_embedding = nn.Embedding(test_config.vocab_size, test_config.n_embd).to(device)\n",
        "test_batch_inputs, _, _, _ = get_dataset(2, test_config.seq_len, 0)\n",
        "print(\"Batch shape:\", test_batch_inputs.shape, \"Batch x Seq Len\")\n",
        "print(\"After embedding:\", token_embedding(test_batch_inputs).shape, \"Batch x Seq Len x Embedding Dim\")\n",
        "print(\"\")\n",
        "print(\"Before embedding\")\n",
        "print(test_batch_inputs)\n",
        "print(\"After embedding\")\n",
        "print(token_embedding(test_batch_inputs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "(En este momento, la elección de dimensión=6 es arbitraria, y los vectores no tienen sentido sino hasta el entrenamiento de la red)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Positional Encoding Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"./imgs/7.png\" width=500>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A cada vector le añadiremos un encoding posicional. ¿Por qué?\n",
        "\n",
        "Considera: \"El planeta es más chico que el otro planeta\".\n",
        "\n",
        "El encoding posicional le permitirá al modelos distinguir entre los dos planetas de la oración."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La fórmula a usar, definida en el paper donde se introduce a los transformers, es la siguiente:\n",
        "\n",
        "Toma una matriz $PE$ de dimensiones (sequence_length) x (dimensión del embedding $d$). Rellenar de acuerdo a:\n",
        "\n",
        "$$PE(POS,2i) = sin(\\frac{pos}{10000^\\frac{2i}{d}})$$\n",
        "$$PE(POS,2i+1) = cos(\\frac{pos}{10000^\\frac{2i}{d}})$$\n",
        "\n",
        "Done $POS$ es la posición del token en la secuencia, e $i$ es el índice de la dimensión de embedding del token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Position encoding shape: torch.Size([1, 4, 6])\n"
          ]
        }
      ],
      "source": [
        "def get_position_encoding(seq_len, d, n=10000):\n",
        "    \"\"\"\n",
        "    Computes the positional encoding matrix of shape (seq_len, d).\n",
        "    \n",
        "    Args:\n",
        "        seq_len (int): Length of the sequence.\n",
        "        d (int): Dimension of the embedding.\n",
        "        n (float): The base for the exponential term (default 10000 in many Transformer implementations).\n",
        "    \n",
        "    Returns:\n",
        "        torch.Tensor: A tensor of shape (seq_len, d) containing the positional encodings.\n",
        "    \"\"\"\n",
        "    \n",
        "    P = torch.zeros(seq_len, d).to(device)\n",
        "    for pos in range(seq_len):\n",
        "        for i in range(0, d // 2):\n",
        "            P[pos, 2 * i] = math.sin(pos / (n ** ((2 * i) / d)))\n",
        "            if i + 1 < d:\n",
        "                P[pos, 2* i + 1] = math.cos(pos / (n ** ((2 * i) / d)))\n",
        "\n",
        "    return P.unsqueeze(0)\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "position_encoding = get_position_encoding(seq_len=test_config.seq_len, d=test_config.n_embd)\n",
        "print(\"Position encoding shape:\", position_encoding.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Una vez que tenemos esa matriz, la sumamos (por elemento) a los vectores de embedding (vía broadcast). Esto significa que la matriz de encoding posicional se añadirá a cada ejemplo/observación en paralelo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"./imgs/8.png\" width=500>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token embeddings shape: torch.Size([2, 4, 6])\n",
            "Position encodings shape: torch.Size([1, 4, 6])\n",
            "Sum of token embeddings and position encodings: torch.Size([2, 4, 6])\n"
          ]
        }
      ],
      "source": [
        "test_embeddings = token_embedding(test_batch_inputs)\n",
        "test_embeddings_with_pos = test_embeddings + position_encoding\n",
        "print(\"Token embeddings shape:\", test_embeddings.shape)\n",
        "print(\"Position encodings shape:\", position_encoding.shape)\n",
        "print(\"Sum of token embeddings and position encodings:\",test_embeddings_with_pos.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Masked Multiheaded Self Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"./imgs/9.png\" width=500>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Llegamos al core del LLM, la parte decoder del transformer. El primer paso del transformer es masked multiheaded self attention, que consiste de tres partes:\n",
        "- auto-atención (cada token puede influenciar a todos los demás tokens, útil para entender contexto)\n",
        "- masking\n",
        "- cabezas múltiples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attention input shape: torch.Size([2, 4, 6])\n",
            "\n",
            "Query weights shape: torch.Size([6, 6])\n",
            "Key weights shape: torch.Size([6, 6])\n",
            "Value weights shape: torch.Size([6, 6])\n",
            "\n",
            "Queries shape: torch.Size([2, 4, 6])\n",
            "Keys shape: torch.Size([2, 4, 6])\n",
            "Values shape: torch.Size([2, 4, 6])\n",
            "\n",
            "QK^T shape: torch.Size([2, 4, 4])\n",
            "\n",
            "Attention output shape: torch.Size([2, 4, 6])\n"
          ]
        }
      ],
      "source": [
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.Wq = nn.Parameter(torch.randn(config.n_embd, config.n_embd)).to(device) # Query weights - will transform input embeddings into queries\n",
        "        self.Wk = nn.Parameter(torch.randn(config.n_embd, config.n_embd)).to(device) # Key weights - will transform input embeddings into keys\n",
        "        self.Wv = nn.Parameter(torch.randn(config.n_embd, config.n_embd)).to(device) # Value weights - will transform input embeddings into values\n",
        "\n",
        "    def forward(self, x):\n",
        "        print(\"Attention input shape:\", x.shape)\n",
        "        print(\"\")\n",
        "        print(\"Query weights shape:\", self.Wq.shape)\n",
        "        print(\"Key weights shape:\", self.Wk.shape)\n",
        "        print(\"Value weights shape:\", self.Wv.shape)\n",
        "        queries = x @ self.Wq # Matrix multiplication to transform input embeddings into queries\n",
        "        keys = x @ self.Wk # Matrix multiplication to transform input embeddings into keys\n",
        "        values = x @ self.Wv # Matrix multiplication to transform input embeddings into values\n",
        "        print(\"\")\n",
        "        print(\"Queries shape:\", queries.shape)\n",
        "        print(\"Keys shape:\", keys.shape)\n",
        "        print(\"Values shape:\", values.shape)\n",
        "\n",
        "        qkt = queries @ keys.transpose(-2, -1) # Calculate QK^T\n",
        "        qkt_scaled = qkt / math.sqrt(queries.size(-1)) # Scale QK^T by the dimension of the keys\n",
        "        qkt_softmax = F.softmax(qkt_scaled, dim=-1) # Apply softmax row-wise to get attention weights\n",
        "        print(\"\")\n",
        "        print(\"QK^T shape:\", qkt.shape)\n",
        "\n",
        "        attn_output = qkt_softmax @ values # Multiply softmax(QK^T) by values\n",
        "        print(\"\")\n",
        "        print(\"Attention output shape:\", attn_output.shape)\n",
        "        return attn_output \n",
        "\n",
        "attention = SelfAttention(test_config)\n",
        "test_out = attention(test_embeddings_with_pos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La versión de atención a usar es de hecho la causal, la cual esconde tokens futuros de las capas de auto-atención para evitar que el modelo “vea” información que aún no ha sido generada y así mantener la naturaleza autoregresiva (predicción solo con elementos anteriores) del modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2, 4, 6])\n"
          ]
        }
      ],
      "source": [
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.Wq = nn.Parameter(torch.randn(config.n_embd, config.n_embd)).to(device) # Query weights - will transform input embeddings into queries\n",
        "        self.Wk = nn.Parameter(torch.randn(config.n_embd, config.n_embd)).to(device) # Key weights - will transform input embeddings into keys\n",
        "        self.Wv = nn.Parameter(torch.randn(config.n_embd, config.n_embd)).to(device) # Value weights - will transform input embeddings into values\n",
        "\n",
        "    def forward(self, x):\n",
        "        seq_len = x.shape[1] # Get sequence length (number of tokens / context window length)\n",
        "        queries = x @ self.Wq # Matrix multiplication to transform input embeddings into queries\n",
        "        keys = x @ self.Wk    # Matrix multiplication to transform input embeddings into keys\n",
        "        values = x @ self.Wv  # Matrix multiplication to transform input embeddings into values\n",
        "        qkt = queries @ keys.transpose(-2, -1)  # Calculate QK^T\n",
        "        qkt_scaled = qkt / math.sqrt(queries.size(-1))  # Scale QK^T by the dimension of the keys\n",
        "\n",
        "        # MASKING\n",
        "        # THIS IS THE ONLY DIFFERENCE, USE -inf FOR UPPER TRIANGLE MASK SO THAT SOFTMAX WILL BE 0\n",
        "        causal_mask = torch.triu(torch.ones(seq_len, seq_len, device=x.device), diagonal=1)\n",
        "        causal_mask = causal_mask.masked_fill(causal_mask == 1, float('-inf'))  # Upper triangle masked with -inf \n",
        "        qkt_scaled = qkt_scaled + causal_mask # Add the mask to the scaled QK^T\n",
        "        # END MASKING\n",
        "\n",
        "        qkt_softmax = F.softmax(qkt_scaled, dim=-1) # Apply softmax row-wise to get attention weights, the -inf values will become 0 here\n",
        "        attn_output = qkt_softmax @ values # Multiply softmax(QK^T) by values\n",
        "        return attn_output\n",
        "\n",
        "\n",
        "attention = CausalSelfAttention(test_config)\n",
        "test_out = attention(test_embeddings_with_pos)\n",
        "print(test_out.shape)  # Output should have shape: (batch_size, seq_len, n_embd)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finalemente, logramos las cabezas múltiples concatenando operaciones CausalAttention en paralelo. Al final de ellas, añadimos una capa para proyectar el output al siguiente tamaño de input."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tener múltiples capas permite que una se enfoque en estructura gramatical, otra en semántica, y otra en significados de palabra, por ejemplo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2, 4, 6])\n"
          ]
        }
      ],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.attn_heads = nn.ModuleList([\n",
        "            CausalSelfAttention(config) for _ in range(config.n_head)\n",
        "        ])  # Create n_head attention heads\n",
        "        self.projection = nn.Linear(config.n_embd * config.n_head, config.n_embd).to(device) # Linear layer to project multi-head attention outputs\n",
        "\n",
        "    def forward(self, x):\n",
        "        head_outputs = [head(x) for head in self.attn_heads] # Get the output of each attention head\n",
        "        multihead_output = torch.cat(head_outputs, dim=-1) # Concatenate the outputs\n",
        "        return self.projection(multihead_output) # Project the concatenated outputs\n",
        "\n",
        "multihead_attn = MultiHeadAttention(test_config)\n",
        "test_out = multihead_attn(test_embeddings_with_pos)\n",
        "print(test_out.shape)  # Output should have shape: (batch_size, seq_len, n_embd)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Acabando el bloque GPT (normalizations y feed forward)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Acabemos el bloque GPT que le pasaremos al modelo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"./imgs/10.png\" width=500>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Normalization layers**: Normaliza los valores de la matriz de entrada a lo largo de la dimensión de features (en nuestro caso, la dimensión 2). Se utiliza para estabilizar el entrenamiento y lograr una convergencia más rápida."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Feed forward layer**: Mientras que la atención captura las relaciones entre tokens, la capa feedforward aplica la misma transformación a cada token en paralelo. Puede implementarse usando capas lineales estándar de PyTorch.\n",
        "\n",
        "Estamos usando un factor de 4x la dimensión del embedding para el tamaño de la capa lineal, tal como se hizo en el artículo original de Attention Is All You Need.\n",
        "\n",
        "Utilizamos la función de activación GELU (Gaussian Error Linear Unit), tal como se implementa en el artículo original de GPT."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2, 4, 6])\n"
          ]
        }
      ],
      "source": [
        "class GPTBlock(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.mha = MultiHeadAttention(config)\n",
        "        self.ln1 = nn.LayerNorm(config.n_embd).to(device)\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(config.n_embd, 4 * config.n_embd),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(4 * config.n_embd, config.n_embd),\n",
        "        ).to(device)\n",
        "        self.ln2 = nn.LayerNorm(config.n_embd).to(device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.mha(self.ln1(x))\n",
        "        x = x + self.ffn(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "block = GPTBlock(test_config)\n",
        "test_out = block(test_embeddings_with_pos)\n",
        "print(test_out.shape)  # Output should have shape: (batch_size, seq_len, n_embd)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Encadenando o enchufando todo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2, 4])\n",
            "torch.Size([2, 4, 50257])\n"
          ]
        }
      ],
      "source": [
        "class GPTModel(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.token_embedding = nn.Embedding(config.vocab_size, config.n_embd).to(device)\n",
        "        self.position_encoding = get_position_encoding(config.seq_len, config.n_embd)\n",
        "        self.blocks = nn.Sequential(*[GPTBlock(config) for _ in range(config.n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(config.n_embd).to(device)\n",
        "        self.head = nn.Linear(config.n_embd, config.vocab_size).to(device)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.token_embedding(x) + self.position_encoding\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        return self.head(x)\n",
        "    \n",
        "gpt = GPTModel(test_config)\n",
        "print(test_batch_inputs.shape)\n",
        "test_out = gpt(test_batch_inputs)\n",
        "print(test_out.shape)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Así, un paso forward completo a través del LLM utiliza un input de tamaño $[batch,tokens]$, y la salida es de tamaño $[batch,tokens,probabilities]$. Para cada token dado en el input, se predicirá una distribución discreta sobre el siguiente token."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"./imgs/11.png\" width=500>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Pre-entrenamiento dummy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La función objetivo a utilizar es:\n",
        "\n",
        "$$L1(U) = \\sum_{i}logP(u_i|u_{i-k}...u_{i-1};\\theta)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- $U$ es la secuencia de tokens\n",
        "- $u_i$ es el current token\n",
        "- $u_{i-k}...u_{i-1}$ son los tokens previos\n",
        "\n",
        "Notar que tomamos la suma a través de todos los tokens en la secuencia.\n",
        "Queremos maximizar la probabilidad de predicción del siguiente token, dados $k$ anteriores y los parámetros entrenados $\\theta$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Esto es equivalente a minimizar la función de pérdida cross entropy:\n",
        "\n",
        "$$H(p, q) = -\\sum_{x} p(x) \\log q(x)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para probar nuestra arquitectura, entrenaremos el modelo con un dataset (10 ejemplos) y veremos si podemos lograr que el modelo overfitteé."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 2/1000, Loss: 10.967658996582031, LR: 0.0005\n",
            "Step 3/1000, Loss: 10.85688591003418, LR: 0.0005\n",
            "Step 4/1000, Loss: 10.777162551879883, LR: 0.0005\n",
            "Step 5/1000, Loss: 10.668031692504883, LR: 0.0005\n",
            "Step 6/1000, Loss: 10.585840225219727, LR: 0.0005\n",
            "Step 7/1000, Loss: 10.469751358032227, LR: 0.0005\n",
            "Step 8/1000, Loss: 10.38199234008789, LR: 0.0005\n",
            "Step 9/1000, Loss: 10.281015396118164, LR: 0.0005\n",
            "Step 10/1000, Loss: 10.190533638000488, LR: 0.0005\n",
            "Step 11/1000, Loss: 10.084761619567871, LR: 0.0005\n",
            "Step 12/1000, Loss: 9.972481727600098, LR: 0.0005\n",
            "Step 13/1000, Loss: 9.863290786743164, LR: 0.0005\n",
            "Step 14/1000, Loss: 9.741707801818848, LR: 0.0005\n",
            "Step 15/1000, Loss: 9.62871265411377, LR: 0.0005\n",
            "Step 16/1000, Loss: 9.505171775817871, LR: 0.0005\n",
            "Step 17/1000, Loss: 9.385248184204102, LR: 0.0005\n",
            "Step 18/1000, Loss: 9.266453742980957, LR: 0.0005\n",
            "Step 19/1000, Loss: 9.146824836730957, LR: 0.0005\n",
            "Step 20/1000, Loss: 9.02399730682373, LR: 0.0005\n",
            "Step 21/1000, Loss: 8.89885425567627, LR: 0.0005\n",
            "Step 22/1000, Loss: 8.774663925170898, LR: 0.0005\n",
            "Step 23/1000, Loss: 8.652946472167969, LR: 0.0005\n",
            "Step 24/1000, Loss: 8.5274076461792, LR: 0.0005\n",
            "Step 25/1000, Loss: 8.396858215332031, LR: 0.0005\n",
            "Step 26/1000, Loss: 8.264533996582031, LR: 0.0005\n",
            "Step 27/1000, Loss: 8.134319305419922, LR: 0.0005\n",
            "Step 28/1000, Loss: 8.000314712524414, LR: 0.0005\n",
            "Step 29/1000, Loss: 7.867947578430176, LR: 0.0005\n",
            "Step 30/1000, Loss: 7.731581687927246, LR: 0.0005\n",
            "Step 31/1000, Loss: 7.597165107727051, LR: 0.0005\n",
            "Step 32/1000, Loss: 7.463318824768066, LR: 0.0005\n",
            "Step 33/1000, Loss: 7.329626560211182, LR: 0.0005\n",
            "Step 34/1000, Loss: 7.201971530914307, LR: 0.0005\n",
            "Step 35/1000, Loss: 7.082633972167969, LR: 0.0005\n",
            "Step 36/1000, Loss: 6.9651360511779785, LR: 0.0005\n",
            "Step 37/1000, Loss: 6.861398220062256, LR: 0.0005\n",
            "Step 38/1000, Loss: 6.753509521484375, LR: 0.0005\n",
            "Step 39/1000, Loss: 6.652728080749512, LR: 0.0005\n",
            "Step 40/1000, Loss: 6.553241729736328, LR: 0.0005\n",
            "Step 41/1000, Loss: 6.451900482177734, LR: 0.0005\n",
            "Step 42/1000, Loss: 6.362810134887695, LR: 0.0005\n",
            "Step 43/1000, Loss: 6.274676322937012, LR: 0.0005\n",
            "Step 44/1000, Loss: 6.194480895996094, LR: 0.0005\n",
            "Step 45/1000, Loss: 6.11739444732666, LR: 0.0005\n",
            "Step 46/1000, Loss: 6.045235633850098, LR: 0.0005\n",
            "Step 47/1000, Loss: 5.982949256896973, LR: 0.0005\n",
            "Step 48/1000, Loss: 5.9209208488464355, LR: 0.0005\n",
            "Step 49/1000, Loss: 5.85759973526001, LR: 0.0005\n",
            "Step 50/1000, Loss: 5.795312404632568, LR: 0.0005\n",
            "Step 51/1000, Loss: 5.7420735359191895, LR: 0.0005\n",
            "Step 52/1000, Loss: 5.694904804229736, LR: 0.0005\n",
            "Step 53/1000, Loss: 5.653714179992676, LR: 0.0005\n",
            "Step 54/1000, Loss: 5.607203960418701, LR: 0.0005\n",
            "Step 55/1000, Loss: 5.57025146484375, LR: 0.0005\n",
            "Step 56/1000, Loss: 5.534480094909668, LR: 0.0005\n",
            "Step 57/1000, Loss: 5.504580020904541, LR: 0.0005\n",
            "Step 58/1000, Loss: 5.468216419219971, LR: 0.0005\n",
            "Step 59/1000, Loss: 5.444606781005859, LR: 0.0005\n",
            "Step 60/1000, Loss: 5.419281959533691, LR: 0.0005\n",
            "Step 61/1000, Loss: 5.392452239990234, LR: 0.0005\n",
            "Step 62/1000, Loss: 5.369950771331787, LR: 0.0005\n",
            "Step 63/1000, Loss: 5.3429460525512695, LR: 0.0005\n",
            "Step 64/1000, Loss: 5.3212995529174805, LR: 0.0005\n",
            "Step 65/1000, Loss: 5.293933868408203, LR: 0.0005\n",
            "Step 66/1000, Loss: 5.275791645050049, LR: 0.0005\n",
            "Step 67/1000, Loss: 5.260615348815918, LR: 0.0005\n",
            "Step 68/1000, Loss: 5.245963096618652, LR: 0.0005\n",
            "Step 69/1000, Loss: 5.220090866088867, LR: 0.0005\n",
            "Step 70/1000, Loss: 5.206206798553467, LR: 0.0005\n",
            "Step 71/1000, Loss: 5.186387062072754, LR: 0.0005\n",
            "Step 72/1000, Loss: 5.171862602233887, LR: 0.0005\n",
            "Step 73/1000, Loss: 5.148059844970703, LR: 0.0005\n",
            "Step 74/1000, Loss: 5.134549140930176, LR: 0.0005\n",
            "Step 75/1000, Loss: 5.1251044273376465, LR: 0.0005\n",
            "Step 76/1000, Loss: 5.1150689125061035, LR: 0.0005\n",
            "Step 77/1000, Loss: 5.105809211730957, LR: 0.0005\n",
            "Step 78/1000, Loss: 5.1017584800720215, LR: 0.0005\n",
            "Step 79/1000, Loss: 5.089953899383545, LR: 0.0005\n",
            "Step 80/1000, Loss: 5.077761650085449, LR: 0.0005\n",
            "Step 81/1000, Loss: 5.057864189147949, LR: 0.0005\n",
            "Step 82/1000, Loss: 5.038480758666992, LR: 0.0005\n",
            "Step 83/1000, Loss: 5.024689674377441, LR: 0.0005\n",
            "Step 84/1000, Loss: 5.013391971588135, LR: 0.0005\n",
            "Step 85/1000, Loss: 4.997678756713867, LR: 0.0005\n",
            "Step 86/1000, Loss: 4.973568916320801, LR: 0.0005\n",
            "Step 87/1000, Loss: 4.944432258605957, LR: 0.0005\n",
            "Step 88/1000, Loss: 4.932539463043213, LR: 0.0005\n",
            "Step 89/1000, Loss: 4.919715881347656, LR: 0.0005\n",
            "Step 90/1000, Loss: 4.915416240692139, LR: 0.0005\n",
            "Step 91/1000, Loss: 4.900571346282959, LR: 0.0005\n",
            "Step 92/1000, Loss: 4.879560470581055, LR: 0.0005\n",
            "Step 93/1000, Loss: 4.85857629776001, LR: 0.0005\n",
            "Step 94/1000, Loss: 4.845907211303711, LR: 0.0005\n",
            "Step 95/1000, Loss: 4.834258079528809, LR: 0.0005\n",
            "Step 96/1000, Loss: 4.812500953674316, LR: 0.0005\n",
            "Step 97/1000, Loss: 4.8053364753723145, LR: 0.0005\n",
            "Step 98/1000, Loss: 4.794131278991699, LR: 0.0005\n",
            "Step 99/1000, Loss: 4.783999443054199, LR: 0.0005\n",
            "Step 100/1000, Loss: 4.7750325202941895, LR: 0.0005\n",
            "Step 101/1000, Loss: 4.771496772766113, LR: 0.0005\n",
            "Step 102/1000, Loss: 4.754128456115723, LR: 0.0005\n",
            "Step 103/1000, Loss: 4.728689193725586, LR: 0.0005\n",
            "Step 104/1000, Loss: 4.717083930969238, LR: 0.0005\n",
            "Step 105/1000, Loss: 4.697932720184326, LR: 0.0005\n",
            "Step 106/1000, Loss: 4.679559230804443, LR: 0.0005\n",
            "Step 107/1000, Loss: 4.66640043258667, LR: 0.0005\n",
            "Step 108/1000, Loss: 4.658744812011719, LR: 0.0005\n",
            "Step 109/1000, Loss: 4.659064292907715, LR: 0.0005\n",
            "Step 110/1000, Loss: 4.637427806854248, LR: 0.0005\n",
            "Step 111/1000, Loss: 4.627077102661133, LR: 0.0005\n",
            "Step 112/1000, Loss: 4.599566459655762, LR: 0.0005\n",
            "Step 113/1000, Loss: 4.601081848144531, LR: 0.0005\n",
            "Step 114/1000, Loss: 4.583940505981445, LR: 0.0005\n",
            "Step 115/1000, Loss: 4.585203647613525, LR: 0.0005\n",
            "Step 116/1000, Loss: 4.568021774291992, LR: 0.0005\n",
            "Step 117/1000, Loss: 4.557181358337402, LR: 0.0005\n",
            "Step 118/1000, Loss: 4.54134464263916, LR: 0.0005\n",
            "Step 119/1000, Loss: 4.526434898376465, LR: 0.0005\n",
            "Step 120/1000, Loss: 4.512308597564697, LR: 0.0005\n",
            "Step 121/1000, Loss: 4.492142200469971, LR: 0.0005\n",
            "Step 122/1000, Loss: 4.48798131942749, LR: 0.0005\n",
            "Step 123/1000, Loss: 4.4805498123168945, LR: 0.0005\n",
            "Step 124/1000, Loss: 4.468844413757324, LR: 0.0005\n",
            "Step 125/1000, Loss: 4.462951183319092, LR: 0.0005\n",
            "Step 126/1000, Loss: 4.434421539306641, LR: 0.0005\n",
            "Step 127/1000, Loss: 4.417495250701904, LR: 0.0005\n",
            "Step 128/1000, Loss: 4.407522201538086, LR: 0.0005\n",
            "Step 129/1000, Loss: 4.401559352874756, LR: 0.0005\n",
            "Step 130/1000, Loss: 4.394003868103027, LR: 0.0005\n",
            "Step 131/1000, Loss: 4.384463310241699, LR: 0.0005\n",
            "Step 132/1000, Loss: 4.374063968658447, LR: 0.0005\n",
            "Step 133/1000, Loss: 4.352975368499756, LR: 0.0005\n",
            "Step 134/1000, Loss: 4.345880031585693, LR: 0.0005\n",
            "Step 135/1000, Loss: 4.329431056976318, LR: 0.0005\n",
            "Step 136/1000, Loss: 4.308016777038574, LR: 0.0005\n",
            "Step 137/1000, Loss: 4.3004655838012695, LR: 0.0005\n",
            "Step 138/1000, Loss: 4.298262119293213, LR: 0.0005\n",
            "Step 139/1000, Loss: 4.285660743713379, LR: 0.0005\n",
            "Step 140/1000, Loss: 4.274592399597168, LR: 0.0005\n",
            "Step 141/1000, Loss: 4.2520365715026855, LR: 0.0005\n",
            "Step 142/1000, Loss: 4.234604835510254, LR: 0.0005\n",
            "Step 143/1000, Loss: 4.204150199890137, LR: 0.0005\n",
            "Step 144/1000, Loss: 4.194119453430176, LR: 0.0005\n",
            "Step 145/1000, Loss: 4.173285961151123, LR: 0.0005\n",
            "Step 146/1000, Loss: 4.143033027648926, LR: 0.0005\n",
            "Step 147/1000, Loss: 4.14162015914917, LR: 0.0005\n",
            "Step 148/1000, Loss: 4.127053260803223, LR: 0.0005\n",
            "Step 149/1000, Loss: 4.101595401763916, LR: 0.0005\n",
            "Step 150/1000, Loss: 4.077259063720703, LR: 0.0005\n",
            "Step 151/1000, Loss: 4.075615882873535, LR: 0.0005\n",
            "Step 152/1000, Loss: 4.037398338317871, LR: 0.0005\n",
            "Step 153/1000, Loss: 4.00080680847168, LR: 0.0005\n",
            "Step 154/1000, Loss: 4.002491474151611, LR: 0.0005\n",
            "Step 155/1000, Loss: 3.9868216514587402, LR: 0.0005\n",
            "Step 156/1000, Loss: 3.9425320625305176, LR: 0.0005\n",
            "Step 157/1000, Loss: 3.9193999767303467, LR: 0.0005\n",
            "Step 158/1000, Loss: 3.910961151123047, LR: 0.0005\n",
            "Step 159/1000, Loss: 3.8992466926574707, LR: 0.0005\n",
            "Step 160/1000, Loss: 3.8716888427734375, LR: 0.0005\n",
            "Step 161/1000, Loss: 3.8629164695739746, LR: 0.0005\n",
            "Step 162/1000, Loss: 3.8181827068328857, LR: 0.0005\n",
            "Step 163/1000, Loss: 3.8121097087860107, LR: 0.0005\n",
            "Step 164/1000, Loss: 3.815507173538208, LR: 0.0005\n",
            "Step 165/1000, Loss: 3.786884307861328, LR: 0.0005\n",
            "Step 166/1000, Loss: 3.7560245990753174, LR: 0.0005\n",
            "Step 167/1000, Loss: 3.725088596343994, LR: 0.0005\n",
            "Step 168/1000, Loss: 3.7211036682128906, LR: 0.0005\n",
            "Step 169/1000, Loss: 3.6878623962402344, LR: 0.0005\n",
            "Step 170/1000, Loss: 3.6708500385284424, LR: 0.0005\n",
            "Step 171/1000, Loss: 3.653564929962158, LR: 0.0005\n",
            "Step 172/1000, Loss: 3.6156325340270996, LR: 0.0005\n",
            "Step 173/1000, Loss: 3.586336851119995, LR: 0.0005\n",
            "Step 174/1000, Loss: 3.5640997886657715, LR: 0.0005\n",
            "Step 175/1000, Loss: 3.5465335845947266, LR: 0.0005\n",
            "Step 176/1000, Loss: 3.511826992034912, LR: 0.0005\n",
            "Step 177/1000, Loss: 3.4743785858154297, LR: 0.0005\n",
            "Step 178/1000, Loss: 3.4601149559020996, LR: 0.0005\n",
            "Step 179/1000, Loss: 3.433750629425049, LR: 0.0005\n",
            "Step 180/1000, Loss: 3.4152112007141113, LR: 0.0005\n",
            "Step 181/1000, Loss: 3.403822660446167, LR: 0.0005\n",
            "Step 182/1000, Loss: 3.4256794452667236, LR: 0.0005\n",
            "Step 183/1000, Loss: 3.3998799324035645, LR: 0.0005\n",
            "Step 184/1000, Loss: 3.3784213066101074, LR: 0.0005\n",
            "Step 185/1000, Loss: 3.344134569168091, LR: 0.0005\n",
            "Step 186/1000, Loss: 3.310579776763916, LR: 0.0005\n",
            "Step 187/1000, Loss: 3.2851688861846924, LR: 0.0005\n",
            "Step 188/1000, Loss: 3.275599241256714, LR: 0.0005\n",
            "Step 189/1000, Loss: 3.2638020515441895, LR: 0.0005\n",
            "Step 190/1000, Loss: 3.2565078735351562, LR: 0.0005\n",
            "Step 191/1000, Loss: 3.2324013710021973, LR: 0.0005\n",
            "Step 192/1000, Loss: 3.202582836151123, LR: 0.0005\n",
            "Step 193/1000, Loss: 3.1878724098205566, LR: 0.0005\n",
            "Step 194/1000, Loss: 3.191157102584839, LR: 0.0005\n",
            "Step 195/1000, Loss: 3.172534942626953, LR: 0.0005\n",
            "Step 196/1000, Loss: 3.1644599437713623, LR: 0.0005\n",
            "Step 197/1000, Loss: 3.1573681831359863, LR: 0.0005\n",
            "Step 198/1000, Loss: 3.1479012966156006, LR: 0.0005\n",
            "Step 199/1000, Loss: 3.1226918697357178, LR: 0.0005\n",
            "Step 200/1000, Loss: 3.097903251647949, LR: 0.0005\n",
            "Step 201/1000, Loss: 3.0665550231933594, LR: 0.0005\n",
            "Step 202/1000, Loss: 3.0587806701660156, LR: 0.0005\n",
            "Step 203/1000, Loss: 3.023871898651123, LR: 0.0005\n",
            "Step 204/1000, Loss: 2.979020357131958, LR: 0.0005\n",
            "Step 205/1000, Loss: 2.9708054065704346, LR: 0.0005\n",
            "Step 206/1000, Loss: 2.954228401184082, LR: 0.0005\n",
            "Step 207/1000, Loss: 2.9390013217926025, LR: 0.0005\n",
            "Step 208/1000, Loss: 2.905569553375244, LR: 0.0005\n",
            "Step 209/1000, Loss: 2.8730244636535645, LR: 0.0005\n",
            "Step 210/1000, Loss: 2.8674609661102295, LR: 0.0005\n",
            "Step 211/1000, Loss: 2.855639934539795, LR: 0.0005\n",
            "Step 212/1000, Loss: 2.8558738231658936, LR: 0.0005\n",
            "Step 213/1000, Loss: 2.86918306350708, LR: 0.0005\n",
            "Step 214/1000, Loss: 2.8461713790893555, LR: 0.0005\n",
            "Step 215/1000, Loss: 2.814182758331299, LR: 0.0005\n",
            "Step 216/1000, Loss: 2.7935945987701416, LR: 0.0005\n",
            "Step 217/1000, Loss: 2.7812681198120117, LR: 0.0005\n",
            "Step 218/1000, Loss: 2.7667975425720215, LR: 0.0005\n",
            "Step 219/1000, Loss: 2.753769636154175, LR: 0.0005\n",
            "Step 220/1000, Loss: 2.7543444633483887, LR: 0.0005\n",
            "Step 221/1000, Loss: 2.721508264541626, LR: 0.0005\n",
            "Step 222/1000, Loss: 2.729163408279419, LR: 0.0005\n",
            "Step 223/1000, Loss: 2.7063753604888916, LR: 0.0005\n",
            "Step 224/1000, Loss: 2.68135929107666, LR: 0.0005\n",
            "Step 225/1000, Loss: 2.669187068939209, LR: 0.0005\n",
            "Step 226/1000, Loss: 2.660661458969116, LR: 0.0005\n",
            "Step 227/1000, Loss: 2.6326799392700195, LR: 0.0005\n",
            "Step 228/1000, Loss: 2.637955665588379, LR: 0.0005\n",
            "Step 229/1000, Loss: 2.6186447143554688, LR: 0.0005\n",
            "Step 230/1000, Loss: 2.5809619426727295, LR: 0.0005\n",
            "Step 231/1000, Loss: 2.571664810180664, LR: 0.0005\n",
            "Step 232/1000, Loss: 2.577444553375244, LR: 0.0005\n",
            "Step 233/1000, Loss: 2.54941987991333, LR: 0.0005\n",
            "Step 234/1000, Loss: 2.5364792346954346, LR: 0.0005\n",
            "Step 235/1000, Loss: 2.5383594036102295, LR: 0.0005\n",
            "Step 236/1000, Loss: 2.520277500152588, LR: 0.0005\n",
            "Step 237/1000, Loss: 2.5046403408050537, LR: 0.0005\n",
            "Step 238/1000, Loss: 2.4785540103912354, LR: 0.0005\n",
            "Step 239/1000, Loss: 2.4809670448303223, LR: 0.0005\n",
            "Step 240/1000, Loss: 2.462975263595581, LR: 0.0005\n",
            "Step 241/1000, Loss: 2.4556148052215576, LR: 0.0005\n",
            "Step 242/1000, Loss: 2.4297916889190674, LR: 0.0005\n",
            "Step 243/1000, Loss: 2.39184832572937, LR: 0.0005\n",
            "Step 244/1000, Loss: 2.3745079040527344, LR: 0.0005\n",
            "Step 245/1000, Loss: 2.3640854358673096, LR: 0.0005\n",
            "Step 246/1000, Loss: 2.3464255332946777, LR: 0.0005\n",
            "Step 247/1000, Loss: 2.3494179248809814, LR: 0.0005\n",
            "Step 248/1000, Loss: 2.331693172454834, LR: 0.0005\n",
            "Step 249/1000, Loss: 2.308777093887329, LR: 0.0005\n",
            "Step 250/1000, Loss: 2.296970844268799, LR: 0.0005\n",
            "Step 251/1000, Loss: 2.2653470039367676, LR: 0.0005\n",
            "Step 252/1000, Loss: 2.236013174057007, LR: 0.0005\n",
            "Step 253/1000, Loss: 2.2128851413726807, LR: 0.0005\n",
            "Step 254/1000, Loss: 2.2008144855499268, LR: 0.0005\n",
            "Step 255/1000, Loss: 2.1996219158172607, LR: 0.0005\n",
            "Step 256/1000, Loss: 2.1947922706604004, LR: 0.0005\n",
            "Step 257/1000, Loss: 2.175595760345459, LR: 0.0005\n",
            "Step 258/1000, Loss: 2.157819986343384, LR: 0.0005\n",
            "Step 259/1000, Loss: 2.1302566528320312, LR: 0.0005\n",
            "Step 260/1000, Loss: 2.1034696102142334, LR: 0.0005\n",
            "Step 261/1000, Loss: 2.0796091556549072, LR: 0.0005\n",
            "Step 262/1000, Loss: 2.064819812774658, LR: 0.0005\n",
            "Step 263/1000, Loss: 2.048025608062744, LR: 0.0005\n",
            "Step 264/1000, Loss: 2.0346102714538574, LR: 0.0005\n",
            "Step 265/1000, Loss: 2.027886390686035, LR: 0.0005\n",
            "Step 266/1000, Loss: 1.9788423776626587, LR: 0.0005\n",
            "Step 267/1000, Loss: 1.9474748373031616, LR: 0.0005\n",
            "Step 268/1000, Loss: 1.9317245483398438, LR: 0.0005\n",
            "Step 269/1000, Loss: 1.9186675548553467, LR: 0.0005\n",
            "Step 270/1000, Loss: 1.8852226734161377, LR: 0.0005\n",
            "Step 271/1000, Loss: 1.876244306564331, LR: 0.0005\n",
            "Step 272/1000, Loss: 1.842331886291504, LR: 0.0005\n",
            "Step 273/1000, Loss: 1.8287944793701172, LR: 0.0005\n",
            "Step 274/1000, Loss: 1.8013193607330322, LR: 0.0005\n",
            "Step 275/1000, Loss: 1.79136061668396, LR: 0.0005\n",
            "Step 276/1000, Loss: 1.7899448871612549, LR: 0.0005\n",
            "Step 277/1000, Loss: 1.7811002731323242, LR: 0.0005\n",
            "Step 278/1000, Loss: 1.764428734779358, LR: 0.0005\n",
            "Step 279/1000, Loss: 1.7321746349334717, LR: 0.0005\n",
            "Step 280/1000, Loss: 1.71257746219635, LR: 0.0005\n",
            "Step 281/1000, Loss: 1.711515188217163, LR: 0.0005\n",
            "Step 282/1000, Loss: 1.687685251235962, LR: 0.0005\n",
            "Step 283/1000, Loss: 1.6757681369781494, LR: 0.0005\n",
            "Step 284/1000, Loss: 1.645421028137207, LR: 0.0005\n",
            "Step 285/1000, Loss: 1.641474723815918, LR: 0.0005\n",
            "Step 286/1000, Loss: 1.615427017211914, LR: 0.0005\n",
            "Step 287/1000, Loss: 1.6107347011566162, LR: 0.0005\n",
            "Step 288/1000, Loss: 1.611518144607544, LR: 0.0005\n",
            "Step 289/1000, Loss: 1.6037676334381104, LR: 0.0005\n",
            "Step 290/1000, Loss: 1.5598657131195068, LR: 0.0005\n",
            "Step 291/1000, Loss: 1.539670705795288, LR: 0.0005\n",
            "Step 292/1000, Loss: 1.5237125158309937, LR: 0.0005\n",
            "Step 293/1000, Loss: 1.515089750289917, LR: 0.0005\n",
            "Step 294/1000, Loss: 1.4829398393630981, LR: 0.0005\n",
            "Step 295/1000, Loss: 1.469557285308838, LR: 0.0005\n",
            "Step 296/1000, Loss: 1.4755609035491943, LR: 0.0005\n",
            "Step 297/1000, Loss: 1.4488952159881592, LR: 0.0005\n",
            "Step 298/1000, Loss: 1.4172840118408203, LR: 0.0005\n",
            "Step 299/1000, Loss: 1.4243783950805664, LR: 0.0005\n",
            "Step 300/1000, Loss: 1.4070521593093872, LR: 0.0005\n",
            "Step 301/1000, Loss: 1.3949347734451294, LR: 0.0005\n",
            "Step 302/1000, Loss: 1.3810094594955444, LR: 0.0005\n",
            "Step 303/1000, Loss: 1.3695746660232544, LR: 0.0005\n",
            "Step 304/1000, Loss: 1.3693941831588745, LR: 0.0005\n",
            "Step 305/1000, Loss: 1.3583314418792725, LR: 0.0005\n",
            "Step 306/1000, Loss: 1.3551133871078491, LR: 0.0005\n",
            "Step 307/1000, Loss: 1.3310461044311523, LR: 0.0005\n",
            "Step 308/1000, Loss: 1.3252372741699219, LR: 0.0005\n",
            "Step 309/1000, Loss: 1.310289740562439, LR: 0.0005\n",
            "Step 310/1000, Loss: 1.3364289999008179, LR: 0.0005\n",
            "Step 311/1000, Loss: 1.3402405977249146, LR: 0.0005\n",
            "Step 312/1000, Loss: 1.3057222366333008, LR: 0.0005\n",
            "Step 313/1000, Loss: 1.2555572986602783, LR: 0.0005\n",
            "Step 314/1000, Loss: 1.2786073684692383, LR: 0.0005\n",
            "Step 315/1000, Loss: 1.2862904071807861, LR: 0.0005\n",
            "Step 316/1000, Loss: 1.2695915699005127, LR: 0.0005\n",
            "Step 317/1000, Loss: 1.2488386631011963, LR: 0.0005\n",
            "Step 318/1000, Loss: 1.2311413288116455, LR: 0.0005\n",
            "Step 319/1000, Loss: 1.195859670639038, LR: 0.0005\n",
            "Step 320/1000, Loss: 1.175930142402649, LR: 0.0005\n",
            "Step 321/1000, Loss: 1.1632606983184814, LR: 0.0005\n",
            "Step 322/1000, Loss: 1.1373732089996338, LR: 0.0005\n",
            "Step 323/1000, Loss: 1.1188726425170898, LR: 0.0005\n",
            "Step 324/1000, Loss: 1.1170337200164795, LR: 0.0005\n",
            "Step 325/1000, Loss: 1.0916062593460083, LR: 0.0005\n",
            "Step 326/1000, Loss: 1.0770444869995117, LR: 0.0005\n",
            "Step 327/1000, Loss: 1.0963935852050781, LR: 0.0005\n",
            "Step 328/1000, Loss: 1.0843746662139893, LR: 0.0005\n",
            "Step 329/1000, Loss: 1.0631479024887085, LR: 0.0005\n",
            "Step 330/1000, Loss: 1.077026128768921, LR: 0.0005\n",
            "Step 331/1000, Loss: 1.0650075674057007, LR: 0.0005\n",
            "Step 332/1000, Loss: 1.0656172037124634, LR: 0.0005\n",
            "Step 333/1000, Loss: 1.0517241954803467, LR: 0.0005\n",
            "Step 334/1000, Loss: 1.0460470914840698, LR: 0.0005\n",
            "Step 335/1000, Loss: 1.0230119228363037, LR: 0.0005\n",
            "Step 336/1000, Loss: 1.00508713722229, LR: 0.0005\n",
            "Step 337/1000, Loss: 0.9700304865837097, LR: 0.0005\n",
            "Step 338/1000, Loss: 0.967957615852356, LR: 0.0005\n",
            "Step 339/1000, Loss: 0.9377196431159973, LR: 0.0005\n",
            "Step 340/1000, Loss: 0.9243243336677551, LR: 0.0005\n",
            "Step 341/1000, Loss: 0.9037895202636719, LR: 0.0005\n",
            "Step 342/1000, Loss: 0.9096879959106445, LR: 0.0005\n",
            "Step 343/1000, Loss: 0.9215117692947388, LR: 0.0005\n",
            "Step 344/1000, Loss: 0.9092743992805481, LR: 0.0005\n",
            "Step 345/1000, Loss: 0.896843433380127, LR: 0.0005\n",
            "Step 346/1000, Loss: 0.9039295315742493, LR: 0.0005\n",
            "Step 347/1000, Loss: 0.9193969964981079, LR: 0.0005\n",
            "Step 348/1000, Loss: 0.9099286794662476, LR: 0.0005\n",
            "Step 349/1000, Loss: 0.9097145795822144, LR: 0.0005\n",
            "Step 350/1000, Loss: 0.8837523460388184, LR: 0.0005\n",
            "Step 351/1000, Loss: 0.8773738741874695, LR: 0.0005\n",
            "Step 352/1000, Loss: 0.8736567497253418, LR: 0.0005\n",
            "Step 353/1000, Loss: 0.8827304840087891, LR: 0.0005\n",
            "Step 354/1000, Loss: 0.861080527305603, LR: 0.0005\n",
            "Step 355/1000, Loss: 0.869343101978302, LR: 0.0005\n",
            "Step 356/1000, Loss: 0.870052695274353, LR: 0.0005\n",
            "Step 357/1000, Loss: 0.8451881408691406, LR: 0.0005\n",
            "Step 358/1000, Loss: 0.8656991124153137, LR: 0.0005\n",
            "Step 359/1000, Loss: 0.8662658929824829, LR: 0.0005\n",
            "Step 360/1000, Loss: 0.8478053212165833, LR: 0.0005\n",
            "Step 361/1000, Loss: 0.8322674632072449, LR: 0.0005\n",
            "Step 362/1000, Loss: 0.8213379979133606, LR: 0.0005\n",
            "Step 363/1000, Loss: 0.8200458288192749, LR: 0.0005\n",
            "Step 364/1000, Loss: 0.8181761503219604, LR: 0.0005\n",
            "Step 365/1000, Loss: 0.8163387179374695, LR: 0.0005\n",
            "Step 366/1000, Loss: 0.7937390208244324, LR: 0.0005\n",
            "Step 367/1000, Loss: 0.7693770527839661, LR: 0.0005\n",
            "Step 368/1000, Loss: 0.7645674347877502, LR: 0.0005\n",
            "Step 369/1000, Loss: 0.737683117389679, LR: 0.0005\n",
            "Step 370/1000, Loss: 0.7150553464889526, LR: 0.0005\n",
            "Step 371/1000, Loss: 0.7140983939170837, LR: 0.0005\n",
            "Step 372/1000, Loss: 0.710894763469696, LR: 0.0005\n",
            "Step 373/1000, Loss: 0.6989497542381287, LR: 0.0005\n",
            "Step 374/1000, Loss: 0.6864019632339478, LR: 0.0005\n",
            "Step 375/1000, Loss: 0.6782218217849731, LR: 0.0005\n",
            "Step 376/1000, Loss: 0.6931029558181763, LR: 0.0005\n",
            "Step 377/1000, Loss: 0.6965305805206299, LR: 0.0005\n",
            "Step 378/1000, Loss: 0.6669130921363831, LR: 0.0005\n",
            "Step 379/1000, Loss: 0.6545764207839966, LR: 0.0005\n",
            "Step 380/1000, Loss: 0.643888533115387, LR: 0.0005\n",
            "Step 381/1000, Loss: 0.628439724445343, LR: 0.0005\n",
            "Step 382/1000, Loss: 0.6198010444641113, LR: 0.0005\n",
            "Step 383/1000, Loss: 0.6036940813064575, LR: 0.0005\n",
            "Step 384/1000, Loss: 0.6095243692398071, LR: 0.0005\n",
            "Step 385/1000, Loss: 0.6129253506660461, LR: 0.0005\n",
            "Step 386/1000, Loss: 0.6325975656509399, LR: 0.0005\n",
            "Step 387/1000, Loss: 0.6411563754081726, LR: 0.0005\n",
            "Step 388/1000, Loss: 0.652446448802948, LR: 0.0005\n",
            "Step 389/1000, Loss: 0.6370224952697754, LR: 0.0005\n",
            "Step 390/1000, Loss: 0.6187399625778198, LR: 0.0005\n",
            "Step 391/1000, Loss: 0.6004735827445984, LR: 0.0005\n",
            "Step 392/1000, Loss: 0.594428539276123, LR: 0.0005\n",
            "Step 393/1000, Loss: 0.6067441701889038, LR: 0.0005\n",
            "Step 394/1000, Loss: 0.6312612891197205, LR: 0.0005\n",
            "Step 395/1000, Loss: 0.6736928224563599, LR: 0.0005\n",
            "Step 396/1000, Loss: 0.6839991211891174, LR: 0.0005\n",
            "Step 397/1000, Loss: 0.6524891257286072, LR: 0.0005\n",
            "Step 398/1000, Loss: 0.6436099410057068, LR: 0.0005\n",
            "Step 399/1000, Loss: 0.6383522152900696, LR: 0.0005\n",
            "Step 400/1000, Loss: 0.6109321117401123, LR: 0.0005\n",
            "Step 401/1000, Loss: 0.6183136105537415, LR: 0.0005\n",
            "Step 402/1000, Loss: 0.5864837169647217, LR: 0.0005\n",
            "Step 403/1000, Loss: 0.584269642829895, LR: 0.0005\n",
            "Step 404/1000, Loss: 0.5934371948242188, LR: 0.0005\n",
            "Step 405/1000, Loss: 0.5757063031196594, LR: 0.0005\n",
            "Step 406/1000, Loss: 0.5703123211860657, LR: 0.0005\n",
            "Step 407/1000, Loss: 0.5434995889663696, LR: 0.0005\n",
            "Step 408/1000, Loss: 0.5514204502105713, LR: 0.0005\n",
            "Step 409/1000, Loss: 0.5597916841506958, LR: 0.0005\n",
            "Step 410/1000, Loss: 0.563462495803833, LR: 0.0005\n",
            "Step 411/1000, Loss: 0.5499494075775146, LR: 0.0005\n",
            "Step 412/1000, Loss: 0.5233972072601318, LR: 0.0005\n",
            "Step 413/1000, Loss: 0.504203736782074, LR: 0.0005\n",
            "Step 414/1000, Loss: 0.4897310733795166, LR: 0.0005\n",
            "Step 415/1000, Loss: 0.494534969329834, LR: 0.0005\n",
            "Step 416/1000, Loss: 0.46887022256851196, LR: 0.0005\n",
            "Step 417/1000, Loss: 0.46209216117858887, LR: 0.0005\n",
            "Step 418/1000, Loss: 0.465052992105484, LR: 0.0005\n",
            "Step 419/1000, Loss: 0.45604199171066284, LR: 0.0005\n",
            "Step 420/1000, Loss: 0.4371207654476166, LR: 0.0005\n",
            "Step 421/1000, Loss: 0.41402560472488403, LR: 0.0005\n",
            "Step 422/1000, Loss: 0.4363742768764496, LR: 0.0005\n",
            "Step 423/1000, Loss: 0.4174273908138275, LR: 0.0005\n",
            "Step 424/1000, Loss: 0.4107434153556824, LR: 0.0005\n",
            "Step 425/1000, Loss: 0.40721216797828674, LR: 0.0005\n",
            "Step 426/1000, Loss: 0.4082273542881012, LR: 0.0005\n",
            "Step 427/1000, Loss: 0.39075595140457153, LR: 0.0005\n",
            "Step 428/1000, Loss: 0.4067247807979584, LR: 0.0005\n",
            "Step 429/1000, Loss: 0.4011359214782715, LR: 0.0005\n",
            "Step 430/1000, Loss: 0.3967561721801758, LR: 0.0005\n",
            "Step 431/1000, Loss: 0.4046071171760559, LR: 0.0005\n",
            "Step 432/1000, Loss: 0.41799020767211914, LR: 0.0005\n",
            "Step 433/1000, Loss: 0.3920240104198456, LR: 0.0005\n",
            "Step 434/1000, Loss: 0.37924373149871826, LR: 0.0005\n",
            "Step 435/1000, Loss: 0.3781460225582123, LR: 0.0005\n",
            "Step 436/1000, Loss: 0.3815513551235199, LR: 0.0005\n",
            "Step 437/1000, Loss: 0.37023890018463135, LR: 0.0005\n",
            "Step 438/1000, Loss: 0.35491085052490234, LR: 0.0005\n",
            "Step 439/1000, Loss: 0.34662842750549316, LR: 0.0005\n",
            "Step 440/1000, Loss: 0.36207273602485657, LR: 0.0005\n",
            "Step 441/1000, Loss: 0.3730314075946808, LR: 0.0005\n",
            "Step 442/1000, Loss: 0.3664458394050598, LR: 0.0005\n",
            "Step 443/1000, Loss: 0.3572966456413269, LR: 0.0005\n",
            "Step 444/1000, Loss: 0.35378462076187134, LR: 0.0005\n",
            "Step 445/1000, Loss: 0.354721337556839, LR: 0.0005\n",
            "Step 446/1000, Loss: 0.3324449360370636, LR: 0.0005\n",
            "Step 447/1000, Loss: 0.32452985644340515, LR: 0.0005\n",
            "Step 448/1000, Loss: 0.32136186957359314, LR: 0.0005\n",
            "Step 449/1000, Loss: 0.32477572560310364, LR: 0.0005\n",
            "Step 450/1000, Loss: 0.32552820444107056, LR: 0.0005\n",
            "Step 451/1000, Loss: 0.3165009617805481, LR: 0.0005\n",
            "Step 452/1000, Loss: 0.2987794280052185, LR: 0.0005\n",
            "Step 453/1000, Loss: 0.29281920194625854, LR: 0.0005\n",
            "Step 454/1000, Loss: 0.2835347652435303, LR: 0.0005\n",
            "Step 455/1000, Loss: 0.2828827500343323, LR: 0.0005\n",
            "Step 456/1000, Loss: 0.2706434428691864, LR: 0.0005\n",
            "Step 457/1000, Loss: 0.28443458676338196, LR: 0.0005\n",
            "Step 458/1000, Loss: 0.28156232833862305, LR: 0.0005\n",
            "Step 459/1000, Loss: 0.27728238701820374, LR: 0.0005\n",
            "Step 460/1000, Loss: 0.28822922706604004, LR: 0.0005\n",
            "Step 461/1000, Loss: 0.2672039866447449, LR: 0.0005\n",
            "Step 462/1000, Loss: 0.253134161233902, LR: 0.0005\n",
            "Step 463/1000, Loss: 0.24115963280200958, LR: 0.0005\n",
            "Step 464/1000, Loss: 0.24002012610435486, LR: 0.0005\n",
            "Step 465/1000, Loss: 0.23908594250679016, LR: 0.0005\n",
            "Step 466/1000, Loss: 0.2319941222667694, LR: 0.0005\n",
            "Step 467/1000, Loss: 0.2285253256559372, LR: 0.0005\n",
            "Step 468/1000, Loss: 0.22348380088806152, LR: 0.0005\n",
            "Step 469/1000, Loss: 0.22138187289237976, LR: 0.0005\n",
            "Step 470/1000, Loss: 0.23238055408000946, LR: 0.0005\n",
            "Step 471/1000, Loss: 0.23226062953472137, LR: 0.0005\n",
            "Step 472/1000, Loss: 0.23205141723155975, LR: 0.0005\n",
            "Step 473/1000, Loss: 0.2329082041978836, LR: 0.0005\n",
            "Step 474/1000, Loss: 0.22224338352680206, LR: 0.0005\n",
            "Step 475/1000, Loss: 0.22853977978229523, LR: 0.0005\n",
            "Step 476/1000, Loss: 0.20879168808460236, LR: 0.0005\n",
            "Step 477/1000, Loss: 0.20795026421546936, LR: 0.0005\n",
            "Step 478/1000, Loss: 0.2103862315416336, LR: 0.0005\n",
            "Step 479/1000, Loss: 0.19135843217372894, LR: 0.0005\n",
            "Step 480/1000, Loss: 0.18687506020069122, LR: 0.0005\n",
            "Step 481/1000, Loss: 0.1904873549938202, LR: 0.0005\n",
            "Step 482/1000, Loss: 0.17999114096164703, LR: 0.0005\n",
            "Step 483/1000, Loss: 0.19540205597877502, LR: 0.0005\n",
            "Step 484/1000, Loss: 0.22388646006584167, LR: 0.0005\n",
            "Step 485/1000, Loss: 0.22279754281044006, LR: 0.0005\n",
            "Step 486/1000, Loss: 0.2158145010471344, LR: 0.0005\n",
            "Step 487/1000, Loss: 0.2166273146867752, LR: 0.0005\n",
            "Step 488/1000, Loss: 0.20572204887866974, LR: 0.0005\n",
            "Step 489/1000, Loss: 0.21187281608581543, LR: 0.0005\n",
            "Step 490/1000, Loss: 0.22066445648670197, LR: 0.0005\n",
            "Step 491/1000, Loss: 0.22465398907661438, LR: 0.0005\n",
            "Step 492/1000, Loss: 0.2200523316860199, LR: 0.0005\n",
            "Step 493/1000, Loss: 0.21883109211921692, LR: 0.0005\n",
            "Step 494/1000, Loss: 0.21836289763450623, LR: 0.0005\n",
            "Step 495/1000, Loss: 0.20147296786308289, LR: 0.0005\n",
            "Step 496/1000, Loss: 0.19445526599884033, LR: 0.0005\n",
            "Step 497/1000, Loss: 0.18928959965705872, LR: 0.0005\n",
            "Step 498/1000, Loss: 0.19034895300865173, LR: 0.0005\n",
            "Step 499/1000, Loss: 0.17834091186523438, LR: 0.0005\n",
            "Step 500/1000, Loss: 0.18654637038707733, LR: 0.0005\n",
            "Step 501/1000, Loss: 0.17985805869102478, LR: 0.0005\n",
            "Step 502/1000, Loss: 0.17028620839118958, LR: 0.0005\n",
            "Step 503/1000, Loss: 0.17000171542167664, LR: 0.0005\n",
            "Step 504/1000, Loss: 0.1748601794242859, LR: 0.0005\n",
            "Step 505/1000, Loss: 0.18722914159297943, LR: 0.0005\n",
            "Step 506/1000, Loss: 0.19759556651115417, LR: 0.0005\n",
            "Step 507/1000, Loss: 0.19890208542346954, LR: 0.0005\n",
            "Step 508/1000, Loss: 0.18704530596733093, LR: 0.0005\n",
            "Step 509/1000, Loss: 0.1849716305732727, LR: 0.0005\n",
            "Step 510/1000, Loss: 0.184732124209404, LR: 0.0005\n",
            "Step 511/1000, Loss: 0.18371516466140747, LR: 0.0005\n",
            "Step 512/1000, Loss: 0.1903942972421646, LR: 0.0005\n",
            "Step 513/1000, Loss: 0.1872595250606537, LR: 0.0005\n",
            "Step 514/1000, Loss: 0.18018800020217896, LR: 0.0005\n",
            "Step 515/1000, Loss: 0.17037823796272278, LR: 0.0005\n",
            "Step 516/1000, Loss: 0.1671590805053711, LR: 0.0005\n",
            "Step 517/1000, Loss: 0.1693660318851471, LR: 0.0005\n",
            "Step 518/1000, Loss: 0.17421209812164307, LR: 0.0005\n",
            "Step 519/1000, Loss: 0.17531129717826843, LR: 0.0005\n",
            "Step 520/1000, Loss: 0.1872468739748001, LR: 0.0005\n",
            "Step 521/1000, Loss: 0.18938492238521576, LR: 0.0005\n",
            "Step 522/1000, Loss: 0.1872682273387909, LR: 0.0005\n",
            "Step 523/1000, Loss: 0.18801698088645935, LR: 0.0005\n",
            "Step 524/1000, Loss: 0.17992208898067474, LR: 0.0005\n",
            "Step 525/1000, Loss: 0.16556301712989807, LR: 0.0005\n",
            "Step 526/1000, Loss: 0.167608842253685, LR: 0.0005\n",
            "Step 527/1000, Loss: 0.16385821998119354, LR: 0.0005\n",
            "Step 528/1000, Loss: 0.16344033181667328, LR: 0.0005\n",
            "Step 529/1000, Loss: 0.15111441910266876, LR: 0.0005\n",
            "Step 530/1000, Loss: 0.1478053331375122, LR: 0.0005\n",
            "Step 531/1000, Loss: 0.15292318165302277, LR: 0.0005\n",
            "Step 532/1000, Loss: 0.15604010224342346, LR: 0.0005\n",
            "Step 533/1000, Loss: 0.1654641032218933, LR: 0.0005\n",
            "Step 534/1000, Loss: 0.15679971873760223, LR: 0.0005\n",
            "Step 535/1000, Loss: 0.14681723713874817, LR: 0.0005\n",
            "Step 536/1000, Loss: 0.15554651618003845, LR: 0.0005\n",
            "Step 537/1000, Loss: 0.1586087942123413, LR: 0.0005\n",
            "Step 538/1000, Loss: 0.15057186782360077, LR: 0.0005\n",
            "Step 539/1000, Loss: 0.1441071629524231, LR: 0.0005\n",
            "Step 540/1000, Loss: 0.13398683071136475, LR: 0.0005\n",
            "Step 541/1000, Loss: 0.12451115995645523, LR: 0.0005\n",
            "Step 542/1000, Loss: 0.12552228569984436, LR: 0.0005\n",
            "Step 543/1000, Loss: 0.12251965701580048, LR: 0.0005\n",
            "Step 544/1000, Loss: 0.12409601360559464, LR: 0.0005\n",
            "Step 545/1000, Loss: 0.11980509757995605, LR: 0.0005\n",
            "Step 546/1000, Loss: 0.11921397596597672, LR: 0.0005\n",
            "Step 547/1000, Loss: 0.12421467155218124, LR: 0.0005\n",
            "Step 548/1000, Loss: 0.12658628821372986, LR: 0.0005\n",
            "Step 549/1000, Loss: 0.13326843082904816, LR: 0.0005\n",
            "Step 550/1000, Loss: 0.12381921708583832, LR: 0.0005\n",
            "Step 551/1000, Loss: 0.11581559479236603, LR: 0.0005\n",
            "Step 552/1000, Loss: 0.11535737663507462, LR: 0.0005\n",
            "Step 553/1000, Loss: 0.13160692155361176, LR: 0.0005\n",
            "Step 554/1000, Loss: 0.12189646065235138, LR: 0.0005\n",
            "Step 555/1000, Loss: 0.11585875600576401, LR: 0.0005\n",
            "Step 556/1000, Loss: 0.1124630942940712, LR: 0.0005\n",
            "Step 557/1000, Loss: 0.1211152896285057, LR: 0.0005\n",
            "Step 558/1000, Loss: 0.1169012039899826, LR: 0.0005\n",
            "Step 559/1000, Loss: 0.112946055829525, LR: 0.0005\n",
            "Step 560/1000, Loss: 0.10744037479162216, LR: 0.0005\n",
            "Step 561/1000, Loss: 0.11075057834386826, LR: 0.0005\n",
            "Step 562/1000, Loss: 0.11693501472473145, LR: 0.0005\n",
            "Step 563/1000, Loss: 0.10756738483905792, LR: 0.0005\n",
            "Step 564/1000, Loss: 0.09948016703128815, LR: 0.0005\n",
            "Step 565/1000, Loss: 0.10241025686264038, LR: 0.0005\n",
            "Step 566/1000, Loss: 0.112420953810215, LR: 0.0005\n",
            "Step 567/1000, Loss: 0.11103066056966782, LR: 0.0005\n",
            "Step 568/1000, Loss: 0.0980253666639328, LR: 0.0005\n",
            "Step 569/1000, Loss: 0.09519194066524506, LR: 0.0005\n",
            "Step 570/1000, Loss: 0.10033954679965973, LR: 0.0005\n",
            "Step 571/1000, Loss: 0.11113598197698593, LR: 0.0005\n",
            "Step 572/1000, Loss: 0.1067027673125267, LR: 0.0005\n",
            "Step 573/1000, Loss: 0.10017013549804688, LR: 0.0005\n",
            "Step 574/1000, Loss: 0.08589186519384384, LR: 0.0005\n",
            "Step 575/1000, Loss: 0.08522863686084747, LR: 0.0005\n",
            "Step 576/1000, Loss: 0.08796512335538864, LR: 0.0005\n",
            "Step 577/1000, Loss: 0.08914685994386673, LR: 0.0005\n",
            "Step 578/1000, Loss: 0.10033176839351654, LR: 0.0005\n",
            "Step 579/1000, Loss: 0.09569718688726425, LR: 0.0005\n",
            "Step 580/1000, Loss: 0.09215684235095978, LR: 0.0005\n",
            "Step 581/1000, Loss: 0.09104704856872559, LR: 0.0005\n",
            "Step 582/1000, Loss: 0.08370417356491089, LR: 0.0005\n",
            "Step 583/1000, Loss: 0.08882755786180496, LR: 0.0005\n",
            "Step 584/1000, Loss: 0.09333042800426483, LR: 0.0005\n",
            "Step 585/1000, Loss: 0.09395621716976166, LR: 0.0005\n",
            "Step 586/1000, Loss: 0.09483806788921356, LR: 0.0005\n",
            "Step 587/1000, Loss: 0.10696176439523697, LR: 0.0005\n",
            "Step 588/1000, Loss: 0.11702649295330048, LR: 0.0005\n",
            "Step 589/1000, Loss: 0.11695094406604767, LR: 0.0005\n",
            "Step 590/1000, Loss: 0.11708281934261322, LR: 0.0005\n",
            "Step 591/1000, Loss: 0.11911292374134064, LR: 0.0005\n",
            "Step 592/1000, Loss: 0.10642746835947037, LR: 0.0005\n",
            "Step 593/1000, Loss: 0.10568960011005402, LR: 0.0005\n",
            "Step 594/1000, Loss: 0.09062449634075165, LR: 0.0005\n",
            "Step 595/1000, Loss: 0.08835713565349579, LR: 0.0005\n",
            "Step 596/1000, Loss: 0.08599722385406494, LR: 0.0005\n",
            "Step 597/1000, Loss: 0.08588212728500366, LR: 0.0005\n",
            "Step 598/1000, Loss: 0.08590028434991837, LR: 0.0005\n",
            "Step 599/1000, Loss: 0.08308453857898712, LR: 0.0005\n",
            "Step 600/1000, Loss: 0.09541193395853043, LR: 0.0005\n",
            "Step 601/1000, Loss: 0.08812487870454788, LR: 0.0005\n",
            "Step 602/1000, Loss: 0.08792287111282349, LR: 0.0005\n",
            "Step 603/1000, Loss: 0.08447670191526413, LR: 0.0005\n",
            "Step 604/1000, Loss: 0.08426915854215622, LR: 0.0005\n",
            "Step 605/1000, Loss: 0.08540277183055878, LR: 0.0005\n",
            "Step 606/1000, Loss: 0.08323539793491364, LR: 0.0005\n",
            "Step 607/1000, Loss: 0.08528713136911392, LR: 0.0005\n",
            "Step 608/1000, Loss: 0.09510378539562225, LR: 0.0005\n",
            "Step 609/1000, Loss: 0.07850505411624908, LR: 0.0005\n",
            "Step 610/1000, Loss: 0.0889134556055069, LR: 0.0005\n",
            "Step 611/1000, Loss: 0.07955340296030045, LR: 0.0005\n",
            "Step 612/1000, Loss: 0.07885599136352539, LR: 0.0005\n",
            "Step 613/1000, Loss: 0.08034021407365799, LR: 0.0005\n",
            "Step 614/1000, Loss: 0.06894141435623169, LR: 0.0005\n",
            "Step 615/1000, Loss: 0.07147861272096634, LR: 0.0005\n",
            "Step 616/1000, Loss: 0.0682905912399292, LR: 0.0005\n",
            "Step 617/1000, Loss: 0.07605328410863876, LR: 0.0005\n",
            "Step 618/1000, Loss: 0.08166414499282837, LR: 0.0005\n",
            "Step 619/1000, Loss: 0.07115388661623001, LR: 0.0005\n",
            "Step 620/1000, Loss: 0.0779682919383049, LR: 0.0005\n",
            "Step 621/1000, Loss: 0.07298511266708374, LR: 0.0005\n",
            "Step 622/1000, Loss: 0.1083405613899231, LR: 0.0005\n",
            "Step 623/1000, Loss: 0.1199323982000351, LR: 0.0005\n",
            "Step 624/1000, Loss: 0.09794630110263824, LR: 0.0005\n",
            "Step 625/1000, Loss: 0.07463125884532928, LR: 0.0005\n",
            "Step 626/1000, Loss: 0.08452042192220688, LR: 0.0005\n",
            "Step 627/1000, Loss: 0.07313860952854156, LR: 0.0005\n",
            "Step 628/1000, Loss: 0.07254444062709808, LR: 0.0005\n",
            "Step 629/1000, Loss: 0.08945196866989136, LR: 0.0005\n",
            "Step 630/1000, Loss: 0.11651842296123505, LR: 0.0005\n",
            "Step 631/1000, Loss: 0.08153659105300903, LR: 0.0005\n",
            "Step 632/1000, Loss: 0.07211168110370636, LR: 0.0005\n",
            "Step 633/1000, Loss: 0.0758204236626625, LR: 0.0005\n",
            "Step 634/1000, Loss: 0.07265093922615051, LR: 0.0005\n",
            "Step 635/1000, Loss: 0.0628904327750206, LR: 0.0005\n",
            "Step 636/1000, Loss: 0.06499655544757843, LR: 0.0005\n",
            "Step 637/1000, Loss: 0.0601317398250103, LR: 0.0005\n",
            "Step 638/1000, Loss: 0.05996574088931084, LR: 0.0005\n",
            "Step 639/1000, Loss: 0.062205951660871506, LR: 0.0005\n",
            "Step 640/1000, Loss: 0.06599584966897964, LR: 0.0005\n",
            "Step 641/1000, Loss: 0.05893300846219063, LR: 0.0005\n",
            "Step 642/1000, Loss: 0.056494928896427155, LR: 0.0005\n",
            "Step 643/1000, Loss: 0.06202076002955437, LR: 0.0005\n",
            "Step 644/1000, Loss: 0.058871109038591385, LR: 0.0005\n",
            "Step 645/1000, Loss: 0.049011193215847015, LR: 0.0005\n",
            "Step 646/1000, Loss: 0.05677228048443794, LR: 0.0005\n",
            "Step 647/1000, Loss: 0.056192588061094284, LR: 0.0005\n",
            "Step 648/1000, Loss: 0.05172703415155411, LR: 0.0005\n",
            "Step 649/1000, Loss: 0.057096682488918304, LR: 0.0005\n",
            "Step 650/1000, Loss: 0.05319962650537491, LR: 0.0005\n",
            "Step 651/1000, Loss: 0.04814205318689346, LR: 0.0005\n",
            "Step 652/1000, Loss: 0.051470182836055756, LR: 0.0005\n",
            "Step 653/1000, Loss: 0.053208671510219574, LR: 0.0005\n",
            "Step 654/1000, Loss: 0.04990384727716446, LR: 0.0005\n",
            "Step 655/1000, Loss: 0.04441634565591812, LR: 0.0005\n",
            "Step 656/1000, Loss: 0.04084813967347145, LR: 0.0005\n",
            "Step 657/1000, Loss: 0.044170595705509186, LR: 0.0005\n",
            "Step 658/1000, Loss: 0.04632062092423439, LR: 0.0005\n",
            "Step 659/1000, Loss: 0.04488269239664078, LR: 0.0005\n",
            "Step 660/1000, Loss: 0.0449594184756279, LR: 0.0005\n",
            "Step 661/1000, Loss: 0.057353146374225616, LR: 0.0005\n",
            "Step 662/1000, Loss: 0.05477500706911087, LR: 0.0005\n",
            "Step 663/1000, Loss: 0.053335029631853104, LR: 0.0005\n",
            "Step 664/1000, Loss: 0.051657289266586304, LR: 0.0005\n",
            "Step 665/1000, Loss: 0.05923281982541084, LR: 0.0005\n",
            "Step 666/1000, Loss: 0.05572907254099846, LR: 0.0005\n",
            "Step 667/1000, Loss: 0.05607025697827339, LR: 0.0005\n",
            "Step 668/1000, Loss: 0.057905398309230804, LR: 0.0005\n",
            "Step 669/1000, Loss: 0.06375221163034439, LR: 0.0005\n",
            "Step 670/1000, Loss: 0.06039778143167496, LR: 0.0005\n",
            "Step 671/1000, Loss: 0.06644179672002792, LR: 0.0005\n",
            "Step 672/1000, Loss: 0.06224949285387993, LR: 0.0005\n",
            "Step 673/1000, Loss: 0.060910988599061966, LR: 0.0005\n",
            "Step 674/1000, Loss: 0.055030010640621185, LR: 0.0005\n",
            "Step 675/1000, Loss: 0.05241537094116211, LR: 0.0005\n",
            "Step 676/1000, Loss: 0.060969460755586624, LR: 0.0005\n",
            "Step 677/1000, Loss: 0.058927275240421295, LR: 0.0001\n",
            "Step 678/1000, Loss: 0.06402787566184998, LR: 0.0001\n",
            "Step 679/1000, Loss: 0.06407123059034348, LR: 0.0001\n",
            "Step 680/1000, Loss: 0.05825525522232056, LR: 0.0001\n",
            "Step 681/1000, Loss: 0.05595303699374199, LR: 0.0001\n",
            "Step 682/1000, Loss: 0.05010877922177315, LR: 0.0001\n",
            "Step 683/1000, Loss: 0.04850585758686066, LR: 0.0001\n",
            "Step 684/1000, Loss: 0.045953407883644104, LR: 0.0001\n",
            "Step 685/1000, Loss: 0.04297474026679993, LR: 0.0001\n",
            "Step 686/1000, Loss: 0.03996017202734947, LR: 0.0001\n",
            "Step 687/1000, Loss: 0.0372818298637867, LR: 0.0001\n",
            "Step 688/1000, Loss: 0.03529901057481766, LR: 0.0001\n",
            "Step 689/1000, Loss: 0.03363201767206192, LR: 0.0001\n",
            "Step 690/1000, Loss: 0.032860495150089264, LR: 0.0001\n",
            "Step 691/1000, Loss: 0.03063543513417244, LR: 0.0001\n",
            "Step 692/1000, Loss: 0.029094496741890907, LR: 0.0001\n",
            "Step 693/1000, Loss: 0.027840223163366318, LR: 0.0001\n",
            "Step 694/1000, Loss: 0.028953909873962402, LR: 0.0001\n",
            "Step 695/1000, Loss: 0.02756868302822113, LR: 0.0001\n",
            "Step 696/1000, Loss: 0.026567626744508743, LR: 0.0001\n",
            "Step 697/1000, Loss: 0.02641737461090088, LR: 0.0001\n",
            "Step 698/1000, Loss: 0.02756710723042488, LR: 0.0001\n",
            "Step 699/1000, Loss: 0.024215396493673325, LR: 0.0001\n",
            "Step 700/1000, Loss: 0.02322467789053917, LR: 0.0001\n",
            "Step 701/1000, Loss: 0.0222491268068552, LR: 0.0001\n",
            "Step 702/1000, Loss: 0.021404754370450974, LR: 0.0001\n",
            "Step 703/1000, Loss: 0.020932018756866455, LR: 0.0001\n",
            "Step 704/1000, Loss: 0.020201925188302994, LR: 0.0001\n",
            "Step 705/1000, Loss: 0.0204827431589365, LR: 0.0001\n",
            "Step 706/1000, Loss: 0.019123699516057968, LR: 0.0001\n",
            "Step 707/1000, Loss: 0.018694665282964706, LR: 0.0001\n",
            "Step 708/1000, Loss: 0.018212392926216125, LR: 0.0001\n",
            "Step 709/1000, Loss: 0.01778905838727951, LR: 0.0001\n",
            "Step 710/1000, Loss: 0.017347224056720734, LR: 0.0001\n",
            "Step 711/1000, Loss: 0.01690581627190113, LR: 0.0001\n",
            "Step 712/1000, Loss: 0.0165349543094635, LR: 0.0001\n",
            "Step 713/1000, Loss: 0.016211170703172684, LR: 0.0001\n",
            "Step 714/1000, Loss: 0.015871090814471245, LR: 0.0001\n",
            "Step 715/1000, Loss: 0.015569751150906086, LR: 0.0001\n",
            "Step 716/1000, Loss: 0.015336255542933941, LR: 0.0001\n",
            "Step 717/1000, Loss: 0.014996635727584362, LR: 0.0001\n",
            "Step 718/1000, Loss: 0.015334012918174267, LR: 0.0001\n",
            "Step 719/1000, Loss: 0.01463706512004137, LR: 0.0001\n",
            "Step 720/1000, Loss: 0.014448484405875206, LR: 0.0001\n",
            "Step 721/1000, Loss: 0.014257475733757019, LR: 0.0001\n",
            "Step 722/1000, Loss: 0.013931864872574806, LR: 0.0001\n",
            "Step 723/1000, Loss: 0.013719340786337852, LR: 0.0001\n",
            "Step 724/1000, Loss: 0.013499232940375805, LR: 0.0001\n",
            "Step 725/1000, Loss: 0.013291612267494202, LR: 0.0001\n",
            "Step 726/1000, Loss: 0.014798027463257313, LR: 0.0001\n",
            "Step 727/1000, Loss: 0.01305677555501461, LR: 0.0001\n",
            "Step 728/1000, Loss: 0.013935339637100697, LR: 0.0001\n",
            "Step 729/1000, Loss: 0.01494878251105547, LR: 0.0001\n",
            "Step 730/1000, Loss: 0.013428077101707458, LR: 0.0001\n",
            "Step 731/1000, Loss: 0.013509241864085197, LR: 0.0001\n",
            "Step 732/1000, Loss: 0.013482932932674885, LR: 0.0001\n",
            "Step 733/1000, Loss: 0.013618290424346924, LR: 0.0001\n",
            "Step 734/1000, Loss: 0.0130748450756073, LR: 0.0001\n",
            "Step 735/1000, Loss: 0.01275942288339138, LR: 0.0001\n",
            "Step 736/1000, Loss: 0.012535681016743183, LR: 0.0001\n",
            "Step 737/1000, Loss: 0.01289400178939104, LR: 0.0001\n",
            "Step 738/1000, Loss: 0.012860444374382496, LR: 0.0001\n",
            "Step 739/1000, Loss: 0.01354834996163845, LR: 0.0001\n",
            "Step 740/1000, Loss: 0.01242379005998373, LR: 0.0001\n",
            "Step 741/1000, Loss: 0.011869930662214756, LR: 0.0001\n",
            "Step 742/1000, Loss: 0.01180819422006607, LR: 0.0001\n",
            "Step 743/1000, Loss: 0.011539941653609276, LR: 0.0001\n",
            "Step 744/1000, Loss: 0.01195994671434164, LR: 0.0001\n",
            "Step 745/1000, Loss: 0.011274044401943684, LR: 0.0001\n",
            "Step 746/1000, Loss: 0.011409847065806389, LR: 0.0001\n",
            "Step 747/1000, Loss: 0.011070836335420609, LR: 0.0001\n",
            "Step 748/1000, Loss: 0.010908852331340313, LR: 0.0001\n",
            "Step 749/1000, Loss: 0.010839962400496006, LR: 0.0001\n",
            "Step 750/1000, Loss: 0.010682085528969765, LR: 0.0001\n",
            "Step 751/1000, Loss: 0.010563052259385586, LR: 0.0001\n",
            "Step 752/1000, Loss: 0.010426904074847698, LR: 0.0001\n",
            "Step 753/1000, Loss: 0.010330324992537498, LR: 0.0001\n",
            "Step 754/1000, Loss: 0.010155769065022469, LR: 0.0001\n",
            "Step 755/1000, Loss: 0.010123392567038536, LR: 0.0001\n",
            "Step 756/1000, Loss: 0.010016610845923424, LR: 0.0001\n",
            "Step 757/1000, Loss: 0.00990261323750019, LR: 0.0001\n",
            "Step 758/1000, Loss: 0.009779585525393486, LR: 0.0001\n",
            "Step 759/1000, Loss: 0.009657101705670357, LR: 0.0001\n",
            "Step 760/1000, Loss: 0.009539510123431683, LR: 0.0001\n",
            "Step 761/1000, Loss: 0.010143707506358624, LR: 0.0001\n",
            "Step 762/1000, Loss: 0.009367367252707481, LR: 0.0001\n",
            "Step 763/1000, Loss: 0.009283630177378654, LR: 0.0001\n",
            "Step 764/1000, Loss: 0.009189988486468792, LR: 0.0001\n",
            "Step 765/1000, Loss: 0.009099817834794521, LR: 0.0001\n",
            "Step 766/1000, Loss: 0.0090244235470891, LR: 0.0001\n",
            "Step 767/1000, Loss: 0.008941411040723324, LR: 0.0001\n",
            "Step 768/1000, Loss: 0.0089210644364357, LR: 0.0001\n",
            "Step 769/1000, Loss: 0.00884384848177433, LR: 0.0001\n",
            "Step 770/1000, Loss: 0.008725387044250965, LR: 0.0001\n",
            "Step 771/1000, Loss: 0.008657841011881828, LR: 0.0001\n",
            "Step 772/1000, Loss: 0.008593878708779812, LR: 0.0001\n",
            "Step 773/1000, Loss: 0.008531033992767334, LR: 0.0001\n",
            "Step 774/1000, Loss: 0.00847670715302229, LR: 0.0001\n",
            "Step 775/1000, Loss: 0.008415953256189823, LR: 0.0001\n",
            "Step 776/1000, Loss: 0.008363204076886177, LR: 0.0001\n",
            "Step 777/1000, Loss: 0.008309995755553246, LR: 0.0001\n",
            "Step 778/1000, Loss: 0.008257298730313778, LR: 0.0001\n",
            "Step 779/1000, Loss: 0.008202975615859032, LR: 0.0001\n",
            "Step 780/1000, Loss: 0.008149301633238792, LR: 0.0001\n",
            "Step 781/1000, Loss: 0.008095674216747284, LR: 0.0001\n",
            "Step 782/1000, Loss: 0.008045346476137638, LR: 0.0001\n",
            "Step 783/1000, Loss: 0.00799850095063448, LR: 0.0001\n",
            "Step 784/1000, Loss: 0.00794619508087635, LR: 0.0001\n",
            "Step 785/1000, Loss: 0.007898970507085323, LR: 0.0001\n",
            "Step 786/1000, Loss: 0.007858267053961754, LR: 0.0001\n",
            "Step 787/1000, Loss: 0.007810953073203564, LR: 0.0001\n",
            "Step 788/1000, Loss: 0.007768124341964722, LR: 0.0001\n",
            "Step 789/1000, Loss: 0.007733012083917856, LR: 0.0001\n",
            "Step 790/1000, Loss: 0.00769192585721612, LR: 0.0001\n",
            "Step 791/1000, Loss: 0.007650978863239288, LR: 0.0001\n",
            "Step 792/1000, Loss: 0.007609694264829159, LR: 0.0001\n",
            "Step 793/1000, Loss: 0.007569072302430868, LR: 0.0001\n",
            "Step 794/1000, Loss: 0.007529460825026035, LR: 0.0001\n",
            "Step 795/1000, Loss: 0.007494403515011072, LR: 0.0001\n",
            "Step 796/1000, Loss: 0.007457704283297062, LR: 0.0001\n",
            "Step 797/1000, Loss: 0.0074221668764948845, LR: 0.0001\n",
            "Step 798/1000, Loss: 0.0073849596083164215, LR: 0.0001\n",
            "Step 799/1000, Loss: 0.00735092256218195, LR: 0.0001\n",
            "Step 800/1000, Loss: 0.007318601012229919, LR: 0.0001\n",
            "Step 801/1000, Loss: 0.007284875959157944, LR: 0.0001\n",
            "Step 802/1000, Loss: 0.007249526679515839, LR: 0.0001\n",
            "Step 803/1000, Loss: 0.007216329220682383, LR: 0.0001\n",
            "Step 804/1000, Loss: 0.007182500325143337, LR: 0.0001\n",
            "Step 805/1000, Loss: 0.007149656303226948, LR: 0.0001\n",
            "Step 806/1000, Loss: 0.007117958273738623, LR: 0.0001\n",
            "Step 807/1000, Loss: 0.007086853496730328, LR: 0.0001\n",
            "Step 808/1000, Loss: 0.007057369686663151, LR: 0.0001\n",
            "Step 809/1000, Loss: 0.00702676922082901, LR: 0.0001\n",
            "Step 810/1000, Loss: 0.006997970398515463, LR: 0.0001\n",
            "Step 811/1000, Loss: 0.006961732171475887, LR: 0.0001\n",
            "Step 812/1000, Loss: 0.006932959891855717, LR: 0.0001\n",
            "Step 813/1000, Loss: 0.006906145717948675, LR: 0.0001\n",
            "Step 814/1000, Loss: 0.006877778563648462, LR: 0.0001\n",
            "Step 815/1000, Loss: 0.006850270088762045, LR: 0.0001\n",
            "Step 816/1000, Loss: 0.00683822575956583, LR: 0.0001\n",
            "Step 817/1000, Loss: 0.006811405532062054, LR: 0.0001\n",
            "Step 818/1000, Loss: 0.006784955505281687, LR: 0.0001\n",
            "Step 819/1000, Loss: 0.006758543197065592, LR: 0.0001\n",
            "Step 820/1000, Loss: 0.006731419358402491, LR: 0.0001\n",
            "Step 821/1000, Loss: 0.006704866886138916, LR: 0.0001\n",
            "Step 822/1000, Loss: 0.006678593344986439, LR: 0.0001\n",
            "Step 823/1000, Loss: 0.006652729120105505, LR: 0.0001\n",
            "Step 824/1000, Loss: 0.0066271573305130005, LR: 0.0001\n",
            "Step 825/1000, Loss: 0.006602270994335413, LR: 0.0001\n",
            "Step 826/1000, Loss: 0.006577847059816122, LR: 0.0001\n",
            "Step 827/1000, Loss: 0.006553393788635731, LR: 0.0001\n",
            "Step 828/1000, Loss: 0.006529939826577902, LR: 0.0001\n",
            "Step 829/1000, Loss: 0.0065064216032624245, LR: 0.0001\n",
            "Step 830/1000, Loss: 0.006483516655862331, LR: 0.0001\n",
            "Step 831/1000, Loss: 0.006460773292928934, LR: 0.0001\n",
            "Step 832/1000, Loss: 0.00643813144415617, LR: 0.0001\n",
            "Step 833/1000, Loss: 0.006415648851543665, LR: 0.0001\n",
            "Step 834/1000, Loss: 0.006394172552973032, LR: 0.0001\n",
            "Step 835/1000, Loss: 0.006372210569679737, LR: 0.0001\n",
            "Step 836/1000, Loss: 0.006349983159452677, LR: 0.0001\n",
            "Step 837/1000, Loss: 0.006329013500362635, LR: 0.0001\n",
            "Step 838/1000, Loss: 0.006308399140834808, LR: 0.0001\n",
            "Step 839/1000, Loss: 0.006287516560405493, LR: 0.0001\n",
            "Step 840/1000, Loss: 0.006265987642109394, LR: 0.0001\n",
            "Step 841/1000, Loss: 0.0062498170882463455, LR: 0.0001\n",
            "Step 842/1000, Loss: 0.006231148727238178, LR: 0.0001\n",
            "Step 843/1000, Loss: 0.006210663355886936, LR: 0.0001\n",
            "Step 844/1000, Loss: 0.0061917076818645, LR: 0.0001\n",
            "Step 845/1000, Loss: 0.006172120105475187, LR: 0.0001\n",
            "Step 846/1000, Loss: 0.006152748595923185, LR: 0.0001\n",
            "Step 847/1000, Loss: 0.006133679766207933, LR: 0.0001\n",
            "Step 848/1000, Loss: 0.0061145988292992115, LR: 0.0001\n",
            "Step 849/1000, Loss: 0.006095671094954014, LR: 0.0001\n",
            "Step 850/1000, Loss: 0.006076617632061243, LR: 0.0001\n",
            "Step 851/1000, Loss: 0.006057293154299259, LR: 0.0001\n",
            "Step 852/1000, Loss: 0.006044856738299131, LR: 0.0001\n",
            "Step 853/1000, Loss: 0.006020909175276756, LR: 0.0001\n",
            "Step 854/1000, Loss: 0.00600203825160861, LR: 0.0001\n",
            "Step 855/1000, Loss: 0.00598194170743227, LR: 0.0001\n",
            "Step 856/1000, Loss: 0.005963760893791914, LR: 0.0001\n",
            "Step 857/1000, Loss: 0.005946979857981205, LR: 0.0001\n",
            "Step 858/1000, Loss: 0.005930349230766296, LR: 0.0001\n",
            "Step 859/1000, Loss: 0.0059133851900696754, LR: 0.0001\n",
            "Step 860/1000, Loss: 0.005896122194826603, LR: 0.0001\n",
            "Step 861/1000, Loss: 0.005879182368516922, LR: 0.0001\n",
            "Step 862/1000, Loss: 0.005862336605787277, LR: 0.0001\n",
            "Step 863/1000, Loss: 0.0058457693085074425, LR: 0.0001\n",
            "Step 864/1000, Loss: 0.005828892812132835, LR: 0.0001\n",
            "Step 865/1000, Loss: 0.005812444724142551, LR: 0.0001\n",
            "Step 866/1000, Loss: 0.005797059275209904, LR: 0.0001\n",
            "Step 867/1000, Loss: 0.005875434260815382, LR: 0.0001\n",
            "Step 868/1000, Loss: 0.005909770727157593, LR: 0.0001\n",
            "Step 869/1000, Loss: 0.005759359337389469, LR: 0.0001\n",
            "Step 870/1000, Loss: 0.0057429363951087, LR: 0.0001\n",
            "Step 871/1000, Loss: 0.005729073658585548, LR: 0.0001\n",
            "Step 872/1000, Loss: 0.00571393733844161, LR: 0.0001\n",
            "Step 873/1000, Loss: 0.0056982324458658695, LR: 0.0001\n",
            "Step 874/1000, Loss: 0.005684015341103077, LR: 0.0001\n",
            "Step 875/1000, Loss: 0.005669407546520233, LR: 0.0001\n",
            "Step 876/1000, Loss: 0.00565493106842041, LR: 0.0001\n",
            "Step 877/1000, Loss: 0.005640487652271986, LR: 0.0001\n",
            "Step 878/1000, Loss: 0.005625112913548946, LR: 0.0001\n",
            "Step 879/1000, Loss: 0.0056097134947776794, LR: 0.0001\n",
            "Step 880/1000, Loss: 0.005594415124505758, LR: 0.0001\n",
            "Step 881/1000, Loss: 0.005579091142863035, LR: 0.0001\n",
            "Step 882/1000, Loss: 0.005563954822719097, LR: 0.0001\n",
            "Step 883/1000, Loss: 0.005548883695155382, LR: 0.0001\n",
            "Step 884/1000, Loss: 0.005533585324883461, LR: 0.0001\n",
            "Step 885/1000, Loss: 0.0055184317752718925, LR: 0.0001\n",
            "Step 886/1000, Loss: 0.005526943132281303, LR: 0.0001\n",
            "Step 887/1000, Loss: 0.0055147456005215645, LR: 0.0001\n",
            "Step 888/1000, Loss: 0.00550167728215456, LR: 0.0001\n",
            "Step 889/1000, Loss: 0.005514197982847691, LR: 0.0001\n",
            "Step 890/1000, Loss: 0.005448635667562485, LR: 0.0001\n",
            "Step 891/1000, Loss: 0.005436110310256481, LR: 0.0001\n",
            "Step 892/1000, Loss: 0.005424098111689091, LR: 0.0001\n",
            "Step 893/1000, Loss: 0.005414609797298908, LR: 0.0001\n",
            "Step 894/1000, Loss: 0.0053999098017811775, LR: 0.0001\n",
            "Step 895/1000, Loss: 0.005386935546994209, LR: 0.0001\n",
            "Step 896/1000, Loss: 0.005373725667595863, LR: 0.0001\n",
            "Step 897/1000, Loss: 0.0053641302511096, LR: 0.0001\n",
            "Step 898/1000, Loss: 0.005347576458007097, LR: 0.0001\n",
            "Step 899/1000, Loss: 0.00533481827005744, LR: 0.0001\n",
            "Step 900/1000, Loss: 0.005318287294358015, LR: 0.0001\n",
            "Step 901/1000, Loss: 0.005304806865751743, LR: 0.0001\n",
            "Step 902/1000, Loss: 0.00529113644734025, LR: 0.0001\n",
            "Step 903/1000, Loss: 0.005278221797198057, LR: 0.0001\n",
            "Step 904/1000, Loss: 0.005264527164399624, LR: 0.0001\n",
            "Step 905/1000, Loss: 0.005250594578683376, LR: 0.0001\n",
            "Step 906/1000, Loss: 0.005237361881881952, LR: 0.0001\n",
            "Step 907/1000, Loss: 0.005224616266787052, LR: 0.0001\n",
            "Step 908/1000, Loss: 0.00521545996889472, LR: 0.0001\n",
            "Step 909/1000, Loss: 0.005209008231759071, LR: 0.0001\n",
            "Step 910/1000, Loss: 0.005186073947697878, LR: 0.0001\n",
            "Step 911/1000, Loss: 0.005171170458197594, LR: 0.0001\n",
            "Step 912/1000, Loss: 0.005156806204468012, LR: 0.0001\n",
            "Step 913/1000, Loss: 0.005143319256603718, LR: 0.0001\n",
            "Step 914/1000, Loss: 0.00513050053268671, LR: 0.0001\n",
            "Step 915/1000, Loss: 0.00511697493493557, LR: 0.0001\n",
            "Step 916/1000, Loss: 0.0051078288815915585, LR: 0.0001\n",
            "Step 917/1000, Loss: 0.005091926082968712, LR: 0.0001\n",
            "Step 918/1000, Loss: 0.0050815800204873085, LR: 0.0001\n",
            "Step 919/1000, Loss: 0.005070338025689125, LR: 0.0001\n",
            "Step 920/1000, Loss: 0.005053011700510979, LR: 0.0001\n",
            "Step 921/1000, Loss: 0.0050390674732625484, LR: 0.0001\n",
            "Step 922/1000, Loss: 0.005026121623814106, LR: 0.0001\n",
            "Step 923/1000, Loss: 0.005011658184230328, LR: 0.0001\n",
            "Step 924/1000, Loss: 0.005003025289624929, LR: 0.0001\n",
            "Step 925/1000, Loss: 0.004990539513528347, LR: 0.0001\n",
            "Step 926/1000, Loss: 0.004974867217242718, LR: 0.0001\n",
            "Step 927/1000, Loss: 0.00496390275657177, LR: 0.0001\n",
            "Step 928/1000, Loss: 0.0049548642709851265, LR: 0.0001\n",
            "Step 929/1000, Loss: 0.004941609688103199, LR: 0.0001\n",
            "Step 930/1000, Loss: 0.004928720183670521, LR: 0.0001\n",
            "Step 931/1000, Loss: 0.004916149191558361, LR: 0.0001\n",
            "Step 932/1000, Loss: 0.004906709771603346, LR: 0.0001\n",
            "Step 933/1000, Loss: 0.004893041215837002, LR: 0.0001\n",
            "Step 934/1000, Loss: 0.004879443906247616, LR: 0.0001\n",
            "Step 935/1000, Loss: 0.00487054418772459, LR: 0.0001\n",
            "Step 936/1000, Loss: 0.004848017822951078, LR: 0.0001\n",
            "Step 937/1000, Loss: 0.004834645893424749, LR: 0.0001\n",
            "Step 938/1000, Loss: 0.004822378512471914, LR: 0.0001\n",
            "Step 939/1000, Loss: 0.004809106700122356, LR: 0.0001\n",
            "Step 940/1000, Loss: 0.0047960104420781136, LR: 0.0001\n",
            "Step 941/1000, Loss: 0.004782628268003464, LR: 0.0001\n",
            "Step 942/1000, Loss: 0.004768282175064087, LR: 0.0001\n",
            "Step 943/1000, Loss: 0.004754784516990185, LR: 0.0001\n",
            "Step 944/1000, Loss: 0.004741511773318052, LR: 0.0001\n",
            "Step 945/1000, Loss: 0.004728074185550213, LR: 0.0001\n",
            "Step 946/1000, Loss: 0.0047147138975560665, LR: 0.0001\n",
            "Step 947/1000, Loss: 0.004701375495642424, LR: 0.0001\n",
            "Step 948/1000, Loss: 0.004688064102083445, LR: 0.0001\n",
            "Step 949/1000, Loss: 0.0046744076535105705, LR: 0.0001\n",
            "Step 950/1000, Loss: 0.0046604531817138195, LR: 0.0001\n",
            "Step 951/1000, Loss: 0.004647126886993647, LR: 0.0001\n",
            "Step 952/1000, Loss: 0.004633200354874134, LR: 0.0001\n",
            "Step 953/1000, Loss: 0.004619373474270105, LR: 0.0001\n",
            "Step 954/1000, Loss: 0.004605497233569622, LR: 0.0001\n",
            "Step 955/1000, Loss: 0.00459182308986783, LR: 0.0001\n",
            "Step 956/1000, Loss: 0.004577638581395149, LR: 0.0001\n",
            "Step 957/1000, Loss: 0.004563467111438513, LR: 0.0001\n",
            "Step 958/1000, Loss: 0.004549335688352585, LR: 0.0001\n",
            "Step 959/1000, Loss: 0.004535208456218243, LR: 0.0001\n",
            "Step 960/1000, Loss: 0.004521423019468784, LR: 0.0001\n",
            "Step 961/1000, Loss: 0.004507188685238361, LR: 0.0001\n",
            "Step 962/1000, Loss: 0.00449378602206707, LR: 0.0001\n",
            "Step 963/1000, Loss: 0.004479792434722185, LR: 0.0001\n",
            "Step 964/1000, Loss: 0.00446388591080904, LR: 0.0001\n",
            "Step 965/1000, Loss: 0.004451134707778692, LR: 0.0001\n",
            "Step 966/1000, Loss: 0.004437240771949291, LR: 0.0001\n",
            "Step 967/1000, Loss: 0.0044227722100913525, LR: 0.0001\n",
            "Step 968/1000, Loss: 0.004408222157508135, LR: 0.0001\n",
            "Step 969/1000, Loss: 0.004393703304231167, LR: 0.0001\n",
            "Step 970/1000, Loss: 0.004378416109830141, LR: 0.0001\n",
            "Step 971/1000, Loss: 0.004363610874861479, LR: 0.0001\n",
            "Step 972/1000, Loss: 0.004348609130829573, LR: 0.0001\n",
            "Step 973/1000, Loss: 0.004333472810685635, LR: 0.0001\n",
            "Step 974/1000, Loss: 0.004318246617913246, LR: 0.0001\n",
            "Step 975/1000, Loss: 0.004302807617932558, LR: 0.0001\n",
            "Step 976/1000, Loss: 0.004287301562726498, LR: 0.0001\n",
            "Step 977/1000, Loss: 0.0042717475444078445, LR: 0.0001\n",
            "Step 978/1000, Loss: 0.00425606919452548, LR: 0.0001\n",
            "Step 979/1000, Loss: 0.004240184091031551, LR: 0.0001\n",
            "Step 980/1000, Loss: 0.004223822616040707, LR: 0.0001\n",
            "Step 981/1000, Loss: 0.004206301178783178, LR: 0.0001\n",
            "Step 982/1000, Loss: 0.004190454259514809, LR: 0.0001\n",
            "Step 983/1000, Loss: 0.004175176378339529, LR: 0.0001\n",
            "Step 984/1000, Loss: 0.004159011412411928, LR: 0.0001\n",
            "Step 985/1000, Loss: 0.004143113736063242, LR: 0.0001\n",
            "Step 986/1000, Loss: 0.0041270749643445015, LR: 0.0001\n",
            "Step 987/1000, Loss: 0.0041109356097877026, LR: 0.0001\n",
            "Step 988/1000, Loss: 0.004095254000276327, LR: 0.0001\n",
            "Step 989/1000, Loss: 0.004079075995832682, LR: 0.0001\n",
            "Step 990/1000, Loss: 0.004062770865857601, LR: 0.0001\n",
            "Step 991/1000, Loss: 0.00404686713591218, LR: 0.0001\n",
            "Step 992/1000, Loss: 0.004031156189739704, LR: 0.0001\n",
            "Step 993/1000, Loss: 0.004015577025711536, LR: 0.0001\n",
            "Step 994/1000, Loss: 0.004000104498118162, LR: 0.0001\n",
            "Step 995/1000, Loss: 0.003984970506280661, LR: 0.0001\n",
            "Step 996/1000, Loss: 0.003970731981098652, LR: 0.0001\n",
            "Step 997/1000, Loss: 0.0039593251422047615, LR: 0.0001\n",
            "Step 998/1000, Loss: 0.0039571551606059074, LR: 0.0001\n",
            "Step 999/1000, Loss: 0.00398876890540123, LR: 0.0001\n",
            "Step 1000/1000, Loss: 0.004170993808656931, LR: 0.0001\n"
          ]
        }
      ],
      "source": [
        "# Example config:\n",
        "batch_size = 10\n",
        "sequence_len = 128\n",
        "num_steps = 1000\n",
        "train_inputs, train_targets, _, _ = get_dataset(10, sequence_len, 0)\n",
        "config = GPTConfig(\n",
        "    vocab_size=tokenizer.n_vocab,\n",
        "    n_layer=4,   # fewer layers for a quick demo\n",
        "    n_head=4,\n",
        "    n_embd=128,\n",
        "    seq_len=sequence_len,\n",
        ")\n",
        "\n",
        "\n",
        "# Create the GPT model\n",
        "model = GPTModel(config)\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
        "\n",
        "# Define Scheduler\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',factor=0.2, patience=20, min_lr=5e-6, threshold=1e-4)\n",
        "\n",
        "# Training loop\n",
        "i = 1\n",
        "losses = []\n",
        "\n",
        "while i < num_steps:\n",
        "    for j in range(0, len(train_inputs), batch_size):\n",
        "        x = train_inputs[j:j+batch_size]\n",
        "        y = train_targets[j:j+batch_size]\n",
        "\n",
        "        # Forward pass\n",
        "        logits = model(x)\n",
        "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1))\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        losses.append(loss.item())\n",
        "        \n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "    \n",
        "\n",
        "        loss = loss.item()\n",
        "        scheduler.step(loss)\n",
        "\n",
        "   \n",
        "        # Print the average loss for the epoch\n",
        "        lr = optimizer.param_groups[0][\"lr\"]\n",
        "        print(f\"Step {i+1}/{num_steps}, Loss: {loss}, LR: {lr}\")\n",
        "\n",
        "        i += 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fa1b9abb110>]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ahora, probaremos la memorización del modelo utilizando la función `inference()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original:   director Takeshi Ozawa . A large team of writers handled the script . The game 's opening\n",
            "Predicted:  director Takeshi Ozawa . A large team of writers handled the script . The game 's opening\n"
          ]
        }
      ],
      "source": [
        "def inference(prompt, max_new_tokens):\n",
        "    tokens = tokenizer.encode(prompt)\n",
        "    for _ in range(max_new_tokens):\n",
        "        num_tokens = len(tokens)\n",
        "        tokens_padded = tokens + [tokenizer.eot_token] * (config.seq_len - num_tokens)\n",
        "        tokens_padded = torch.tensor(tokens_padded).unsqueeze(0).to(device)\n",
        "        logits = model(tokens_padded)\n",
        "        predicted_token = torch.argmax(logits[0, num_tokens-1, :]).item()\n",
        "        tokens.append(predicted_token)\n",
        "    return tokenizer.decode(tokens)\n",
        "    \n",
        "print(\"Original: \", tokenizer.decode(train_inputs[2].tolist())[:90])\n",
        "print(\"Predicted:\", inference(\" director Takeshi Ozawa . A large team of writers handled the script\", max_new_tokens=6))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Pre-entrenamiento real"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Utilizaremos ahore el *Huggingface Streaming Dataset*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load dataset in streaming mode\n",
        "ds = load_dataset(\"abisee/cnn_dailymail\", \"3.0.0\", split=\"train\")\n",
        "hf_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "def check_dataset_exists():\n",
        "    try:\n",
        "        # Attempt to load the dataset with reuse_cache_if_exists mode\n",
        "        load_dataset(\"parquet\", data_files=\"cnn_dailymail_train.parquet\", split=\"train\")\n",
        "        load_dataset(\"parquet\", data_files=\"cnn_dailymail_test.parquet\", split=\"train\")\n",
        "        return True\n",
        "    except FileNotFoundError:\n",
        "        return False\n",
        "    \n",
        "if not check_dataset_exists():\n",
        "    print(\"Tokenized dataset does not exist locally... Generating and saving to disk.\")\n",
        "\n",
        "    def tokenize_and_chunk(dataset, tokenizer, chunk_size=512, train_rows=100_000, test_rows=500):\n",
        "        \"\"\"\n",
        "        Tokenizes and chunks the dataset into fixed-length 512-token segments.\n",
        "        The 'target' sequence is shifted left by 1 token.\n",
        "        Stops after generating `train_rows + test_rows` tokenized chunks.\n",
        "        \"\"\"\n",
        "        buffer = []  # Rolling buffer for tokens\n",
        "        row_count = 0\n",
        "\n",
        "        for example in dataset:\n",
        "            tokens = tokenizer(example[\"article\"], truncation=False, padding=False)['input_ids']\n",
        "            buffer.extend(tokens)\n",
        "\n",
        "            # Yield full chunks until we reach train_rows + test_rows\n",
        "            while len(buffer) >= chunk_size + 1:  # +1 to ensure we can shift target\n",
        "                if row_count >= (train_rows + test_rows):\n",
        "                    return  # Stop yielding once enough rows are reached\n",
        "\n",
        "                # Create input-target pairs\n",
        "                input_chunk = buffer[:chunk_size]         # First 512 tokens\n",
        "                target_chunk = buffer[1:chunk_size + 1]  # Shifted by 1 token\n",
        "                \n",
        "                # Assign to train or test split\n",
        "                split = \"train\" if row_count < train_rows else \"test\"\n",
        "\n",
        "                yield {\n",
        "                    \"split\": split,\n",
        "                    \"input\": input_chunk, \n",
        "                    \"target\": target_chunk\n",
        "                }\n",
        "                \n",
        "                buffer = buffer[chunk_size:]  # Remove used tokens\n",
        "                row_count += 1\n",
        "\n",
        "    # Set the max number of rows for training and testing\n",
        "    TRAIN_ROWS = 1400000  # Adjust as needed\n",
        "    TEST_ROWS = 500   # Adjust as needed\n",
        "    CHUNK_SIZE = 128\n",
        "\n",
        "    # Convert generator to a Hugging Face Dataset\n",
        "    tokenized_ds = Dataset.from_generator(lambda: tokenize_and_chunk(ds, hf_tokenizer,chunk_size=CHUNK_SIZE, train_rows=TRAIN_ROWS, test_rows=TEST_ROWS))\n",
        "\n",
        "    # Split the dataset into `train` and `test`\n",
        "    dataset_splits = tokenized_ds.train_test_split(test_size=TEST_ROWS / (TRAIN_ROWS + TEST_ROWS), seed=42)\n",
        "\n",
        "    # Save to disk\n",
        "    dataset_splits[\"train\"].to_parquet(\"cnn_dailymail_train.parquet\")\n",
        "    dataset_splits[\"test\"].to_parquet(\"cnn_dailymail_test.parquet\")\n",
        "\n",
        "    print(f\"✅ Saved {TRAIN_ROWS} train rows and {TEST_ROWS} test rows.\")\n",
        "else:\n",
        "    print(\"Tokenized dataset already exists locally.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Utilizamos parquet para el manejo de datasets grandes, así el modelo no tendrá que tener el dataset entero en memoria."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d1130e2a017040c2a4bbad82c36ea3d0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "af6ea50988934ac08364d5596d7c7056",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 100/150000, Loss: 8.435567221641541, Test Loss: 7.553930699825287, LR: 0.0005, Elapsed Time: 10.30 seconds\n",
            "Step 200/150000, Loss: 7.54877815246582, Test Loss: 7.525772750377655, LR: 0.0005, Elapsed Time: 20.54 seconds\n",
            "Step 300/150000, Loss: 7.526817994117737, Test Loss: 7.517217397689819, LR: 0.0005, Elapsed Time: 30.79 seconds\n",
            "Step 400/150000, Loss: 7.927651295661926, Test Loss: 7.508810877799988, LR: 0.0005, Elapsed Time: 41.07 seconds\n",
            "Step 500/150000, Loss: 7.511321864128113, Test Loss: 7.487947523593903, LR: 0.0005, Elapsed Time: 51.26 seconds\n",
            "Step 600/150000, Loss: 7.476530842781067, Test Loss: 7.447194814682007, LR: 0.0005, Elapsed Time: 61.53 seconds\n",
            "Step 700/150000, Loss: 7.439467644691467, Test Loss: 7.41978245973587, LR: 0.0005, Elapsed Time: 71.85 seconds\n",
            "Step 800/150000, Loss: 7.416441283226013, Test Loss: 7.398671627044678, LR: 0.0005, Elapsed Time: 82.34 seconds\n",
            "Step 900/150000, Loss: 7.397604651451111, Test Loss: 7.382740795612335, LR: 0.0005, Elapsed Time: 92.67 seconds\n",
            "Step 1000/150000, Loss: 7.381043090820312, Test Loss: 7.369001686573029, LR: 0.0005, Elapsed Time: 103.01 seconds\n",
            "Step 1100/150000, Loss: 7.368967571258545, Test Loss: 7.357015609741211, LR: 0.0005, Elapsed Time: 113.36 seconds\n",
            "Step 1200/150000, Loss: 7.354331102371216, Test Loss: 7.345624268054962, LR: 0.0005, Elapsed Time: 123.73 seconds\n",
            "Step 1300/150000, Loss: 7.337831945419311, Test Loss: 7.3321720361709595, LR: 0.0005, Elapsed Time: 134.05 seconds\n",
            "Step 1400/150000, Loss: 7.343300309181213, Test Loss: 7.326752722263336, LR: 0.0005, Elapsed Time: 144.42 seconds\n",
            "Step 1500/150000, Loss: 7.338461394309998, Test Loss: 7.328832685947418, LR: 0.0005, Elapsed Time: 154.74 seconds\n",
            "Step 1600/150000, Loss: 7.306818609237671, Test Loss: 7.305445730686188, LR: 0.0005, Elapsed Time: 165.07 seconds\n",
            "Step 1700/150000, Loss: 7.306557531356812, Test Loss: 7.295311152935028, LR: 0.0005, Elapsed Time: 175.42 seconds\n",
            "Step 1800/150000, Loss: 7.298436379432678, Test Loss: 7.294580817222595, LR: 0.0005, Elapsed Time: 185.85 seconds\n",
            "Step 1900/150000, Loss: 7.286991534233093, Test Loss: 7.286538183689117, LR: 0.0005, Elapsed Time: 196.23 seconds\n",
            "Step 2000/150000, Loss: 7.289896626472473, Test Loss: 7.27922135591507, LR: 0.0005, Elapsed Time: 206.66 seconds\n",
            "Step 2100/150000, Loss: 7.288860077857971, Test Loss: 7.266847491264343, LR: 0.0005, Elapsed Time: 217.04 seconds\n",
            "Step 2200/150000, Loss: 7.269688520431519, Test Loss: 7.265383839607239, LR: 0.0005, Elapsed Time: 227.58 seconds\n",
            "Step 2300/150000, Loss: 7.268224635124207, Test Loss: 7.26043838262558, LR: 0.0005, Elapsed Time: 238.06 seconds\n",
            "Step 2400/150000, Loss: 7.265768866539002, Test Loss: 7.2547767162323, LR: 0.0005, Elapsed Time: 248.47 seconds\n",
            "Step 2500/150000, Loss: 7.252669172286987, Test Loss: 7.244467198848724, LR: 0.0005, Elapsed Time: 258.87 seconds\n",
            "Step 2600/150000, Loss: 7.247338519096375, Test Loss: 7.229368269443512, LR: 0.0005, Elapsed Time: 269.35 seconds\n",
            "Step 2700/150000, Loss: 7.224129657745362, Test Loss: 7.234573543071747, LR: 0.0005, Elapsed Time: 279.78 seconds\n",
            "Step 2800/150000, Loss: 7.227110447883606, Test Loss: 7.229214012622833, LR: 0.0005, Elapsed Time: 290.18 seconds\n",
            "Step 2900/150000, Loss: 7.22124216556549, Test Loss: 7.2122557163238525, LR: 0.0005, Elapsed Time: 300.61 seconds\n",
            "Step 3000/150000, Loss: 7.213673672676086, Test Loss: 7.206285536289215, LR: 0.0005, Elapsed Time: 311.10 seconds\n",
            "Step 3100/150000, Loss: 7.21203209400177, Test Loss: 7.211934864521027, LR: 0.0005, Elapsed Time: 321.52 seconds\n",
            "Step 3200/150000, Loss: 7.206606884002685, Test Loss: 7.210776209831238, LR: 0.0005, Elapsed Time: 331.95 seconds\n",
            "Step 3300/150000, Loss: 7.1979950284957885, Test Loss: 7.1861525774002075, LR: 0.0005, Elapsed Time: 342.38 seconds\n",
            "Step 3400/150000, Loss: 7.186545124053955, Test Loss: 7.177075028419495, LR: 0.0005, Elapsed Time: 352.92 seconds\n",
            "Step 3500/150000, Loss: 7.181328716278077, Test Loss: 7.166107714176178, LR: 0.0005, Elapsed Time: 363.44 seconds\n",
            "Step 3600/150000, Loss: 7.175849142074585, Test Loss: 7.15721732378006, LR: 0.0005, Elapsed Time: 373.86 seconds\n",
            "Step 3700/150000, Loss: 7.16748969078064, Test Loss: 7.151500761508942, LR: 0.0005, Elapsed Time: 384.29 seconds\n",
            "Step 3800/150000, Loss: 7.164966082572937, Test Loss: 7.1530749797821045, LR: 0.0005, Elapsed Time: 394.70 seconds\n",
            "Step 3900/150000, Loss: 7.158694138526917, Test Loss: 7.147201478481293, LR: 0.0005, Elapsed Time: 405.15 seconds\n",
            "Step 4000/150000, Loss: 7.153739581108093, Test Loss: 7.136781811714172, LR: 0.0005, Elapsed Time: 415.59 seconds\n",
            "Step 4100/150000, Loss: 7.134174795150757, Test Loss: 7.138992071151733, LR: 0.0005, Elapsed Time: 426.00 seconds\n",
            "Step 4200/150000, Loss: 7.139421229362488, Test Loss: 7.132061719894409, LR: 0.0005, Elapsed Time: 436.42 seconds\n",
            "Step 4300/150000, Loss: 7.138796529769897, Test Loss: 7.123697400093079, LR: 0.0005, Elapsed Time: 446.88 seconds\n",
            "Step 4400/150000, Loss: 7.117022309303284, Test Loss: 7.108416318893433, LR: 0.0005, Elapsed Time: 457.45 seconds\n",
            "Step 4500/150000, Loss: 7.113546361923218, Test Loss: 7.103397786617279, LR: 0.0005, Elapsed Time: 467.83 seconds\n",
            "Step 4600/150000, Loss: 7.107618660926819, Test Loss: 7.095369100570679, LR: 0.0005, Elapsed Time: 478.26 seconds\n",
            "Step 4700/150000, Loss: 7.093686423301697, Test Loss: 7.085271537303925, LR: 0.0005, Elapsed Time: 488.74 seconds\n",
            "Step 4800/150000, Loss: 7.096328368186951, Test Loss: 7.095501601696014, LR: 0.0005, Elapsed Time: 499.20 seconds\n",
            "Step 4900/150000, Loss: 7.082043323516846, Test Loss: 7.0681822299957275, LR: 0.0005, Elapsed Time: 509.65 seconds\n",
            "Step 5000/150000, Loss: 7.071612229347229, Test Loss: 7.077954411506653, LR: 0.0005, Elapsed Time: 520.12 seconds\n",
            "Step 5100/150000, Loss: 7.069280514717102, Test Loss: 7.064966022968292, LR: 0.0005, Elapsed Time: 530.53 seconds\n",
            "Step 5200/150000, Loss: 7.064275426864624, Test Loss: 7.0552626848220825, LR: 0.0005, Elapsed Time: 540.94 seconds\n",
            "Step 5300/150000, Loss: 7.052716789245605, Test Loss: 7.0463568568229675, LR: 0.0005, Elapsed Time: 551.43 seconds\n",
            "Step 5400/150000, Loss: 7.049996829032898, Test Loss: 7.032907068729401, LR: 0.0005, Elapsed Time: 561.89 seconds\n",
            "Step 5500/150000, Loss: 7.043382434844971, Test Loss: 7.028134822845459, LR: 0.0005, Elapsed Time: 572.30 seconds\n",
            "Step 5600/150000, Loss: 7.050579061508179, Test Loss: 7.027352869510651, LR: 0.0005, Elapsed Time: 582.75 seconds\n",
            "Step 5700/150000, Loss: 7.0176494646072385, Test Loss: 7.014744281768799, LR: 0.0005, Elapsed Time: 593.21 seconds\n",
            "Step 5800/150000, Loss: 7.012702417373657, Test Loss: 7.006887197494507, LR: 0.0005, Elapsed Time: 603.68 seconds\n",
            "Step 5900/150000, Loss: 7.013630666732788, Test Loss: 7.013012230396271, LR: 0.0005, Elapsed Time: 614.14 seconds\n",
            "Step 6000/150000, Loss: 7.0234130859375, Test Loss: 7.0039860010147095, LR: 0.0005, Elapsed Time: 624.67 seconds\n",
            "Step 6100/150000, Loss: 7.007867722511292, Test Loss: 6.993161916732788, LR: 0.0005, Elapsed Time: 635.19 seconds\n",
            "Step 6200/150000, Loss: 6.99984989643097, Test Loss: 6.989144802093506, LR: 0.0005, Elapsed Time: 645.78 seconds\n",
            "Step 6300/150000, Loss: 6.992976989746094, Test Loss: 6.977555692195892, LR: 0.0005, Elapsed Time: 656.37 seconds\n",
            "Step 6400/150000, Loss: 6.9823104810714725, Test Loss: 6.978590250015259, LR: 0.0005, Elapsed Time: 666.95 seconds\n",
            "Step 6500/150000, Loss: 6.979199652671814, Test Loss: 6.969618499279022, LR: 0.0005, Elapsed Time: 677.51 seconds\n",
            "Step 6600/150000, Loss: 6.977144961357117, Test Loss: 6.964677274227142, LR: 0.0005, Elapsed Time: 688.04 seconds\n",
            "Step 6700/150000, Loss: 6.9690747785568234, Test Loss: 6.9579432010650635, LR: 0.0005, Elapsed Time: 698.60 seconds\n",
            "Step 6800/150000, Loss: 6.968114175796509, Test Loss: 6.954050719738007, LR: 0.0005, Elapsed Time: 709.07 seconds\n",
            "Step 6900/150000, Loss: 6.964566860198975, Test Loss: 6.952076077461243, LR: 0.0005, Elapsed Time: 719.56 seconds\n",
            "Step 7000/150000, Loss: 6.958062806129456, Test Loss: 6.95401918888092, LR: 0.0005, Elapsed Time: 730.07 seconds\n",
            "Step 7100/150000, Loss: 6.946221036911011, Test Loss: 6.945705533027649, LR: 0.0005, Elapsed Time: 740.59 seconds\n",
            "Step 7200/150000, Loss: 6.935234818458557, Test Loss: 6.943190574645996, LR: 0.0005, Elapsed Time: 751.07 seconds\n",
            "Step 7300/150000, Loss: 6.942540864944458, Test Loss: 6.9327269196510315, LR: 0.0005, Elapsed Time: 761.65 seconds\n",
            "Step 7400/150000, Loss: 6.9417256212234495, Test Loss: 6.940238654613495, LR: 0.0005, Elapsed Time: 772.21 seconds\n",
            "Step 7500/150000, Loss: 6.929655418395996, Test Loss: 6.916813850402832, LR: 0.0005, Elapsed Time: 782.88 seconds\n",
            "Step 7600/150000, Loss: 6.923329024314881, Test Loss: 6.914795935153961, LR: 0.0005, Elapsed Time: 793.46 seconds\n",
            "Step 7700/150000, Loss: 6.9257378578186035, Test Loss: 6.905551373958588, LR: 0.0005, Elapsed Time: 804.06 seconds\n",
            "Step 7800/150000, Loss: 6.917951383590698, Test Loss: 6.909545838832855, LR: 0.0005, Elapsed Time: 814.62 seconds\n",
            "Step 7900/150000, Loss: 6.917766194343567, Test Loss: 6.899311721324921, LR: 0.0005, Elapsed Time: 825.18 seconds\n",
            "Step 8000/150000, Loss: 6.900511713027954, Test Loss: 6.904193639755249, LR: 0.0005, Elapsed Time: 835.73 seconds\n",
            "Step 8100/150000, Loss: 6.901459703445434, Test Loss: 6.884034514427185, LR: 0.0005, Elapsed Time: 846.26 seconds\n",
            "Step 8200/150000, Loss: 6.8871054935455325, Test Loss: 6.880636274814606, LR: 0.0005, Elapsed Time: 856.79 seconds\n",
            "Step 8300/150000, Loss: 6.894446053504944, Test Loss: 6.883534610271454, LR: 0.0005, Elapsed Time: 867.33 seconds\n",
            "Step 8400/150000, Loss: 6.8837585496902465, Test Loss: 6.8744590282440186, LR: 0.0005, Elapsed Time: 877.87 seconds\n",
            "Step 8500/150000, Loss: 6.887346677780151, Test Loss: 6.874442636966705, LR: 0.0005, Elapsed Time: 888.45 seconds\n",
            "Step 8600/150000, Loss: 6.866189632415772, Test Loss: 6.867535948753357, LR: 0.0005, Elapsed Time: 899.02 seconds\n",
            "Step 8700/150000, Loss: 6.858483424186707, Test Loss: 6.863547086715698, LR: 0.0005, Elapsed Time: 909.62 seconds\n",
            "Step 8800/150000, Loss: 6.853890800476075, Test Loss: 6.860527515411377, LR: 0.0005, Elapsed Time: 920.18 seconds\n",
            "Step 8900/150000, Loss: 6.86736876487732, Test Loss: 6.852349758148193, LR: 0.0005, Elapsed Time: 930.75 seconds\n",
            "Step 9000/150000, Loss: 6.854738512039185, Test Loss: 6.851053953170776, LR: 0.0005, Elapsed Time: 941.37 seconds\n",
            "Step 9100/150000, Loss: 6.843123269081116, Test Loss: 6.843343198299408, LR: 0.0005, Elapsed Time: 951.94 seconds\n",
            "Step 9200/150000, Loss: 6.848175349235535, Test Loss: 6.842785656452179, LR: 0.0005, Elapsed Time: 962.48 seconds\n",
            "Step 9300/150000, Loss: 6.836861019134521, Test Loss: 6.826335310935974, LR: 0.0005, Elapsed Time: 973.04 seconds\n",
            "Step 9400/150000, Loss: 6.828686275482178, Test Loss: 6.836514174938202, LR: 0.0005, Elapsed Time: 983.58 seconds\n",
            "Step 9500/150000, Loss: 6.825911259651184, Test Loss: 6.8181822299957275, LR: 0.0005, Elapsed Time: 994.15 seconds\n",
            "Step 9600/150000, Loss: 6.830202875137329, Test Loss: 6.815926432609558, LR: 0.0005, Elapsed Time: 1004.70 seconds\n",
            "Step 9700/150000, Loss: 6.8186551952362064, Test Loss: 6.811767995357513, LR: 0.0005, Elapsed Time: 1015.26 seconds\n",
            "Step 9800/150000, Loss: 6.804091877937317, Test Loss: 6.8131250739097595, LR: 0.0005, Elapsed Time: 1025.77 seconds\n",
            "Step 9900/150000, Loss: 6.819313902854919, Test Loss: 6.799443066120148, LR: 0.0005, Elapsed Time: 1036.32 seconds\n",
            "Step 10000/150000, Loss: 6.79480315208435, Test Loss: 6.7904172539711, LR: 0.0005, Elapsed Time: 1046.81 seconds\n",
            "Step 10100/150000, Loss: 6.804099850654602, Test Loss: 6.789290547370911, LR: 0.0005, Elapsed Time: 1057.33 seconds\n",
            "Step 10200/150000, Loss: 6.795985345840454, Test Loss: 6.779886364936829, LR: 0.0005, Elapsed Time: 1067.85 seconds\n",
            "Step 10300/150000, Loss: 6.786369271278382, Test Loss: 6.783883690834045, LR: 0.0005, Elapsed Time: 1078.46 seconds\n",
            "Step 10400/150000, Loss: 6.788990592956543, Test Loss: 6.771858096122742, LR: 0.0005, Elapsed Time: 1089.02 seconds\n",
            "Step 10500/150000, Loss: 6.76845308303833, Test Loss: 6.764224350452423, LR: 0.0005, Elapsed Time: 1099.56 seconds\n",
            "Step 10600/150000, Loss: 6.781026287078857, Test Loss: 6.759154796600342, LR: 0.0005, Elapsed Time: 1110.07 seconds\n",
            "Step 10700/150000, Loss: 6.754421992301941, Test Loss: 6.752796113491058, LR: 0.0005, Elapsed Time: 1120.57 seconds\n",
            "Step 10800/150000, Loss: 6.76339587688446, Test Loss: 6.746412396430969, LR: 0.0005, Elapsed Time: 1131.09 seconds\n",
            "Step 10900/150000, Loss: 6.757268538475037, Test Loss: 6.744214475154877, LR: 0.0005, Elapsed Time: 1141.67 seconds\n",
            "Step 11000/150000, Loss: 6.755900716781616, Test Loss: 6.744207978248596, LR: 0.0005, Elapsed Time: 1152.16 seconds\n",
            "Step 11100/150000, Loss: 6.73089708328247, Test Loss: 6.746663868427277, LR: 0.0005, Elapsed Time: 1162.66 seconds\n",
            "Step 11200/150000, Loss: 6.739979648590088, Test Loss: 6.725337088108063, LR: 0.0005, Elapsed Time: 1173.25 seconds\n",
            "Step 11300/150000, Loss: 6.740782465934753, Test Loss: 6.722867488861084, LR: 0.0005, Elapsed Time: 1183.77 seconds\n",
            "Step 11400/150000, Loss: 6.722141880989074, Test Loss: 6.720248222351074, LR: 0.0005, Elapsed Time: 1194.29 seconds\n",
            "Step 11500/150000, Loss: 6.730128712654114, Test Loss: 6.714776039123535, LR: 0.0005, Elapsed Time: 1204.81 seconds\n",
            "Step 11600/150000, Loss: 6.7219167852401736, Test Loss: 6.703527808189392, LR: 0.0005, Elapsed Time: 1215.32 seconds\n",
            "Step 11700/150000, Loss: 6.703344078063965, Test Loss: 6.717762649059296, LR: 0.0005, Elapsed Time: 1225.86 seconds\n",
            "Step 11800/150000, Loss: 6.713574757575989, Test Loss: 6.7081698179244995, LR: 0.0005, Elapsed Time: 1236.49 seconds\n",
            "Step 11900/150000, Loss: 6.698301086425781, Test Loss: 6.696203947067261, LR: 0.0005, Elapsed Time: 1247.22 seconds\n",
            "Step 12000/150000, Loss: 6.708526597023011, Test Loss: 6.694559156894684, LR: 0.0005, Elapsed Time: 1257.91 seconds\n",
            "Step 12100/150000, Loss: 6.7048102426528935, Test Loss: 6.681866943836212, LR: 0.0005, Elapsed Time: 1268.63 seconds\n",
            "Step 12200/150000, Loss: 6.699475960731506, Test Loss: 6.684414029121399, LR: 0.0005, Elapsed Time: 1279.21 seconds\n",
            "Step 12300/150000, Loss: 6.6868267297744755, Test Loss: 6.6754156947135925, LR: 0.0005, Elapsed Time: 1289.85 seconds\n",
            "Step 12400/150000, Loss: 6.674562578201294, Test Loss: 6.684077084064484, LR: 0.0005, Elapsed Time: 1300.45 seconds\n",
            "Step 12500/150000, Loss: 6.677119479179383, Test Loss: 6.676577389240265, LR: 0.0005, Elapsed Time: 1311.00 seconds\n",
            "Step 12600/150000, Loss: 6.681392693519593, Test Loss: 6.66412079334259, LR: 0.0005, Elapsed Time: 1321.62 seconds\n",
            "Step 12700/150000, Loss: 6.6648876953125, Test Loss: 6.663511395454407, LR: 0.0005, Elapsed Time: 1332.17 seconds\n",
            "Step 12800/150000, Loss: 6.67307677268982, Test Loss: 6.649197101593018, LR: 0.0005, Elapsed Time: 1342.71 seconds\n",
            "Step 12900/150000, Loss: 6.657832083702087, Test Loss: 6.650216281414032, LR: 0.0005, Elapsed Time: 1353.24 seconds\n",
            "Step 13000/150000, Loss: 6.652597069740295, Test Loss: 6.639411389827728, LR: 0.0005, Elapsed Time: 1363.83 seconds\n",
            "Step 13100/150000, Loss: 6.637246680259705, Test Loss: 6.639971971511841, LR: 0.0005, Elapsed Time: 1374.45 seconds\n",
            "Step 13200/150000, Loss: 6.634117999076843, Test Loss: 6.633730053901672, LR: 0.0005, Elapsed Time: 1385.05 seconds\n",
            "Step 13300/150000, Loss: 6.647670211791993, Test Loss: 6.628168046474457, LR: 0.0005, Elapsed Time: 1395.65 seconds\n",
            "Step 13400/150000, Loss: 6.627514114379883, Test Loss: 6.628120243549347, LR: 0.0005, Elapsed Time: 1406.25 seconds\n",
            "Step 13500/150000, Loss: 6.628901286125183, Test Loss: 6.615938603878021, LR: 0.0005, Elapsed Time: 1416.90 seconds\n",
            "Step 13600/150000, Loss: 6.624326753616333, Test Loss: 6.6214759349823, LR: 0.0005, Elapsed Time: 1427.47 seconds\n",
            "Step 13700/150000, Loss: 6.61751268863678, Test Loss: 6.609710097312927, LR: 0.0005, Elapsed Time: 1438.07 seconds\n",
            "Step 13800/150000, Loss: 6.602189602851868, Test Loss: 6.608081936836243, LR: 0.0005, Elapsed Time: 1448.62 seconds\n",
            "Step 13900/150000, Loss: 6.607090282440185, Test Loss: 6.609498202800751, LR: 0.0005, Elapsed Time: 1459.25 seconds\n",
            "Step 14000/150000, Loss: 6.607324085235596, Test Loss: 6.5987125635147095, LR: 0.0005, Elapsed Time: 1469.85 seconds\n",
            "Step 14100/150000, Loss: 6.602432012557983, Test Loss: 6.596570730209351, LR: 0.0005, Elapsed Time: 1480.42 seconds\n",
            "Step 14200/150000, Loss: 6.600292644500732, Test Loss: 6.5954506397247314, LR: 0.0005, Elapsed Time: 1491.04 seconds\n",
            "Step 14300/150000, Loss: 6.601419410705566, Test Loss: 6.5890308022499084, LR: 0.0005, Elapsed Time: 1501.61 seconds\n",
            "Step 14400/150000, Loss: 6.588587336540222, Test Loss: 6.580264329910278, LR: 0.0005, Elapsed Time: 1512.16 seconds\n",
            "Step 14500/150000, Loss: 6.590622005462646, Test Loss: 6.5771055817604065, LR: 0.0005, Elapsed Time: 1522.75 seconds\n",
            "Step 14600/150000, Loss: 6.5810211515426635, Test Loss: 6.572095036506653, LR: 0.0005, Elapsed Time: 1533.35 seconds\n",
            "Step 14700/150000, Loss: 6.5652279281616215, Test Loss: 6.570411503314972, LR: 0.0005, Elapsed Time: 1543.92 seconds\n",
            "Step 14800/150000, Loss: 6.574029922485352, Test Loss: 6.574530601501465, LR: 0.0005, Elapsed Time: 1554.50 seconds\n",
            "Step 14900/150000, Loss: 6.571135225296021, Test Loss: 6.566188871860504, LR: 0.0005, Elapsed Time: 1565.05 seconds\n",
            "Step 15000/150000, Loss: 6.569562268257141, Test Loss: 6.557141661643982, LR: 0.0005, Elapsed Time: 1575.60 seconds\n",
            "Step 15100/150000, Loss: 6.570797562599182, Test Loss: 6.552781701087952, LR: 0.0005, Elapsed Time: 1586.19 seconds\n",
            "Step 15200/150000, Loss: 6.5492801141738894, Test Loss: 6.549380660057068, LR: 0.0005, Elapsed Time: 1596.80 seconds\n",
            "Step 15300/150000, Loss: 6.544323568344116, Test Loss: 6.55324387550354, LR: 0.0005, Elapsed Time: 1607.39 seconds\n",
            "Step 15400/150000, Loss: 6.536939973831177, Test Loss: 6.5497618317604065, LR: 0.0005, Elapsed Time: 1617.96 seconds\n",
            "Step 15500/150000, Loss: 6.56450659275055, Test Loss: 6.541414201259613, LR: 0.0005, Elapsed Time: 1628.67 seconds\n",
            "Step 15600/150000, Loss: 6.526670250892639, Test Loss: 6.537434995174408, LR: 0.0005, Elapsed Time: 1639.34 seconds\n",
            "Step 15700/150000, Loss: 6.55069571018219, Test Loss: 6.534805953502655, LR: 0.0005, Elapsed Time: 1649.93 seconds\n",
            "Step 15800/150000, Loss: 6.540039100646973, Test Loss: 6.5309025049209595, LR: 0.0005, Elapsed Time: 1660.56 seconds\n",
            "Step 15900/150000, Loss: 6.528563408851624, Test Loss: 6.521130383014679, LR: 0.0005, Elapsed Time: 1671.19 seconds\n",
            "Step 16000/150000, Loss: 6.50876401424408, Test Loss: 6.522227466106415, LR: 0.0005, Elapsed Time: 1681.80 seconds\n",
            "Step 16100/150000, Loss: 6.5254049968719485, Test Loss: 6.511383473873138, LR: 0.0005, Elapsed Time: 1692.36 seconds\n",
            "Step 16200/150000, Loss: 6.517147054672241, Test Loss: 6.512513339519501, LR: 0.0005, Elapsed Time: 1702.92 seconds\n",
            "Step 16300/150000, Loss: 6.5192236232757566, Test Loss: 6.501937687397003, LR: 0.0005, Elapsed Time: 1713.53 seconds\n",
            "Step 16400/150000, Loss: 6.51023494720459, Test Loss: 6.498667001724243, LR: 0.0005, Elapsed Time: 1724.12 seconds\n",
            "Step 16500/150000, Loss: 6.502197513580322, Test Loss: 6.500382721424103, LR: 0.0005, Elapsed Time: 1734.70 seconds\n",
            "Step 16600/150000, Loss: 6.51261917591095, Test Loss: 6.49462890625, LR: 0.0005, Elapsed Time: 1745.35 seconds\n",
            "Step 16700/150000, Loss: 6.508185267448425, Test Loss: 6.498956382274628, LR: 0.0005, Elapsed Time: 1755.96 seconds\n",
            "Step 16800/150000, Loss: 6.499858722686768, Test Loss: 6.488281190395355, LR: 0.0005, Elapsed Time: 1766.56 seconds\n",
            "Step 16900/150000, Loss: 6.493766951560974, Test Loss: 6.489078462123871, LR: 0.0005, Elapsed Time: 1777.11 seconds\n",
            "Step 17000/150000, Loss: 6.487625350952149, Test Loss: 6.484674572944641, LR: 0.0005, Elapsed Time: 1787.70 seconds\n",
            "Step 17100/150000, Loss: 6.482082872390747, Test Loss: 6.4762014746665955, LR: 0.0005, Elapsed Time: 1798.32 seconds\n",
            "Step 17200/150000, Loss: 6.49480700969696, Test Loss: 6.476258873939514, LR: 0.0005, Elapsed Time: 1808.87 seconds\n",
            "Step 17300/150000, Loss: 6.480599088668823, Test Loss: 6.480451822280884, LR: 0.0005, Elapsed Time: 1819.47 seconds\n",
            "Step 17400/150000, Loss: 6.491000347137451, Test Loss: 6.466275691986084, LR: 0.0005, Elapsed Time: 1830.02 seconds\n",
            "Step 17500/150000, Loss: 6.4494054412841795, Test Loss: 6.463012278079987, LR: 0.0005, Elapsed Time: 1840.69 seconds\n",
            "Step 17600/150000, Loss: 6.464988780021668, Test Loss: 6.454334020614624, LR: 0.0005, Elapsed Time: 1851.28 seconds\n",
            "Step 17700/150000, Loss: 6.460893087387085, Test Loss: 6.4548065066337585, LR: 0.0005, Elapsed Time: 1861.88 seconds\n",
            "Step 17800/150000, Loss: 6.470332183837891, Test Loss: 6.456714153289795, LR: 0.0005, Elapsed Time: 1872.48 seconds\n",
            "Step 17900/150000, Loss: 6.466375818252564, Test Loss: 6.449868381023407, LR: 0.0005, Elapsed Time: 1883.02 seconds\n",
            "Step 18000/150000, Loss: 6.454435615539551, Test Loss: 6.457311153411865, LR: 0.0005, Elapsed Time: 1893.59 seconds\n",
            "Step 18100/150000, Loss: 6.448432216644287, Test Loss: 6.449561774730682, LR: 0.0005, Elapsed Time: 1904.21 seconds\n",
            "Step 18200/150000, Loss: 6.450870785713196, Test Loss: 6.440909206867218, LR: 0.0005, Elapsed Time: 1914.87 seconds\n",
            "Step 18300/150000, Loss: 6.434606690406799, Test Loss: 6.439033031463623, LR: 0.0005, Elapsed Time: 1925.53 seconds\n",
            "Step 18400/150000, Loss: 6.438348422050476, Test Loss: 6.435120463371277, LR: 0.0005, Elapsed Time: 1936.11 seconds\n",
            "Step 18500/150000, Loss: 6.444020161628723, Test Loss: 6.433395802974701, LR: 0.0005, Elapsed Time: 1946.72 seconds\n",
            "Step 18600/150000, Loss: 6.443592319488525, Test Loss: 6.434479475021362, LR: 0.0005, Elapsed Time: 1957.27 seconds\n",
            "Step 18700/150000, Loss: 6.4349985027313235, Test Loss: 6.425047874450684, LR: 0.0005, Elapsed Time: 1967.87 seconds\n",
            "Step 18800/150000, Loss: 6.432713174819947, Test Loss: 6.418551445007324, LR: 0.0005, Elapsed Time: 1978.46 seconds\n",
            "Step 18900/150000, Loss: 6.429965615272522, Test Loss: 6.420734524726868, LR: 0.0005, Elapsed Time: 1989.03 seconds\n",
            "Step 19000/150000, Loss: 6.423684306144715, Test Loss: 6.4192734360694885, LR: 0.0005, Elapsed Time: 1999.64 seconds\n",
            "Step 19100/150000, Loss: 6.4195552253723145, Test Loss: 6.414090812206268, LR: 0.0005, Elapsed Time: 2010.22 seconds\n",
            "Step 19200/150000, Loss: 6.405357303619385, Test Loss: 6.405922114849091, LR: 0.0005, Elapsed Time: 2020.91 seconds\n",
            "Step 19300/150000, Loss: 6.404822511672974, Test Loss: 6.405009627342224, LR: 0.0005, Elapsed Time: 2031.55 seconds\n",
            "Step 19400/150000, Loss: 6.415255007743835, Test Loss: 6.402331709861755, LR: 0.0005, Elapsed Time: 2042.16 seconds\n",
            "Step 19500/150000, Loss: 6.405190634727478, Test Loss: 6.404933333396912, LR: 0.0005, Elapsed Time: 2052.77 seconds\n",
            "Step 19600/150000, Loss: 6.399961256980896, Test Loss: 6.394679009914398, LR: 0.0005, Elapsed Time: 2063.40 seconds\n",
            "Step 19700/150000, Loss: 6.394190077781677, Test Loss: 6.393055558204651, LR: 0.0005, Elapsed Time: 2073.99 seconds\n",
            "Step 19800/150000, Loss: 6.414848728179932, Test Loss: 6.395237922668457, LR: 0.0005, Elapsed Time: 2084.64 seconds\n",
            "Step 19900/150000, Loss: 6.3962875080108645, Test Loss: 6.386175692081451, LR: 0.0005, Elapsed Time: 2095.30 seconds\n",
            "Step 20000/150000, Loss: 6.402195482254029, Test Loss: 6.379953682422638, LR: 0.0005, Elapsed Time: 2105.93 seconds\n",
            "Step 20100/150000, Loss: 6.387098479270935, Test Loss: 6.375067710876465, LR: 0.0005, Elapsed Time: 2116.58 seconds\n",
            "Step 20200/150000, Loss: 6.380781197547913, Test Loss: 6.377723157405853, LR: 0.0005, Elapsed Time: 2127.24 seconds\n",
            "Step 20300/150000, Loss: 6.374047350883484, Test Loss: 6.375344514846802, LR: 0.0005, Elapsed Time: 2137.88 seconds\n",
            "Step 20400/150000, Loss: 6.374481873512268, Test Loss: 6.3679733872413635, LR: 0.0005, Elapsed Time: 2148.49 seconds\n",
            "Step 20500/150000, Loss: 6.377660479545593, Test Loss: 6.3735891580581665, LR: 0.0005, Elapsed Time: 2159.16 seconds\n",
            "Step 20600/150000, Loss: 6.367081460952758, Test Loss: 6.3758509159088135, LR: 0.0005, Elapsed Time: 2169.81 seconds\n",
            "Step 20700/150000, Loss: 6.3774712896347046, Test Loss: 6.365654647350311, LR: 0.0005, Elapsed Time: 2180.51 seconds\n",
            "Step 20800/150000, Loss: 6.360362243652344, Test Loss: 6.357201159000397, LR: 0.0005, Elapsed Time: 2191.18 seconds\n",
            "Step 20900/150000, Loss: 6.3589804744720455, Test Loss: 6.362372040748596, LR: 0.0005, Elapsed Time: 2201.84 seconds\n",
            "Step 21000/150000, Loss: 6.369034671783448, Test Loss: 6.358105301856995, LR: 0.0005, Elapsed Time: 2212.56 seconds\n",
            "Step 21100/150000, Loss: 6.354185891151428, Test Loss: 6.350670397281647, LR: 0.0005, Elapsed Time: 2223.20 seconds\n",
            "Step 21200/150000, Loss: 6.357424311637878, Test Loss: 6.3464654088020325, LR: 0.0005, Elapsed Time: 2233.78 seconds\n",
            "Step 21300/150000, Loss: 6.35779791355133, Test Loss: 6.344950079917908, LR: 0.0005, Elapsed Time: 2244.50 seconds\n",
            "Step 21400/150000, Loss: 6.347241988182068, Test Loss: 6.336641013622284, LR: 0.0005, Elapsed Time: 2255.18 seconds\n",
            "Step 21500/150000, Loss: 6.358279576301575, Test Loss: 6.335911929607391, LR: 0.0005, Elapsed Time: 2265.80 seconds\n",
            "Step 21600/150000, Loss: 6.335744690895081, Test Loss: 6.334254264831543, LR: 0.0005, Elapsed Time: 2276.45 seconds\n",
            "Step 21700/150000, Loss: 6.348782386779785, Test Loss: 6.328651070594788, LR: 0.0005, Elapsed Time: 2287.12 seconds\n",
            "Step 21800/150000, Loss: 6.321436200141907, Test Loss: 6.326053261756897, LR: 0.0005, Elapsed Time: 2297.78 seconds\n",
            "Step 21900/150000, Loss: 6.342089877128601, Test Loss: 6.3293914794921875, LR: 0.0005, Elapsed Time: 2308.46 seconds\n",
            "Step 22000/150000, Loss: 6.314436440467834, Test Loss: 6.328100562095642, LR: 0.0005, Elapsed Time: 2318.91 seconds\n",
            "Step 22100/150000, Loss: 6.326700057983398, Test Loss: 6.319675445556641, LR: 0.0005, Elapsed Time: 2329.40 seconds\n",
            "Step 22200/150000, Loss: 6.33526002407074, Test Loss: 6.321130096912384, LR: 0.0005, Elapsed Time: 2339.83 seconds\n",
            "Step 22300/150000, Loss: 6.323133850097657, Test Loss: 6.318789720535278, LR: 0.0005, Elapsed Time: 2350.29 seconds\n",
            "Step 22400/150000, Loss: 6.3232031393051145, Test Loss: 6.313169062137604, LR: 0.0005, Elapsed Time: 2360.76 seconds\n",
            "Step 22500/150000, Loss: 6.315156741142273, Test Loss: 6.309141397476196, LR: 0.0005, Elapsed Time: 2371.16 seconds\n",
            "Step 22600/150000, Loss: 6.316521048545837, Test Loss: 6.305017113685608, LR: 0.0005, Elapsed Time: 2381.57 seconds\n",
            "Step 22700/150000, Loss: 6.308063268661499, Test Loss: 6.304156482219696, LR: 0.0005, Elapsed Time: 2391.98 seconds\n",
            "Step 22800/150000, Loss: 6.302033014297486, Test Loss: 6.309248983860016, LR: 0.0005, Elapsed Time: 2402.39 seconds\n",
            "Step 22900/150000, Loss: 6.286345524787903, Test Loss: 6.30749124288559, LR: 0.0005, Elapsed Time: 2412.87 seconds\n",
            "Step 23000/150000, Loss: 6.310992612838745, Test Loss: 6.293560981750488, LR: 0.0005, Elapsed Time: 2423.31 seconds\n",
            "Step 23100/150000, Loss: 6.289297895431519, Test Loss: 6.297674119472504, LR: 0.0005, Elapsed Time: 2433.75 seconds\n",
            "Step 23200/150000, Loss: 6.288921642303467, Test Loss: 6.284840226173401, LR: 0.0005, Elapsed Time: 2444.12 seconds\n",
            "Step 23300/150000, Loss: 6.302255926132202, Test Loss: 6.294129133224487, LR: 0.0005, Elapsed Time: 2454.59 seconds\n",
            "Step 23400/150000, Loss: 6.297336287498474, Test Loss: 6.283893585205078, LR: 0.0005, Elapsed Time: 2465.03 seconds\n",
            "Step 23500/150000, Loss: 6.281531610488892, Test Loss: 6.286109745502472, LR: 0.0005, Elapsed Time: 2475.40 seconds\n",
            "Step 23600/150000, Loss: 6.301163878440857, Test Loss: 6.281963407993317, LR: 0.0005, Elapsed Time: 2485.82 seconds\n",
            "Step 23700/150000, Loss: 6.2762983036041256, Test Loss: 6.277430713176727, LR: 0.0005, Elapsed Time: 2496.27 seconds\n",
            "Step 23800/150000, Loss: 6.2828288173675535, Test Loss: 6.272550284862518, LR: 0.0005, Elapsed Time: 2506.67 seconds\n",
            "Step 23900/150000, Loss: 6.279073147773743, Test Loss: 6.274662911891937, LR: 0.0005, Elapsed Time: 2517.12 seconds\n",
            "Step 24000/150000, Loss: 6.281305069923401, Test Loss: 6.2732818722724915, LR: 0.0005, Elapsed Time: 2527.56 seconds\n",
            "Step 24100/150000, Loss: 6.263688373565674, Test Loss: 6.2711456418037415, LR: 0.0005, Elapsed Time: 2537.97 seconds\n",
            "Step 24200/150000, Loss: 6.273648619651794, Test Loss: 6.265712201595306, LR: 0.0005, Elapsed Time: 2548.40 seconds\n",
            "Step 24300/150000, Loss: 6.276871194839478, Test Loss: 6.265872597694397, LR: 0.0005, Elapsed Time: 2558.83 seconds\n",
            "Step 24400/150000, Loss: 6.2687137413024905, Test Loss: 6.261537075042725, LR: 0.0005, Elapsed Time: 2569.25 seconds\n",
            "Step 24500/150000, Loss: 6.266632571220398, Test Loss: 6.270905315876007, LR: 0.0005, Elapsed Time: 2579.65 seconds\n",
            "Step 24600/150000, Loss: 6.248898158073425, Test Loss: 6.251947641372681, LR: 0.0005, Elapsed Time: 2590.11 seconds\n",
            "Step 24700/150000, Loss: 6.246304478645325, Test Loss: 6.25252503156662, LR: 0.0005, Elapsed Time: 2600.55 seconds\n",
            "Step 24800/150000, Loss: 6.258883337974549, Test Loss: 6.247434079647064, LR: 0.0005, Elapsed Time: 2610.99 seconds\n",
            "Step 24900/150000, Loss: 6.245472912788391, Test Loss: 6.252581477165222, LR: 0.0005, Elapsed Time: 2621.45 seconds\n",
            "Step 25000/150000, Loss: 6.25300696849823, Test Loss: 6.2479347586631775, LR: 0.0005, Elapsed Time: 2631.87 seconds\n",
            "Step 25100/150000, Loss: 6.252842655181885, Test Loss: 6.243476331233978, LR: 0.0005, Elapsed Time: 2642.36 seconds\n",
            "Step 25200/150000, Loss: 6.245774707794189, Test Loss: 6.245626449584961, LR: 0.0005, Elapsed Time: 2652.78 seconds\n",
            "Step 25300/150000, Loss: 6.245088291168213, Test Loss: 6.238719463348389, LR: 0.0005, Elapsed Time: 2663.25 seconds\n",
            "Step 25400/150000, Loss: 6.247505536079407, Test Loss: 6.244243621826172, LR: 0.0005, Elapsed Time: 2673.67 seconds\n",
            "Step 25500/150000, Loss: 6.24777398109436, Test Loss: 6.232848286628723, LR: 0.0005, Elapsed Time: 2684.10 seconds\n",
            "Step 25600/150000, Loss: 6.234500923156738, Test Loss: 6.239774107933044, LR: 0.0005, Elapsed Time: 2694.58 seconds\n",
            "Step 25700/150000, Loss: 6.240865755081177, Test Loss: 6.229046821594238, LR: 0.0005, Elapsed Time: 2705.08 seconds\n",
            "Step 25800/150000, Loss: 6.227641453742981, Test Loss: 6.226473808288574, LR: 0.0005, Elapsed Time: 2715.57 seconds\n",
            "Step 25900/150000, Loss: 6.233865780830383, Test Loss: 6.221351623535156, LR: 0.0005, Elapsed Time: 2725.95 seconds\n",
            "Step 26000/150000, Loss: 6.220003499984741, Test Loss: 6.224205374717712, LR: 0.0005, Elapsed Time: 2736.40 seconds\n",
            "Step 26100/150000, Loss: 6.216693725585937, Test Loss: 6.219252705574036, LR: 0.0005, Elapsed Time: 2746.85 seconds\n",
            "Step 26200/150000, Loss: 6.230173382759094, Test Loss: 6.219891428947449, LR: 0.0005, Elapsed Time: 2757.36 seconds\n",
            "Step 26300/150000, Loss: 6.218601913452148, Test Loss: 6.2154900431633, LR: 0.0005, Elapsed Time: 2767.83 seconds\n",
            "Step 26400/150000, Loss: 6.210569849014282, Test Loss: 6.212136924266815, LR: 0.0005, Elapsed Time: 2778.23 seconds\n",
            "Step 26500/150000, Loss: 6.21944751739502, Test Loss: 6.203849673271179, LR: 0.0005, Elapsed Time: 2788.67 seconds\n",
            "Step 26600/150000, Loss: 6.210926289558411, Test Loss: 6.208175957202911, LR: 0.0005, Elapsed Time: 2799.12 seconds\n",
            "Step 26700/150000, Loss: 6.214290590286255, Test Loss: 6.203885078430176, LR: 0.0005, Elapsed Time: 2809.56 seconds\n",
            "Step 26800/150000, Loss: 6.207031335830688, Test Loss: 6.203709423542023, LR: 0.0005, Elapsed Time: 2820.00 seconds\n",
            "Step 26900/150000, Loss: 6.20112823009491, Test Loss: 6.199526906013489, LR: 0.0005, Elapsed Time: 2830.45 seconds\n",
            "Step 27000/150000, Loss: 6.195217218399048, Test Loss: 6.20100736618042, LR: 0.0005, Elapsed Time: 2840.89 seconds\n",
            "Step 27100/150000, Loss: 6.1974418306350705, Test Loss: 6.196983993053436, LR: 0.0005, Elapsed Time: 2851.34 seconds\n",
            "Step 27200/150000, Loss: 6.19078115940094, Test Loss: 6.1950324177742, LR: 0.0005, Elapsed Time: 2861.89 seconds\n",
            "Step 27300/150000, Loss: 6.19658929347992, Test Loss: 6.194707930088043, LR: 0.0005, Elapsed Time: 2872.31 seconds\n",
            "Step 27400/150000, Loss: 6.199545073509216, Test Loss: 6.192168176174164, LR: 0.0005, Elapsed Time: 2882.77 seconds\n",
            "Step 27500/150000, Loss: 6.199369215965271, Test Loss: 6.1875364780426025, LR: 0.0005, Elapsed Time: 2893.26 seconds\n",
            "Step 27600/150000, Loss: 6.185737738609314, Test Loss: 6.180055677890778, LR: 0.0005, Elapsed Time: 2903.71 seconds\n",
            "Step 27700/150000, Loss: 6.168271150588989, Test Loss: 6.181223809719086, LR: 0.0005, Elapsed Time: 2914.18 seconds\n",
            "Step 27800/150000, Loss: 6.190137052536011, Test Loss: 6.178031742572784, LR: 0.0005, Elapsed Time: 2924.62 seconds\n",
            "Step 27900/150000, Loss: 6.1977942752838135, Test Loss: 6.1742554903030396, LR: 0.0005, Elapsed Time: 2935.06 seconds\n",
            "Step 28000/150000, Loss: 6.170997138023377, Test Loss: 6.172465562820435, LR: 0.0005, Elapsed Time: 2945.51 seconds\n",
            "Step 28100/150000, Loss: 6.182390213012695, Test Loss: 6.173965871334076, LR: 0.0005, Elapsed Time: 2956.00 seconds\n",
            "Step 28200/150000, Loss: 6.176844487190246, Test Loss: 6.17498505115509, LR: 0.0005, Elapsed Time: 2966.42 seconds\n",
            "Step 28300/150000, Loss: 6.168439087867736, Test Loss: 6.165223658084869, LR: 0.0005, Elapsed Time: 2976.95 seconds\n",
            "Step 28400/150000, Loss: 6.171880202293396, Test Loss: 6.170812368392944, LR: 0.0005, Elapsed Time: 2987.50 seconds\n",
            "Step 28500/150000, Loss: 6.174358925819397, Test Loss: 6.161598026752472, LR: 0.0005, Elapsed Time: 2998.14 seconds\n",
            "Step 28600/150000, Loss: 6.157126908302307, Test Loss: 6.161474049091339, LR: 0.0005, Elapsed Time: 3008.74 seconds\n",
            "Step 28700/150000, Loss: 6.162721390724182, Test Loss: 6.163299143314362, LR: 0.0005, Elapsed Time: 3019.36 seconds\n",
            "Step 28800/150000, Loss: 6.158284425735474, Test Loss: 6.158563911914825, LR: 0.0005, Elapsed Time: 3029.87 seconds\n",
            "Step 28900/150000, Loss: 6.151708302497863, Test Loss: 6.163728952407837, LR: 0.0005, Elapsed Time: 3040.34 seconds\n",
            "Step 29000/150000, Loss: 6.149479413032532, Test Loss: 6.156359791755676, LR: 0.0005, Elapsed Time: 3050.78 seconds\n",
            "Step 29100/150000, Loss: 6.150423216819763, Test Loss: 6.153329014778137, LR: 0.0005, Elapsed Time: 3061.25 seconds\n",
            "Step 29200/150000, Loss: 6.145188775062561, Test Loss: 6.155669152736664, LR: 0.0005, Elapsed Time: 3071.68 seconds\n",
            "Step 29300/150000, Loss: 6.16031277179718, Test Loss: 6.1513184905052185, LR: 0.0005, Elapsed Time: 3082.16 seconds\n",
            "Step 29400/150000, Loss: 6.145023317337036, Test Loss: 6.144165277481079, LR: 0.0005, Elapsed Time: 3092.70 seconds\n",
            "Step 29500/150000, Loss: 6.140694913864135, Test Loss: 6.141407072544098, LR: 0.0005, Elapsed Time: 3103.17 seconds\n",
            "Step 29600/150000, Loss: 6.154821062088013, Test Loss: 6.142060399055481, LR: 0.0005, Elapsed Time: 3113.60 seconds\n",
            "Step 29700/150000, Loss: 6.133518748283386, Test Loss: 6.136374115943909, LR: 0.0005, Elapsed Time: 3124.05 seconds\n",
            "Step 29800/150000, Loss: 6.151553411483764, Test Loss: 6.137484014034271, LR: 0.0005, Elapsed Time: 3134.49 seconds\n",
            "Step 29900/150000, Loss: 6.131563773155213, Test Loss: 6.1362468004226685, LR: 0.0005, Elapsed Time: 3144.91 seconds\n",
            "Step 30000/150000, Loss: 6.137345452308654, Test Loss: 6.130531370639801, LR: 0.0005, Elapsed Time: 3155.27 seconds\n",
            "Step 30100/150000, Loss: 6.135455660820007, Test Loss: 6.135493695735931, LR: 0.0005, Elapsed Time: 3165.66 seconds\n",
            "Step 30200/150000, Loss: 6.133080949783325, Test Loss: 6.1243502497673035, LR: 0.0005, Elapsed Time: 3176.09 seconds\n",
            "Step 30300/150000, Loss: 6.135017194747925, Test Loss: 6.124425292015076, LR: 0.0005, Elapsed Time: 3186.54 seconds\n",
            "Step 30400/150000, Loss: 6.133797154426575, Test Loss: 6.12871378660202, LR: 0.0005, Elapsed Time: 3196.91 seconds\n",
            "Step 30500/150000, Loss: 6.12192367553711, Test Loss: 6.129230976104736, LR: 0.0005, Elapsed Time: 3207.38 seconds\n",
            "Step 30600/150000, Loss: 6.119800090789795, Test Loss: 6.117292582988739, LR: 0.0005, Elapsed Time: 3217.82 seconds\n",
            "Step 30700/150000, Loss: 6.1218158149719235, Test Loss: 6.116804301738739, LR: 0.0005, Elapsed Time: 3228.26 seconds\n",
            "Step 30800/150000, Loss: 6.123212518692017, Test Loss: 6.109973728656769, LR: 0.0005, Elapsed Time: 3238.70 seconds\n",
            "Step 30900/150000, Loss: 6.1166763639450075, Test Loss: 6.11478978395462, LR: 0.0005, Elapsed Time: 3249.16 seconds\n",
            "Step 31000/150000, Loss: 6.111529121398926, Test Loss: 6.111300349235535, LR: 0.0005, Elapsed Time: 3259.59 seconds\n",
            "Step 31100/150000, Loss: 6.117410712242126, Test Loss: 6.109693169593811, LR: 0.0005, Elapsed Time: 3270.03 seconds\n",
            "Step 31200/150000, Loss: 6.108611512184143, Test Loss: 6.105596363544464, LR: 0.0005, Elapsed Time: 3280.49 seconds\n",
            "Step 31300/150000, Loss: 6.100753235816955, Test Loss: 6.107708156108856, LR: 0.0005, Elapsed Time: 3291.00 seconds\n",
            "Step 31400/150000, Loss: 6.10855836391449, Test Loss: 6.102022230625153, LR: 0.0005, Elapsed Time: 3301.49 seconds\n",
            "Step 31500/150000, Loss: 6.112256917953491, Test Loss: 6.103033483028412, LR: 0.0005, Elapsed Time: 3311.93 seconds\n",
            "Step 31600/150000, Loss: 6.109223289489746, Test Loss: 6.1005332469940186, LR: 0.0005, Elapsed Time: 3322.37 seconds\n",
            "Step 31700/150000, Loss: 6.090992574691772, Test Loss: 6.1010043025016785, LR: 0.0005, Elapsed Time: 3332.80 seconds\n",
            "Step 31800/150000, Loss: 6.105441083908081, Test Loss: 6.09434449672699, LR: 0.0005, Elapsed Time: 3343.24 seconds\n",
            "Step 31900/150000, Loss: 6.093516616821289, Test Loss: 6.097595751285553, LR: 0.0005, Elapsed Time: 3353.67 seconds\n",
            "Step 32000/150000, Loss: 6.109400243759155, Test Loss: 6.087871611118317, LR: 0.0005, Elapsed Time: 3364.08 seconds\n",
            "Step 32100/150000, Loss: 6.093338103294372, Test Loss: 6.0981621742248535, LR: 0.0005, Elapsed Time: 3374.52 seconds\n",
            "Step 32200/150000, Loss: 6.096166443824768, Test Loss: 6.088769733905792, LR: 0.0005, Elapsed Time: 3384.92 seconds\n",
            "Step 32300/150000, Loss: 6.102646465301514, Test Loss: 6.087979018688202, LR: 0.0005, Elapsed Time: 3395.40 seconds\n",
            "Step 32400/150000, Loss: 6.09002049446106, Test Loss: 6.082152545452118, LR: 0.0005, Elapsed Time: 3405.82 seconds\n",
            "Step 32500/150000, Loss: 6.093568787574768, Test Loss: 6.085061728954315, LR: 0.0005, Elapsed Time: 3416.24 seconds\n",
            "Step 32600/150000, Loss: 6.088463568687439, Test Loss: 6.080061972141266, LR: 0.0005, Elapsed Time: 3426.73 seconds\n",
            "Step 32700/150000, Loss: 6.09066370010376, Test Loss: 6.080202102661133, LR: 0.0005, Elapsed Time: 3437.21 seconds\n",
            "Step 32800/150000, Loss: 6.091840500831604, Test Loss: 6.074113607406616, LR: 0.0005, Elapsed Time: 3447.64 seconds\n",
            "Step 32900/150000, Loss: 6.078388471603393, Test Loss: 6.075653076171875, LR: 0.0005, Elapsed Time: 3458.03 seconds\n",
            "Step 33000/150000, Loss: 6.070182995796204, Test Loss: 6.072208881378174, LR: 0.0005, Elapsed Time: 3468.46 seconds\n",
            "Step 33100/150000, Loss: 6.0636973524093625, Test Loss: 6.067295849323273, LR: 0.0005, Elapsed Time: 3478.94 seconds\n",
            "Step 33200/150000, Loss: 6.081500978469848, Test Loss: 6.063136339187622, LR: 0.0005, Elapsed Time: 3489.34 seconds\n",
            "Step 33300/150000, Loss: 6.069124450683594, Test Loss: 6.064612507820129, LR: 0.0005, Elapsed Time: 3499.83 seconds\n",
            "Step 33400/150000, Loss: 6.076454834938049, Test Loss: 6.060621738433838, LR: 0.0005, Elapsed Time: 3510.28 seconds\n",
            "Step 33500/150000, Loss: 6.068816485404969, Test Loss: 6.058933198451996, LR: 0.0005, Elapsed Time: 3520.70 seconds\n",
            "Step 33600/150000, Loss: 6.058795394897461, Test Loss: 6.067911863327026, LR: 0.0005, Elapsed Time: 3531.08 seconds\n",
            "Step 33700/150000, Loss: 6.059148707389832, Test Loss: 6.055261969566345, LR: 0.0005, Elapsed Time: 3541.53 seconds\n",
            "Step 33800/150000, Loss: 6.0572194528579715, Test Loss: 6.053225934505463, LR: 0.0005, Elapsed Time: 3552.02 seconds\n",
            "Step 33900/150000, Loss: 6.067165002822876, Test Loss: 6.051542341709137, LR: 0.0005, Elapsed Time: 3562.41 seconds\n",
            "Step 34000/150000, Loss: 6.064450054168701, Test Loss: 6.055772602558136, LR: 0.0005, Elapsed Time: 3572.83 seconds\n",
            "Step 34100/150000, Loss: 6.0601071882247926, Test Loss: 6.051725506782532, LR: 0.0005, Elapsed Time: 3583.27 seconds\n",
            "Step 34200/150000, Loss: 6.060403504371643, Test Loss: 6.046834647655487, LR: 0.0005, Elapsed Time: 3593.69 seconds\n",
            "Step 34300/150000, Loss: 6.04362729549408, Test Loss: 6.044512093067169, LR: 0.0005, Elapsed Time: 3604.16 seconds\n",
            "Step 34400/150000, Loss: 6.052924880981445, Test Loss: 6.04515153169632, LR: 0.0005, Elapsed Time: 3614.63 seconds\n",
            "Step 34500/150000, Loss: 6.053833904266358, Test Loss: 6.046478092670441, LR: 0.0005, Elapsed Time: 3625.14 seconds\n",
            "Step 34600/150000, Loss: 6.043240647315979, Test Loss: 6.038540065288544, LR: 0.0005, Elapsed Time: 3635.56 seconds\n",
            "Step 34700/150000, Loss: 6.044962048530579, Test Loss: 6.041688144207001, LR: 0.0005, Elapsed Time: 3645.97 seconds\n",
            "Step 34800/150000, Loss: 6.051593914031982, Test Loss: 6.036306083202362, LR: 0.0005, Elapsed Time: 3656.43 seconds\n",
            "Step 34900/150000, Loss: 6.035616126060486, Test Loss: 6.0310288071632385, LR: 0.0005, Elapsed Time: 3666.81 seconds\n",
            "Step 35000/150000, Loss: 6.026905360221863, Test Loss: 6.030189037322998, LR: 0.0005, Elapsed Time: 3677.23 seconds\n",
            "Step 35100/150000, Loss: 6.042121119499207, Test Loss: 6.028798162937164, LR: 0.0005, Elapsed Time: 3687.69 seconds\n",
            "Step 35200/150000, Loss: 6.039070744514465, Test Loss: 6.024654865264893, LR: 0.0005, Elapsed Time: 3698.12 seconds\n",
            "Step 35300/150000, Loss: 6.0257692050933835, Test Loss: 6.026781260967255, LR: 0.0005, Elapsed Time: 3708.55 seconds\n",
            "Step 35400/150000, Loss: 6.030350117683411, Test Loss: 6.01838618516922, LR: 0.0005, Elapsed Time: 3719.04 seconds\n",
            "Step 35500/150000, Loss: 6.019498686790467, Test Loss: 6.0221928358078, LR: 0.0005, Elapsed Time: 3729.55 seconds\n",
            "Step 35600/150000, Loss: 6.022944941520691, Test Loss: 6.017552554607391, LR: 0.0005, Elapsed Time: 3739.99 seconds\n",
            "Step 35700/150000, Loss: 6.0129370641708375, Test Loss: 6.012584745883942, LR: 0.0005, Elapsed Time: 3750.37 seconds\n",
            "Step 35800/150000, Loss: 6.017200932502747, Test Loss: 6.014536023139954, LR: 0.0005, Elapsed Time: 3760.82 seconds\n",
            "Step 35900/150000, Loss: 6.02465003490448, Test Loss: 6.021388590335846, LR: 0.0005, Elapsed Time: 3771.25 seconds\n",
            "Step 36000/150000, Loss: 6.01933720111847, Test Loss: 6.009196877479553, LR: 0.0005, Elapsed Time: 3781.64 seconds\n",
            "Step 36100/150000, Loss: 6.012976698875427, Test Loss: 6.01215660572052, LR: 0.0005, Elapsed Time: 3792.11 seconds\n",
            "Step 36200/150000, Loss: 6.019264578819275, Test Loss: 6.008389890193939, LR: 0.0005, Elapsed Time: 3802.55 seconds\n",
            "Step 36300/150000, Loss: 6.014468836784363, Test Loss: 6.007692992687225, LR: 0.0005, Elapsed Time: 3812.99 seconds\n",
            "Step 36400/150000, Loss: 6.009807748794556, Test Loss: 5.997376441955566, LR: 0.0005, Elapsed Time: 3823.48 seconds\n",
            "Step 36500/150000, Loss: 6.00594925403595, Test Loss: 6.002587914466858, LR: 0.0005, Elapsed Time: 3833.93 seconds\n",
            "Step 36600/150000, Loss: 5.995894179344178, Test Loss: 5.999123573303223, LR: 0.0005, Elapsed Time: 3844.38 seconds\n",
            "Step 36700/150000, Loss: 5.990535726547241, Test Loss: 6.002555668354034, LR: 0.0005, Elapsed Time: 3854.84 seconds\n",
            "Step 36800/150000, Loss: 6.013510704040527, Test Loss: 5.997834742069244, LR: 0.0005, Elapsed Time: 3865.27 seconds\n",
            "Step 36900/150000, Loss: 5.999830737113952, Test Loss: 5.991721570491791, LR: 0.0005, Elapsed Time: 3875.67 seconds\n",
            "Step 37000/150000, Loss: 6.005103611946106, Test Loss: 5.991996765136719, LR: 0.0005, Elapsed Time: 3886.08 seconds\n",
            "Step 37100/150000, Loss: 5.990902214050293, Test Loss: 5.989355683326721, LR: 0.0005, Elapsed Time: 3896.50 seconds\n",
            "Step 37200/150000, Loss: 5.979286127090454, Test Loss: 5.9909480810165405, LR: 0.0005, Elapsed Time: 3907.01 seconds\n",
            "Step 37300/150000, Loss: 5.990879378318787, Test Loss: 5.986709415912628, LR: 0.0005, Elapsed Time: 3917.48 seconds\n",
            "Step 37400/150000, Loss: 6.000451536178589, Test Loss: 5.979166567325592, LR: 0.0005, Elapsed Time: 3927.87 seconds\n",
            "Step 37500/150000, Loss: 5.976320447921753, Test Loss: 5.976952910423279, LR: 0.0005, Elapsed Time: 3938.41 seconds\n",
            "Step 37600/150000, Loss: 5.997237071990967, Test Loss: 5.978242993354797, LR: 0.0005, Elapsed Time: 3948.89 seconds\n",
            "Step 37700/150000, Loss: 5.986970491409302, Test Loss: 5.982095420360565, LR: 0.0005, Elapsed Time: 3959.35 seconds\n",
            "Step 37800/150000, Loss: 5.978527455329895, Test Loss: 5.977895975112915, LR: 0.0005, Elapsed Time: 3969.85 seconds\n",
            "Step 37900/150000, Loss: 5.96548282623291, Test Loss: 5.975106358528137, LR: 0.0005, Elapsed Time: 3980.31 seconds\n",
            "Step 38000/150000, Loss: 5.985452189445495, Test Loss: 5.977665364742279, LR: 0.0005, Elapsed Time: 3990.79 seconds\n",
            "Step 38100/150000, Loss: 5.969007239341736, Test Loss: 5.975178778171539, LR: 0.0005, Elapsed Time: 4001.19 seconds\n",
            "Step 38200/150000, Loss: 5.979971027374267, Test Loss: 5.9702741503715515, LR: 0.0005, Elapsed Time: 4011.64 seconds\n",
            "Step 38300/150000, Loss: 5.974960370063782, Test Loss: 5.968958735466003, LR: 0.0005, Elapsed Time: 4022.10 seconds\n",
            "Step 38400/150000, Loss: 5.968921222686768, Test Loss: 5.965389609336853, LR: 0.0005, Elapsed Time: 4032.57 seconds\n",
            "Step 38500/150000, Loss: 5.982145557403564, Test Loss: 5.963244736194611, LR: 0.0005, Elapsed Time: 4043.03 seconds\n",
            "Step 38600/150000, Loss: 5.968026962280273, Test Loss: 5.964480221271515, LR: 0.0005, Elapsed Time: 4053.51 seconds\n",
            "Step 38700/150000, Loss: 5.972889919281005, Test Loss: 5.956547379493713, LR: 0.0005, Elapsed Time: 4063.97 seconds\n",
            "Step 38800/150000, Loss: 5.960025534629822, Test Loss: 5.959240734577179, LR: 0.0005, Elapsed Time: 4074.45 seconds\n",
            "Step 38900/150000, Loss: 5.955317902565002, Test Loss: 5.957756757736206, LR: 0.0005, Elapsed Time: 4084.87 seconds\n",
            "Step 39000/150000, Loss: 5.966357536315918, Test Loss: 5.951875865459442, LR: 0.0005, Elapsed Time: 4095.26 seconds\n",
            "Step 39100/150000, Loss: 5.957729330062866, Test Loss: 5.951167702674866, LR: 0.0005, Elapsed Time: 4105.69 seconds\n",
            "Step 39200/150000, Loss: 5.962760801315308, Test Loss: 5.950755715370178, LR: 0.0005, Elapsed Time: 4116.12 seconds\n",
            "Step 39300/150000, Loss: 5.959100346565247, Test Loss: 5.948922038078308, LR: 0.0005, Elapsed Time: 4126.55 seconds\n",
            "Step 39400/150000, Loss: 5.928620958328247, Test Loss: 5.943805694580078, LR: 0.0005, Elapsed Time: 4136.96 seconds\n",
            "Step 39500/150000, Loss: 5.943184213638306, Test Loss: 5.943460822105408, LR: 0.0005, Elapsed Time: 4147.41 seconds\n",
            "Step 39600/150000, Loss: 5.948457698822022, Test Loss: 5.946314573287964, LR: 0.0005, Elapsed Time: 4157.87 seconds\n",
            "Step 39700/150000, Loss: 5.953649568557739, Test Loss: 5.941150426864624, LR: 0.0005, Elapsed Time: 4168.22 seconds\n",
            "Step 39800/150000, Loss: 5.942794432640076, Test Loss: 5.942875444889069, LR: 0.0005, Elapsed Time: 4178.65 seconds\n",
            "Step 39900/150000, Loss: 5.94575873374939, Test Loss: 5.93720930814743, LR: 0.0005, Elapsed Time: 4189.09 seconds\n",
            "Step 40000/150000, Loss: 5.938135714530945, Test Loss: 5.937519073486328, LR: 0.0005, Elapsed Time: 4199.59 seconds\n",
            "Step 40100/150000, Loss: 5.931557874679566, Test Loss: 5.932717740535736, LR: 0.0005, Elapsed Time: 4210.05 seconds\n",
            "Step 40200/150000, Loss: 5.9232305860519405, Test Loss: 5.9313605427742, LR: 0.0005, Elapsed Time: 4220.49 seconds\n",
            "Step 40300/150000, Loss: 5.936990547180176, Test Loss: 5.928319573402405, LR: 0.0005, Elapsed Time: 4230.97 seconds\n",
            "Step 40400/150000, Loss: 5.938570022583008, Test Loss: 5.931916415691376, LR: 0.0005, Elapsed Time: 4241.46 seconds\n",
            "Step 40500/150000, Loss: 5.937492055892944, Test Loss: 5.9264161586761475, LR: 0.0005, Elapsed Time: 4251.92 seconds\n",
            "Step 40600/150000, Loss: 5.923813109397888, Test Loss: 5.933529496192932, LR: 0.0005, Elapsed Time: 4262.36 seconds\n",
            "Step 40700/150000, Loss: 5.934683403968811, Test Loss: 5.92292720079422, LR: 0.0005, Elapsed Time: 4272.81 seconds\n",
            "Step 40800/150000, Loss: 5.933063597679138, Test Loss: 5.924282014369965, LR: 0.0005, Elapsed Time: 4283.23 seconds\n",
            "Step 40900/150000, Loss: 5.919202733039856, Test Loss: 5.91918671131134, LR: 0.0005, Elapsed Time: 4293.68 seconds\n",
            "Step 41000/150000, Loss: 5.914374885559082, Test Loss: 5.915018618106842, LR: 0.0005, Elapsed Time: 4304.13 seconds\n",
            "Step 41100/150000, Loss: 5.90896996974945, Test Loss: 5.915798664093018, LR: 0.0005, Elapsed Time: 4314.53 seconds\n",
            "Step 41200/150000, Loss: 5.913657970428467, Test Loss: 5.918470680713654, LR: 0.0005, Elapsed Time: 4324.99 seconds\n",
            "Step 41300/150000, Loss: 5.917114014625549, Test Loss: 5.913124740123749, LR: 0.0005, Elapsed Time: 4335.39 seconds\n",
            "Step 41400/150000, Loss: 5.9099878358840945, Test Loss: 5.910626828670502, LR: 0.0005, Elapsed Time: 4345.88 seconds\n",
            "Step 41500/150000, Loss: 5.903722825050354, Test Loss: 5.9093546867370605, LR: 0.0005, Elapsed Time: 4356.33 seconds\n",
            "Step 41600/150000, Loss: 5.913482222557068, Test Loss: 5.908509850502014, LR: 0.0005, Elapsed Time: 4366.76 seconds\n",
            "Step 41700/150000, Loss: 5.921222243309021, Test Loss: 5.897541165351868, LR: 0.0005, Elapsed Time: 4377.20 seconds\n",
            "Step 41800/150000, Loss: 5.902980070114136, Test Loss: 5.901935279369354, LR: 0.0005, Elapsed Time: 4387.64 seconds\n",
            "Step 41900/150000, Loss: 5.914592185020447, Test Loss: 5.897011876106262, LR: 0.0005, Elapsed Time: 4398.11 seconds\n",
            "Step 42000/150000, Loss: 5.90191499710083, Test Loss: 5.908784806728363, LR: 0.0005, Elapsed Time: 4408.55 seconds\n",
            "Step 42100/150000, Loss: 5.891466546058655, Test Loss: 5.894311368465424, LR: 0.0005, Elapsed Time: 4419.05 seconds\n",
            "Step 42200/150000, Loss: 5.901901683807373, Test Loss: 5.8902939558029175, LR: 0.0005, Elapsed Time: 4429.43 seconds\n",
            "Step 42300/150000, Loss: 5.885509872436524, Test Loss: 5.888437330722809, LR: 0.0005, Elapsed Time: 4439.88 seconds\n",
            "Step 42400/150000, Loss: 5.901699542999268, Test Loss: 5.886762082576752, LR: 0.0005, Elapsed Time: 4450.43 seconds\n",
            "Step 42500/150000, Loss: 5.884639883041382, Test Loss: 5.884190082550049, LR: 0.0005, Elapsed Time: 4460.85 seconds\n",
            "Step 42600/150000, Loss: 5.900937023162842, Test Loss: 5.88665235042572, LR: 0.0005, Elapsed Time: 4471.28 seconds\n",
            "Step 42700/150000, Loss: 5.8818558502197265, Test Loss: 5.885623753070831, LR: 0.0005, Elapsed Time: 4481.71 seconds\n",
            "Step 42800/150000, Loss: 5.8879849195480345, Test Loss: 5.880548894405365, LR: 0.0005, Elapsed Time: 4492.15 seconds\n",
            "Step 42900/150000, Loss: 5.899122343063355, Test Loss: 5.877642631530762, LR: 0.0005, Elapsed Time: 4502.64 seconds\n",
            "Step 43000/150000, Loss: 5.873701076507569, Test Loss: 5.878451943397522, LR: 0.0005, Elapsed Time: 4513.13 seconds\n",
            "Step 43100/150000, Loss: 5.890672583580017, Test Loss: 5.876022815704346, LR: 0.0005, Elapsed Time: 4523.67 seconds\n",
            "Step 43200/150000, Loss: 5.8862994146347045, Test Loss: 5.876372456550598, LR: 0.0005, Elapsed Time: 4534.17 seconds\n",
            "Step 43300/150000, Loss: 5.882007732391357, Test Loss: 5.877281308174133, LR: 0.0005, Elapsed Time: 4544.62 seconds\n",
            "Step 43400/150000, Loss: 5.8748514938354495, Test Loss: 5.873298645019531, LR: 0.0005, Elapsed Time: 4555.14 seconds\n",
            "Step 43500/150000, Loss: 5.876629953384399, Test Loss: 5.871146380901337, LR: 0.0005, Elapsed Time: 4565.62 seconds\n",
            "Step 43600/150000, Loss: 5.876505823135376, Test Loss: 5.877763330936432, LR: 0.0005, Elapsed Time: 4576.11 seconds\n",
            "Step 43700/150000, Loss: 5.867029638290405, Test Loss: 5.864722847938538, LR: 0.0005, Elapsed Time: 4586.57 seconds\n",
            "Step 43800/150000, Loss: 5.866069984436035, Test Loss: 5.8683894872665405, LR: 0.0005, Elapsed Time: 4597.02 seconds\n",
            "Step 43900/150000, Loss: 5.855792636871338, Test Loss: 5.862679600715637, LR: 0.0005, Elapsed Time: 4607.46 seconds\n",
            "Step 44000/150000, Loss: 5.869513192176819, Test Loss: 5.858771324157715, LR: 0.0005, Elapsed Time: 4617.95 seconds\n",
            "Step 44100/150000, Loss: 5.864533257484436, Test Loss: 5.857348561286926, LR: 0.0005, Elapsed Time: 4628.41 seconds\n",
            "Step 44200/150000, Loss: 5.862675275802612, Test Loss: 5.854569733142853, LR: 0.0005, Elapsed Time: 4638.88 seconds\n",
            "Step 44300/150000, Loss: 5.851214408874512, Test Loss: 5.8506457805633545, LR: 0.0005, Elapsed Time: 4649.36 seconds\n",
            "Step 44400/150000, Loss: 5.861044640541077, Test Loss: 5.8536030650138855, LR: 0.0005, Elapsed Time: 4659.86 seconds\n",
            "Step 44500/150000, Loss: 5.859848227500915, Test Loss: 5.854314982891083, LR: 0.0005, Elapsed Time: 4670.35 seconds\n",
            "Step 44600/150000, Loss: 5.849500579833984, Test Loss: 5.843581020832062, LR: 0.0005, Elapsed Time: 4680.78 seconds\n",
            "Step 44700/150000, Loss: 5.838568511009217, Test Loss: 5.846759557723999, LR: 0.0005, Elapsed Time: 4691.26 seconds\n",
            "Step 44800/150000, Loss: 5.834904537200928, Test Loss: 5.848383784294128, LR: 0.0005, Elapsed Time: 4701.72 seconds\n",
            "Step 44900/150000, Loss: 5.850045833587647, Test Loss: 5.845510482788086, LR: 0.0005, Elapsed Time: 4712.13 seconds\n",
            "Step 45000/150000, Loss: 5.84927390575409, Test Loss: 5.8463550209999084, LR: 0.0005, Elapsed Time: 4722.60 seconds\n",
            "Step 45100/150000, Loss: 5.830613083839417, Test Loss: 5.837386608123779, LR: 0.0005, Elapsed Time: 4733.01 seconds\n",
            "Step 45200/150000, Loss: 5.853322615623474, Test Loss: 5.835253477096558, LR: 0.0005, Elapsed Time: 4743.50 seconds\n",
            "Step 45300/150000, Loss: 5.832345261573791, Test Loss: 5.838078081607819, LR: 0.0005, Elapsed Time: 4753.96 seconds\n",
            "Step 45400/150000, Loss: 5.843818249702454, Test Loss: 5.8386335372924805, LR: 0.0005, Elapsed Time: 4764.49 seconds\n",
            "Step 45500/150000, Loss: 5.842650179862976, Test Loss: 5.8367942571640015, LR: 0.0005, Elapsed Time: 4774.94 seconds\n",
            "Step 45600/150000, Loss: 5.828404932022095, Test Loss: 5.829302608966827, LR: 0.0005, Elapsed Time: 4785.38 seconds\n",
            "Step 45700/150000, Loss: 5.82669798374176, Test Loss: 5.830215394496918, LR: 0.0005, Elapsed Time: 4795.85 seconds\n",
            "Step 45800/150000, Loss: 5.838660144805909, Test Loss: 5.828903555870056, LR: 0.0005, Elapsed Time: 4806.26 seconds\n",
            "Step 45900/150000, Loss: 5.825883069038391, Test Loss: 5.828873038291931, LR: 0.0005, Elapsed Time: 4816.69 seconds\n",
            "Step 46000/150000, Loss: 5.824935231208801, Test Loss: 5.828267276287079, LR: 0.0005, Elapsed Time: 4827.10 seconds\n",
            "Step 46100/150000, Loss: 5.8315033435821535, Test Loss: 5.821519494056702, LR: 0.0005, Elapsed Time: 4837.53 seconds\n",
            "Step 46200/150000, Loss: 5.825870933532715, Test Loss: 5.825476706027985, LR: 0.0005, Elapsed Time: 4847.98 seconds\n",
            "Step 46300/150000, Loss: 5.821720261573791, Test Loss: 5.8179327845573425, LR: 0.0005, Elapsed Time: 4858.45 seconds\n",
            "Step 46400/150000, Loss: 5.821828060150146, Test Loss: 5.815400719642639, LR: 0.0005, Elapsed Time: 4868.89 seconds\n",
            "Step 46500/150000, Loss: 5.811000561714172, Test Loss: 5.8163570165634155, LR: 0.0005, Elapsed Time: 4879.38 seconds\n",
            "Step 46600/150000, Loss: 5.804362273216247, Test Loss: 5.817447006702423, LR: 0.0005, Elapsed Time: 4889.85 seconds\n",
            "Step 46700/150000, Loss: 5.814602522850037, Test Loss: 5.822064518928528, LR: 0.0005, Elapsed Time: 4900.30 seconds\n",
            "Step 46800/150000, Loss: 5.806937408447266, Test Loss: 5.810829401016235, LR: 0.0005, Elapsed Time: 4910.71 seconds\n",
            "Step 46900/150000, Loss: 5.810415759086609, Test Loss: 5.804976999759674, LR: 0.0005, Elapsed Time: 4921.10 seconds\n",
            "Step 47000/150000, Loss: 5.815558662414551, Test Loss: 5.810133934020996, LR: 0.0005, Elapsed Time: 4931.57 seconds\n",
            "Step 47100/150000, Loss: 5.807269148826599, Test Loss: 5.807037353515625, LR: 0.0005, Elapsed Time: 4942.00 seconds\n",
            "Step 47200/150000, Loss: 5.817147989273071, Test Loss: 5.799008011817932, LR: 0.0005, Elapsed Time: 4952.40 seconds\n",
            "Step 47300/150000, Loss: 5.806409749984741, Test Loss: 5.799977898597717, LR: 0.0005, Elapsed Time: 4962.83 seconds\n",
            "Step 47400/150000, Loss: 5.800261840820313, Test Loss: 5.7986374497413635, LR: 0.0005, Elapsed Time: 4973.30 seconds\n",
            "Step 47500/150000, Loss: 5.8027080631256105, Test Loss: 5.7976155281066895, LR: 0.0005, Elapsed Time: 4983.72 seconds\n",
            "Step 47600/150000, Loss: 5.8087613487243654, Test Loss: 5.796683490276337, LR: 0.0005, Elapsed Time: 4994.12 seconds\n",
            "Step 47700/150000, Loss: 5.791577043533326, Test Loss: 5.790598809719086, LR: 0.0005, Elapsed Time: 5004.61 seconds\n",
            "Step 47800/150000, Loss: 5.791577301025391, Test Loss: 5.786646485328674, LR: 0.0005, Elapsed Time: 5015.06 seconds\n",
            "Step 47900/150000, Loss: 5.782914552688599, Test Loss: 5.786516070365906, LR: 0.0005, Elapsed Time: 5025.47 seconds\n",
            "Step 48000/150000, Loss: 5.78526882648468, Test Loss: 5.789699554443359, LR: 0.0005, Elapsed Time: 5035.84 seconds\n",
            "Step 48100/150000, Loss: 5.801855220794677, Test Loss: 5.797843515872955, LR: 0.0005, Elapsed Time: 5046.25 seconds\n",
            "Step 48200/150000, Loss: 5.780281414985657, Test Loss: 5.784226775169373, LR: 0.0005, Elapsed Time: 5056.63 seconds\n",
            "Step 48300/150000, Loss: 5.784627995491028, Test Loss: 5.783224582672119, LR: 0.0005, Elapsed Time: 5067.09 seconds\n",
            "Step 48400/150000, Loss: 5.776226944923401, Test Loss: 5.78382009267807, LR: 0.0005, Elapsed Time: 5077.53 seconds\n",
            "Step 48500/150000, Loss: 5.783359613418579, Test Loss: 5.780032217502594, LR: 0.0005, Elapsed Time: 5087.93 seconds\n",
            "Step 48600/150000, Loss: 5.784785137176514, Test Loss: 5.779132187366486, LR: 0.0005, Elapsed Time: 5098.33 seconds\n",
            "Step 48700/150000, Loss: 5.781630439758301, Test Loss: 5.776617169380188, LR: 0.0005, Elapsed Time: 5108.74 seconds\n",
            "Step 48800/150000, Loss: 5.770406036376953, Test Loss: 5.776076078414917, LR: 0.0005, Elapsed Time: 5119.19 seconds\n",
            "Step 48900/150000, Loss: 5.768522310256958, Test Loss: 5.774195849895477, LR: 0.0005, Elapsed Time: 5129.68 seconds\n",
            "Step 49000/150000, Loss: 5.762334475517273, Test Loss: 5.771038472652435, LR: 0.0005, Elapsed Time: 5140.18 seconds\n",
            "Step 49100/150000, Loss: 5.7668607139587404, Test Loss: 5.770519375801086, LR: 0.0005, Elapsed Time: 5150.65 seconds\n",
            "Step 49200/150000, Loss: 5.777495994567871, Test Loss: 5.766781866550446, LR: 0.0005, Elapsed Time: 5161.07 seconds\n",
            "Step 49300/150000, Loss: 5.76821852684021, Test Loss: 5.762523293495178, LR: 0.0005, Elapsed Time: 5171.49 seconds\n",
            "Step 49400/150000, Loss: 5.762973222732544, Test Loss: 5.760149657726288, LR: 0.0005, Elapsed Time: 5181.92 seconds\n",
            "Step 49500/150000, Loss: 5.7599051523208615, Test Loss: 5.766585886478424, LR: 0.0005, Elapsed Time: 5192.36 seconds\n",
            "Step 49600/150000, Loss: 5.747420997619629, Test Loss: 5.759222030639648, LR: 0.0005, Elapsed Time: 5202.81 seconds\n",
            "Step 49700/150000, Loss: 5.7633383131027225, Test Loss: 5.767602980136871, LR: 0.0005, Elapsed Time: 5213.27 seconds\n",
            "Step 49800/150000, Loss: 5.767099347114563, Test Loss: 5.756198346614838, LR: 0.0005, Elapsed Time: 5223.71 seconds\n",
            "Step 49900/150000, Loss: 5.74473021030426, Test Loss: 5.754476547241211, LR: 0.0005, Elapsed Time: 5234.19 seconds\n",
            "Step 50000/150000, Loss: 5.753892431259155, Test Loss: 5.752468109130859, LR: 0.0005, Elapsed Time: 5244.55 seconds\n",
            "Saving model checkpoint at step 50000\n",
            "Step 50100/150000, Loss: 5.755419297218323, Test Loss: 5.748484790325165, LR: 0.0005, Elapsed Time: 5255.12 seconds\n",
            "Step 50200/150000, Loss: 5.740385332107544, Test Loss: 5.7500370144844055, LR: 0.0005, Elapsed Time: 5265.56 seconds\n",
            "Step 50300/150000, Loss: 5.751336932182312, Test Loss: 5.748665630817413, LR: 0.0005, Elapsed Time: 5275.98 seconds\n",
            "Step 50400/150000, Loss: 5.737398219108582, Test Loss: 5.744154453277588, LR: 0.0005, Elapsed Time: 5286.40 seconds\n",
            "Step 50500/150000, Loss: 5.735332345962524, Test Loss: 5.7421650886535645, LR: 0.0005, Elapsed Time: 5296.77 seconds\n",
            "Step 50600/150000, Loss: 5.74207944393158, Test Loss: 5.744273662567139, LR: 0.0005, Elapsed Time: 5307.19 seconds\n",
            "Step 50700/150000, Loss: 5.736642251014709, Test Loss: 5.738032579421997, LR: 0.0005, Elapsed Time: 5317.55 seconds\n",
            "Step 50800/150000, Loss: 5.727261786460876, Test Loss: 5.74356883764267, LR: 0.0005, Elapsed Time: 5327.99 seconds\n",
            "Step 50900/150000, Loss: 5.72660535812378, Test Loss: 5.733484387397766, LR: 0.0005, Elapsed Time: 5338.34 seconds\n",
            "Step 51000/150000, Loss: 5.7266232824325565, Test Loss: 5.731435835361481, LR: 0.0005, Elapsed Time: 5348.78 seconds\n",
            "Step 51100/150000, Loss: 5.717036581039428, Test Loss: 5.7295753955841064, LR: 0.0005, Elapsed Time: 5359.26 seconds\n",
            "Step 51200/150000, Loss: 5.734103055000305, Test Loss: 5.739583492279053, LR: 0.0005, Elapsed Time: 5369.70 seconds\n",
            "Step 51300/150000, Loss: 5.72184910774231, Test Loss: 5.728099346160889, LR: 0.0005, Elapsed Time: 5380.23 seconds\n",
            "Step 51400/150000, Loss: 5.71991144657135, Test Loss: 5.725741326808929, LR: 0.0005, Elapsed Time: 5390.71 seconds\n",
            "Step 51500/150000, Loss: 5.7273851585388185, Test Loss: 5.726957738399506, LR: 0.0005, Elapsed Time: 5401.20 seconds\n",
            "Step 51600/150000, Loss: 5.715541696548462, Test Loss: 5.720550954341888, LR: 0.0005, Elapsed Time: 5411.69 seconds\n",
            "Step 51700/150000, Loss: 5.718877835273743, Test Loss: 5.716041564941406, LR: 0.0005, Elapsed Time: 5422.09 seconds\n",
            "Step 51800/150000, Loss: 5.7087333345413205, Test Loss: 5.71467524766922, LR: 0.0005, Elapsed Time: 5432.57 seconds\n",
            "Step 51900/150000, Loss: 5.715588955879212, Test Loss: 5.711975276470184, LR: 0.0005, Elapsed Time: 5443.05 seconds\n",
            "Step 52000/150000, Loss: 5.712391657829285, Test Loss: 5.711428105831146, LR: 0.0005, Elapsed Time: 5453.52 seconds\n",
            "Step 52100/150000, Loss: 5.718744149208069, Test Loss: 5.708009660243988, LR: 0.0005, Elapsed Time: 5463.99 seconds\n",
            "Step 52200/150000, Loss: 5.706308016777038, Test Loss: 5.710836112499237, LR: 0.0005, Elapsed Time: 5474.44 seconds\n",
            "Step 52300/150000, Loss: 5.708078117370605, Test Loss: 5.711504578590393, LR: 0.0005, Elapsed Time: 5484.84 seconds\n",
            "Step 52400/150000, Loss: 5.6987220287323, Test Loss: 5.7051613330841064, LR: 0.0005, Elapsed Time: 5495.26 seconds\n",
            "Step 52500/150000, Loss: 5.6947549629211425, Test Loss: 5.707547843456268, LR: 0.0005, Elapsed Time: 5505.73 seconds\n",
            "Step 52600/150000, Loss: 5.696124267578125, Test Loss: 5.698739230632782, LR: 0.0005, Elapsed Time: 5516.17 seconds\n",
            "Step 52700/150000, Loss: 5.698147439956665, Test Loss: 5.694122016429901, LR: 0.0005, Elapsed Time: 5526.59 seconds\n",
            "Step 52800/150000, Loss: 5.6836727380752565, Test Loss: 5.693726480007172, LR: 0.0005, Elapsed Time: 5537.00 seconds\n",
            "Step 52900/150000, Loss: 5.691180486679077, Test Loss: 5.695088565349579, LR: 0.0005, Elapsed Time: 5547.41 seconds\n",
            "Step 53000/150000, Loss: 5.6981464385986325, Test Loss: 5.692256033420563, LR: 0.0005, Elapsed Time: 5557.82 seconds\n",
            "Step 53100/150000, Loss: 5.681665859222412, Test Loss: 5.687709212303162, LR: 0.0005, Elapsed Time: 5568.25 seconds\n",
            "Step 53200/150000, Loss: 5.675207862854004, Test Loss: 5.69061952829361, LR: 0.0005, Elapsed Time: 5578.69 seconds\n",
            "Step 53300/150000, Loss: 5.6948469591140745, Test Loss: 5.6901357769966125, LR: 0.0005, Elapsed Time: 5589.13 seconds\n",
            "Step 53400/150000, Loss: 5.683055510520935, Test Loss: 5.6843090653419495, LR: 0.0005, Elapsed Time: 5599.56 seconds\n",
            "Step 53500/150000, Loss: 5.686636686325073, Test Loss: 5.680770814418793, LR: 0.0005, Elapsed Time: 5609.97 seconds\n",
            "Step 53600/150000, Loss: 5.671270871162415, Test Loss: 5.681318700313568, LR: 0.0005, Elapsed Time: 5620.43 seconds\n",
            "Step 53700/150000, Loss: 5.6728986120224, Test Loss: 5.683937251567841, LR: 0.0005, Elapsed Time: 5630.87 seconds\n",
            "Step 53800/150000, Loss: 5.676726293563843, Test Loss: 5.675660490989685, LR: 0.0005, Elapsed Time: 5641.28 seconds\n",
            "Step 53900/150000, Loss: 5.676638979911804, Test Loss: 5.672456741333008, LR: 0.0005, Elapsed Time: 5651.67 seconds\n",
            "Step 54000/150000, Loss: 5.6786082029342655, Test Loss: 5.681153953075409, LR: 0.0005, Elapsed Time: 5662.13 seconds\n",
            "Step 54100/150000, Loss: 5.668110585212707, Test Loss: 5.673001170158386, LR: 0.0005, Elapsed Time: 5672.56 seconds\n",
            "Step 54200/150000, Loss: 5.668136143684388, Test Loss: 5.665448784828186, LR: 0.0005, Elapsed Time: 5683.00 seconds\n",
            "Step 54300/150000, Loss: 5.678878655433655, Test Loss: 5.665912628173828, LR: 0.0005, Elapsed Time: 5693.45 seconds\n",
            "Step 54400/150000, Loss: 5.667887487411499, Test Loss: 5.6695656180381775, LR: 0.0005, Elapsed Time: 5703.88 seconds\n",
            "Step 54500/150000, Loss: 5.664739036560059, Test Loss: 5.661923766136169, LR: 0.0005, Elapsed Time: 5714.32 seconds\n",
            "Step 54600/150000, Loss: 5.661275568008423, Test Loss: 5.659638345241547, LR: 0.0005, Elapsed Time: 5724.72 seconds\n",
            "Step 54700/150000, Loss: 5.67301007270813, Test Loss: 5.6553027629852295, LR: 0.0005, Elapsed Time: 5735.18 seconds\n",
            "Step 54800/150000, Loss: 5.64791844367981, Test Loss: 5.656724691390991, LR: 0.0005, Elapsed Time: 5745.68 seconds\n",
            "Step 54900/150000, Loss: 5.644829535484314, Test Loss: 5.6482908725738525, LR: 0.0005, Elapsed Time: 5756.16 seconds\n",
            "Step 55000/150000, Loss: 5.646475939750672, Test Loss: 5.648631989955902, LR: 0.0005, Elapsed Time: 5766.62 seconds\n",
            "Step 55100/150000, Loss: 5.6427884817123415, Test Loss: 5.647720217704773, LR: 0.0005, Elapsed Time: 5777.07 seconds\n",
            "Step 55200/150000, Loss: 5.653655405044556, Test Loss: 5.648307740688324, LR: 0.0005, Elapsed Time: 5787.56 seconds\n",
            "Step 55300/150000, Loss: 5.647854809761047, Test Loss: 5.646568179130554, LR: 0.0005, Elapsed Time: 5798.05 seconds\n",
            "Step 55400/150000, Loss: 5.647254600524902, Test Loss: 5.637981176376343, LR: 0.0005, Elapsed Time: 5808.52 seconds\n",
            "Step 55500/150000, Loss: 5.63037483215332, Test Loss: 5.644718408584595, LR: 0.0005, Elapsed Time: 5819.01 seconds\n",
            "Step 55600/150000, Loss: 5.62849027633667, Test Loss: 5.640498399734497, LR: 0.0005, Elapsed Time: 5829.47 seconds\n",
            "Step 55700/150000, Loss: 5.637386832237244, Test Loss: 5.634766459465027, LR: 0.0005, Elapsed Time: 5839.91 seconds\n",
            "Step 55800/150000, Loss: 5.6354866170883176, Test Loss: 5.628650367259979, LR: 0.0005, Elapsed Time: 5850.40 seconds\n",
            "Step 55900/150000, Loss: 5.641723837852478, Test Loss: 5.63021320104599, LR: 0.0005, Elapsed Time: 5860.87 seconds\n",
            "Step 56000/150000, Loss: 5.637692546844482, Test Loss: 5.625627517700195, LR: 0.0005, Elapsed Time: 5871.31 seconds\n",
            "Step 56100/150000, Loss: 5.6313524913787845, Test Loss: 5.6303152441978455, LR: 0.0005, Elapsed Time: 5881.76 seconds\n",
            "Step 56200/150000, Loss: 5.612042121887207, Test Loss: 5.62293666601181, LR: 0.0005, Elapsed Time: 5892.20 seconds\n",
            "Step 56300/150000, Loss: 5.629338946342468, Test Loss: 5.629343807697296, LR: 0.0005, Elapsed Time: 5902.62 seconds\n",
            "Step 56400/150000, Loss: 5.627821450233459, Test Loss: 5.619750916957855, LR: 0.0005, Elapsed Time: 5913.04 seconds\n",
            "Step 56500/150000, Loss: 5.625721368789673, Test Loss: 5.618183791637421, LR: 0.0005, Elapsed Time: 5923.49 seconds\n",
            "Step 56600/150000, Loss: 5.616006951332093, Test Loss: 5.613088667392731, LR: 0.0005, Elapsed Time: 5933.93 seconds\n",
            "Step 56700/150000, Loss: 5.621056046485901, Test Loss: 5.614741086959839, LR: 0.0005, Elapsed Time: 5944.34 seconds\n",
            "Step 56800/150000, Loss: 5.600639119148254, Test Loss: 5.6133118867874146, LR: 0.0005, Elapsed Time: 5954.79 seconds\n",
            "Step 56900/150000, Loss: 5.60641420841217, Test Loss: 5.603604078292847, LR: 0.0005, Elapsed Time: 5965.21 seconds\n",
            "Step 57000/150000, Loss: 5.608161916732788, Test Loss: 5.605472564697266, LR: 0.0005, Elapsed Time: 5975.67 seconds\n",
            "Step 57100/150000, Loss: 5.6160082387924195, Test Loss: 5.604896664619446, LR: 0.0005, Elapsed Time: 5986.17 seconds\n",
            "Step 57200/150000, Loss: 5.60330258846283, Test Loss: 5.607714235782623, LR: 0.0005, Elapsed Time: 5996.62 seconds\n",
            "Step 57300/150000, Loss: 5.592936367988586, Test Loss: 5.598628342151642, LR: 0.0005, Elapsed Time: 6007.03 seconds\n",
            "Step 57400/150000, Loss: 5.597210130691528, Test Loss: 5.598338544368744, LR: 0.0005, Elapsed Time: 6017.46 seconds\n",
            "Step 57500/150000, Loss: 5.589829106330871, Test Loss: 5.594490647315979, LR: 0.0005, Elapsed Time: 6027.89 seconds\n",
            "Step 57600/150000, Loss: 5.5904812288284305, Test Loss: 5.593830704689026, LR: 0.0005, Elapsed Time: 6038.38 seconds\n",
            "Step 57700/150000, Loss: 5.5909467124938965, Test Loss: 5.594666123390198, LR: 0.0005, Elapsed Time: 6048.81 seconds\n",
            "Step 57800/150000, Loss: 5.5929826688766475, Test Loss: 5.592221319675446, LR: 0.0005, Elapsed Time: 6059.26 seconds\n",
            "Step 57900/150000, Loss: 5.595340332984924, Test Loss: 5.583671569824219, LR: 0.0005, Elapsed Time: 6069.62 seconds\n",
            "Step 58000/150000, Loss: 5.584020671844482, Test Loss: 5.585106670856476, LR: 0.0005, Elapsed Time: 6080.04 seconds\n",
            "Step 58100/150000, Loss: 5.580442209243774, Test Loss: 5.582390308380127, LR: 0.0005, Elapsed Time: 6090.48 seconds\n",
            "Step 58200/150000, Loss: 5.590455331802368, Test Loss: 5.579961478710175, LR: 0.0005, Elapsed Time: 6100.90 seconds\n",
            "Step 58300/150000, Loss: 5.575482954978943, Test Loss: 5.574374794960022, LR: 0.0005, Elapsed Time: 6111.34 seconds\n",
            "Step 58400/150000, Loss: 5.567083826065064, Test Loss: 5.576476335525513, LR: 0.0005, Elapsed Time: 6121.82 seconds\n",
            "Step 58500/150000, Loss: 5.568543968200683, Test Loss: 5.572911441326141, LR: 0.0005, Elapsed Time: 6132.18 seconds\n",
            "Step 58600/150000, Loss: 5.563841109275818, Test Loss: 5.56520676612854, LR: 0.0005, Elapsed Time: 6142.62 seconds\n",
            "Step 58700/150000, Loss: 5.571560616493225, Test Loss: 5.567773938179016, LR: 0.0005, Elapsed Time: 6153.03 seconds\n",
            "Step 58800/150000, Loss: 5.57853741645813, Test Loss: 5.561257183551788, LR: 0.0005, Elapsed Time: 6163.40 seconds\n",
            "Step 58900/150000, Loss: 5.561404495239258, Test Loss: 5.561555922031403, LR: 0.0005, Elapsed Time: 6173.80 seconds\n",
            "Step 59000/150000, Loss: 5.552557744979858, Test Loss: 5.559659481048584, LR: 0.0005, Elapsed Time: 6184.23 seconds\n",
            "Step 59100/150000, Loss: 5.543534913063049, Test Loss: 5.554928779602051, LR: 0.0005, Elapsed Time: 6194.66 seconds\n",
            "Step 59200/150000, Loss: 5.568130536079407, Test Loss: 5.559712707996368, LR: 0.0005, Elapsed Time: 6205.09 seconds\n",
            "Step 59300/150000, Loss: 5.5561520385742185, Test Loss: 5.5570343136787415, LR: 0.0005, Elapsed Time: 6215.54 seconds\n",
            "Step 59400/150000, Loss: 5.54873631477356, Test Loss: 5.554377317428589, LR: 0.0005, Elapsed Time: 6225.94 seconds\n",
            "Step 59500/150000, Loss: 5.55423478603363, Test Loss: 5.547853410243988, LR: 0.0005, Elapsed Time: 6236.36 seconds\n",
            "Step 59600/150000, Loss: 5.551368441581726, Test Loss: 5.54738837480545, LR: 0.0005, Elapsed Time: 6246.83 seconds\n",
            "Step 59700/150000, Loss: 5.53619960308075, Test Loss: 5.54361629486084, LR: 0.0005, Elapsed Time: 6257.24 seconds\n",
            "Step 59800/150000, Loss: 5.538480167388916, Test Loss: 5.549842000007629, LR: 0.0005, Elapsed Time: 6267.68 seconds\n",
            "Step 59900/150000, Loss: 5.546247601509094, Test Loss: 5.541487455368042, LR: 0.0005, Elapsed Time: 6278.10 seconds\n",
            "Step 60000/150000, Loss: 5.530522193908691, Test Loss: 5.53598940372467, LR: 0.0005, Elapsed Time: 6288.58 seconds\n",
            "Step 60100/150000, Loss: 5.547292594909668, Test Loss: 5.532052278518677, LR: 0.0005, Elapsed Time: 6299.00 seconds\n",
            "Step 60200/150000, Loss: 5.527333459854126, Test Loss: 5.530036628246307, LR: 0.0005, Elapsed Time: 6309.41 seconds\n",
            "Step 60300/150000, Loss: 5.531564440727234, Test Loss: 5.5310850739479065, LR: 0.0005, Elapsed Time: 6319.87 seconds\n",
            "Step 60400/150000, Loss: 5.541308889389038, Test Loss: 5.522123992443085, LR: 0.0005, Elapsed Time: 6330.26 seconds\n",
            "Step 60500/150000, Loss: 5.533833398818969, Test Loss: 5.523458003997803, LR: 0.0005, Elapsed Time: 6340.70 seconds\n",
            "Step 60600/150000, Loss: 5.524838895797729, Test Loss: 5.52068418264389, LR: 0.0005, Elapsed Time: 6351.15 seconds\n",
            "Step 60700/150000, Loss: 5.517883625030517, Test Loss: 5.524653315544128, LR: 0.0005, Elapsed Time: 6361.62 seconds\n",
            "Step 60800/150000, Loss: 5.51391863822937, Test Loss: 5.514987349510193, LR: 0.0005, Elapsed Time: 6372.10 seconds\n",
            "Step 60900/150000, Loss: 5.519231457710266, Test Loss: 5.512127578258514, LR: 0.0005, Elapsed Time: 6382.53 seconds\n",
            "Step 61000/150000, Loss: 5.516327118873596, Test Loss: 5.513739347457886, LR: 0.0005, Elapsed Time: 6392.97 seconds\n",
            "Step 61100/150000, Loss: 5.529553956985474, Test Loss: 5.502422213554382, LR: 0.0005, Elapsed Time: 6403.43 seconds\n",
            "Step 61200/150000, Loss: 5.495486755371093, Test Loss: 5.503015518188477, LR: 0.0005, Elapsed Time: 6413.84 seconds\n",
            "Step 61300/150000, Loss: 5.4916477870941165, Test Loss: 5.501346111297607, LR: 0.0005, Elapsed Time: 6424.34 seconds\n",
            "Step 61400/150000, Loss: 5.505483918190002, Test Loss: 5.499835431575775, LR: 0.0005, Elapsed Time: 6434.75 seconds\n",
            "Step 61500/150000, Loss: 5.497504811286927, Test Loss: 5.495510220527649, LR: 0.0005, Elapsed Time: 6445.16 seconds\n",
            "Step 61600/150000, Loss: 5.50904263973236, Test Loss: 5.493737757205963, LR: 0.0005, Elapsed Time: 6455.62 seconds\n",
            "Step 61700/150000, Loss: 5.491435470581055, Test Loss: 5.487443327903748, LR: 0.0005, Elapsed Time: 6466.12 seconds\n",
            "Step 61800/150000, Loss: 5.493075842857361, Test Loss: 5.490986227989197, LR: 0.0005, Elapsed Time: 6476.50 seconds\n",
            "Step 61900/150000, Loss: 5.480420198440552, Test Loss: 5.486856579780579, LR: 0.0005, Elapsed Time: 6486.92 seconds\n",
            "Step 62000/150000, Loss: 5.488509993553162, Test Loss: 5.478318989276886, LR: 0.0005, Elapsed Time: 6497.33 seconds\n",
            "Step 62100/150000, Loss: 5.470327210426331, Test Loss: 5.479218542575836, LR: 0.0005, Elapsed Time: 6507.83 seconds\n",
            "Step 62200/150000, Loss: 5.4787893486022945, Test Loss: 5.470667004585266, LR: 0.0005, Elapsed Time: 6518.27 seconds\n",
            "Step 62300/150000, Loss: 5.489966115951538, Test Loss: 5.476600527763367, LR: 0.0005, Elapsed Time: 6528.70 seconds\n",
            "Step 62400/150000, Loss: 5.482001314163208, Test Loss: 5.476853251457214, LR: 0.0005, Elapsed Time: 6539.13 seconds\n",
            "Step 62500/150000, Loss: 5.469401154518128, Test Loss: 5.472087383270264, LR: 0.0005, Elapsed Time: 6549.60 seconds\n",
            "Step 62600/150000, Loss: 5.47974308013916, Test Loss: 5.461105406284332, LR: 0.0005, Elapsed Time: 6560.09 seconds\n",
            "Step 62700/150000, Loss: 5.462952451705933, Test Loss: 5.461405098438263, LR: 0.0005, Elapsed Time: 6570.57 seconds\n",
            "Step 62800/150000, Loss: 5.465084986686707, Test Loss: 5.461001932621002, LR: 0.0005, Elapsed Time: 6581.09 seconds\n",
            "Step 62900/150000, Loss: 5.458996248245239, Test Loss: 5.4559900760650635, LR: 0.0005, Elapsed Time: 6591.53 seconds\n",
            "Step 63000/150000, Loss: 5.447991847991943, Test Loss: 5.456497073173523, LR: 0.0005, Elapsed Time: 6601.98 seconds\n",
            "Step 63100/150000, Loss: 5.45621841430664, Test Loss: 5.455772161483765, LR: 0.0005, Elapsed Time: 6612.32 seconds\n",
            "Step 63200/150000, Loss: 5.446766324043274, Test Loss: 5.453738808631897, LR: 0.0005, Elapsed Time: 6622.76 seconds\n",
            "Step 63300/150000, Loss: 5.4466165351867675, Test Loss: 5.4492703676223755, LR: 0.0005, Elapsed Time: 6633.13 seconds\n",
            "Step 63400/150000, Loss: 5.441807818412781, Test Loss: 5.445022702217102, LR: 0.0005, Elapsed Time: 6643.63 seconds\n",
            "Step 63500/150000, Loss: 5.450954174995422, Test Loss: 5.440025210380554, LR: 0.0005, Elapsed Time: 6654.04 seconds\n",
            "Step 63600/150000, Loss: 5.4451498031616214, Test Loss: 5.437790632247925, LR: 0.0005, Elapsed Time: 6664.49 seconds\n",
            "Step 63700/150000, Loss: 5.438723106384277, Test Loss: 5.433136463165283, LR: 0.0005, Elapsed Time: 6674.94 seconds\n",
            "Step 63800/150000, Loss: 5.444198060035705, Test Loss: 5.431013643741608, LR: 0.0005, Elapsed Time: 6685.39 seconds\n",
            "Step 63900/150000, Loss: 5.434566760063172, Test Loss: 5.4253010749816895, LR: 0.0005, Elapsed Time: 6695.85 seconds\n",
            "Step 64000/150000, Loss: 5.4132376432418825, Test Loss: 5.42661714553833, LR: 0.0005, Elapsed Time: 6706.24 seconds\n",
            "Step 64100/150000, Loss: 5.431676621437073, Test Loss: 5.425026059150696, LR: 0.0005, Elapsed Time: 6716.66 seconds\n",
            "Step 64200/150000, Loss: 5.4101944923400875, Test Loss: 5.413289546966553, LR: 0.0005, Elapsed Time: 6727.10 seconds\n",
            "Step 64300/150000, Loss: 5.420518355369568, Test Loss: 5.412765026092529, LR: 0.0005, Elapsed Time: 6737.66 seconds\n",
            "Step 64400/150000, Loss: 5.411990575790405, Test Loss: 5.406775712966919, LR: 0.0005, Elapsed Time: 6748.13 seconds\n",
            "Step 64500/150000, Loss: 5.418866367340088, Test Loss: 5.404885292053223, LR: 0.0005, Elapsed Time: 6758.56 seconds\n",
            "Step 64600/150000, Loss: 5.395028109550476, Test Loss: 5.4111146330833435, LR: 0.0005, Elapsed Time: 6768.93 seconds\n",
            "Step 64700/150000, Loss: 5.407996468544006, Test Loss: 5.393102705478668, LR: 0.0005, Elapsed Time: 6779.31 seconds\n",
            "Step 64800/150000, Loss: 5.412163515090942, Test Loss: 5.393809914588928, LR: 0.0005, Elapsed Time: 6789.74 seconds\n",
            "Step 64900/150000, Loss: 5.393600001335144, Test Loss: 5.396836519241333, LR: 0.0005, Elapsed Time: 6800.16 seconds\n",
            "Step 65000/150000, Loss: 5.405455746650696, Test Loss: 5.390476167201996, LR: 0.0005, Elapsed Time: 6810.54 seconds\n",
            "Step 65100/150000, Loss: 5.396682920455933, Test Loss: 5.385003924369812, LR: 0.0005, Elapsed Time: 6820.93 seconds\n",
            "Step 65200/150000, Loss: 5.397874155044556, Test Loss: 5.379472255706787, LR: 0.0005, Elapsed Time: 6831.33 seconds\n",
            "Step 65300/150000, Loss: 5.376475811004639, Test Loss: 5.380341112613678, LR: 0.0005, Elapsed Time: 6841.79 seconds\n",
            "Step 65400/150000, Loss: 5.379507145881653, Test Loss: 5.375071823596954, LR: 0.0005, Elapsed Time: 6852.20 seconds\n",
            "Step 65500/150000, Loss: 5.376823544502258, Test Loss: 5.36929577589035, LR: 0.0005, Elapsed Time: 6862.68 seconds\n",
            "Step 65600/150000, Loss: 5.370702910423279, Test Loss: 5.366422176361084, LR: 0.0005, Elapsed Time: 6873.14 seconds\n",
            "Step 65700/150000, Loss: 5.3629554271697994, Test Loss: 5.367552399635315, LR: 0.0005, Elapsed Time: 6883.65 seconds\n",
            "Step 65800/150000, Loss: 5.362802391052246, Test Loss: 5.358013868331909, LR: 0.0005, Elapsed Time: 6894.12 seconds\n",
            "Step 65900/150000, Loss: 5.359031982421875, Test Loss: 5.354712724685669, LR: 0.0005, Elapsed Time: 6904.60 seconds\n",
            "Step 66000/150000, Loss: 5.348923435211182, Test Loss: 5.34974080324173, LR: 0.0005, Elapsed Time: 6915.06 seconds\n",
            "Step 66100/150000, Loss: 5.36217472076416, Test Loss: 5.34639185667038, LR: 0.0005, Elapsed Time: 6925.47 seconds\n",
            "Step 66200/150000, Loss: 5.3429257106781005, Test Loss: 5.342728137969971, LR: 0.0005, Elapsed Time: 6935.96 seconds\n",
            "Step 66300/150000, Loss: 5.347363924980163, Test Loss: 5.335454761981964, LR: 0.0005, Elapsed Time: 6946.46 seconds\n",
            "Step 66400/150000, Loss: 5.338300895690918, Test Loss: 5.329885244369507, LR: 0.0005, Elapsed Time: 6956.97 seconds\n",
            "Step 66500/150000, Loss: 5.32712655544281, Test Loss: 5.3313708901405334, LR: 0.0005, Elapsed Time: 6967.48 seconds\n",
            "Step 66600/150000, Loss: 5.31554416179657, Test Loss: 5.324614524841309, LR: 0.0005, Elapsed Time: 6978.08 seconds\n",
            "Step 66700/150000, Loss: 5.313377046585083, Test Loss: 5.327148020267487, LR: 0.0005, Elapsed Time: 6988.54 seconds\n",
            "Step 66800/150000, Loss: 5.319546875953674, Test Loss: 5.313472926616669, LR: 0.0005, Elapsed Time: 6999.11 seconds\n",
            "Step 66900/150000, Loss: 5.309781937599182, Test Loss: 5.3084147572517395, LR: 0.0005, Elapsed Time: 7009.65 seconds\n",
            "Step 67000/150000, Loss: 5.295537047386169, Test Loss: 5.302645266056061, LR: 0.0005, Elapsed Time: 7020.17 seconds\n",
            "Step 67100/150000, Loss: 5.315203452110291, Test Loss: 5.295942485332489, LR: 0.0005, Elapsed Time: 7030.74 seconds\n",
            "Step 67200/150000, Loss: 5.291923146247864, Test Loss: 5.2943994998931885, LR: 0.0005, Elapsed Time: 7041.31 seconds\n",
            "Step 67300/150000, Loss: 5.2950273036956785, Test Loss: 5.2863006591796875, LR: 0.0005, Elapsed Time: 7051.93 seconds\n",
            "Step 67400/150000, Loss: 5.29222843170166, Test Loss: 5.278313219547272, LR: 0.0005, Elapsed Time: 7062.49 seconds\n",
            "Step 67500/150000, Loss: 5.266166205406189, Test Loss: 5.278250753879547, LR: 0.0005, Elapsed Time: 7073.09 seconds\n",
            "Step 67600/150000, Loss: 5.272240891456604, Test Loss: 5.267304360866547, LR: 0.0005, Elapsed Time: 7083.63 seconds\n",
            "Step 67700/150000, Loss: 5.27227089881897, Test Loss: 5.264086425304413, LR: 0.0005, Elapsed Time: 7094.17 seconds\n",
            "Step 67800/150000, Loss: 5.264011583328247, Test Loss: 5.250308632850647, LR: 0.0005, Elapsed Time: 7104.70 seconds\n",
            "Step 67900/150000, Loss: 5.254223971366883, Test Loss: 5.247463703155518, LR: 0.0005, Elapsed Time: 7115.26 seconds\n",
            "Step 68000/150000, Loss: 5.250198707580567, Test Loss: 5.24625563621521, LR: 0.0005, Elapsed Time: 7125.80 seconds\n",
            "Step 68100/150000, Loss: 5.247724232673645, Test Loss: 5.236429810523987, LR: 0.0005, Elapsed Time: 7136.35 seconds\n",
            "Step 68200/150000, Loss: 5.232674021720886, Test Loss: 5.231915652751923, LR: 0.0005, Elapsed Time: 7146.89 seconds\n",
            "Step 68300/150000, Loss: 5.222946405410767, Test Loss: 5.227723300457001, LR: 0.0005, Elapsed Time: 7157.44 seconds\n",
            "Step 68400/150000, Loss: 5.219474577903748, Test Loss: 5.230738341808319, LR: 0.0005, Elapsed Time: 7167.95 seconds\n",
            "Step 68500/150000, Loss: 5.211254692077636, Test Loss: 5.213435769081116, LR: 0.0005, Elapsed Time: 7178.47 seconds\n",
            "Step 68600/150000, Loss: 5.205923275947571, Test Loss: 5.206523954868317, LR: 0.0005, Elapsed Time: 7188.96 seconds\n",
            "Step 68700/150000, Loss: 5.202418808937073, Test Loss: 5.20121842622757, LR: 0.0005, Elapsed Time: 7199.43 seconds\n",
            "Step 68800/150000, Loss: 5.207288222312927, Test Loss: 5.208313882350922, LR: 0.0005, Elapsed Time: 7209.87 seconds\n",
            "Step 68900/150000, Loss: 5.188586168289184, Test Loss: 5.203306138515472, LR: 0.0005, Elapsed Time: 7220.37 seconds\n",
            "Step 69000/150000, Loss: 5.196520295143127, Test Loss: 5.187402665615082, LR: 0.0005, Elapsed Time: 7230.82 seconds\n",
            "Step 69100/150000, Loss: 5.193294668197632, Test Loss: 5.179160892963409, LR: 0.0005, Elapsed Time: 7241.36 seconds\n",
            "Step 69200/150000, Loss: 5.186762108802795, Test Loss: 5.169788718223572, LR: 0.0005, Elapsed Time: 7251.96 seconds\n",
            "Step 69300/150000, Loss: 5.167738437652588, Test Loss: 5.179641127586365, LR: 0.0005, Elapsed Time: 7262.40 seconds\n",
            "Step 69400/150000, Loss: 5.165948138237, Test Loss: 5.1708632707595825, LR: 0.0005, Elapsed Time: 7272.85 seconds\n",
            "Step 69500/150000, Loss: 5.179466962814331, Test Loss: 5.159137308597565, LR: 0.0005, Elapsed Time: 7283.32 seconds\n",
            "Step 69600/150000, Loss: 5.155001244544983, Test Loss: 5.157383620738983, LR: 0.0005, Elapsed Time: 7293.77 seconds\n",
            "Step 69700/150000, Loss: 5.152563853263855, Test Loss: 5.1452266573905945, LR: 0.0005, Elapsed Time: 7304.18 seconds\n",
            "Step 69800/150000, Loss: 5.136392068862915, Test Loss: 5.144532859325409, LR: 0.0005, Elapsed Time: 7314.54 seconds\n",
            "Step 69900/150000, Loss: 5.143714723587036, Test Loss: 5.1445313692092896, LR: 0.0005, Elapsed Time: 7325.02 seconds\n",
            "Step 70000/150000, Loss: 5.148491468429565, Test Loss: 5.137582063674927, LR: 0.0005, Elapsed Time: 7335.44 seconds\n",
            "Step 70100/150000, Loss: 5.122361621856689, Test Loss: 5.132647454738617, LR: 0.0005, Elapsed Time: 7345.98 seconds\n",
            "Step 70200/150000, Loss: 5.132266206741333, Test Loss: 5.127383291721344, LR: 0.0005, Elapsed Time: 7356.48 seconds\n",
            "Step 70300/150000, Loss: 5.118939661979676, Test Loss: 5.1252601146698, LR: 0.0005, Elapsed Time: 7366.90 seconds\n",
            "Step 70400/150000, Loss: 5.123626437187195, Test Loss: 5.118603408336639, LR: 0.0005, Elapsed Time: 7377.37 seconds\n",
            "Step 70500/150000, Loss: 5.121994657516479, Test Loss: 5.120437800884247, LR: 0.0005, Elapsed Time: 7387.87 seconds\n",
            "Step 70600/150000, Loss: 5.110734448432923, Test Loss: 5.1134419441223145, LR: 0.0005, Elapsed Time: 7398.32 seconds\n",
            "Step 70700/150000, Loss: 5.098849425315857, Test Loss: 5.105983018875122, LR: 0.0005, Elapsed Time: 7408.73 seconds\n",
            "Step 70800/150000, Loss: 5.106832284927368, Test Loss: 5.10359114408493, LR: 0.0005, Elapsed Time: 7419.24 seconds\n",
            "Step 70900/150000, Loss: 5.076606960296631, Test Loss: 5.095781445503235, LR: 0.0005, Elapsed Time: 7429.68 seconds\n",
            "Step 71000/150000, Loss: 5.090879683494568, Test Loss: 5.096704661846161, LR: 0.0005, Elapsed Time: 7440.14 seconds\n",
            "Step 71100/150000, Loss: 5.094643654823304, Test Loss: 5.093998074531555, LR: 0.0005, Elapsed Time: 7450.59 seconds\n",
            "Step 71200/150000, Loss: 5.092743215560913, Test Loss: 5.087096333503723, LR: 0.0005, Elapsed Time: 7461.06 seconds\n",
            "Step 71300/150000, Loss: 5.074050207138061, Test Loss: 5.081305205821991, LR: 0.0005, Elapsed Time: 7471.53 seconds\n",
            "Step 71400/150000, Loss: 5.067913756370545, Test Loss: 5.077621579170227, LR: 0.0005, Elapsed Time: 7481.98 seconds\n",
            "Step 71500/150000, Loss: 5.0640016746521, Test Loss: 5.071775317192078, LR: 0.0005, Elapsed Time: 7492.38 seconds\n",
            "Step 71600/150000, Loss: 5.077561831474304, Test Loss: 5.066811621189117, LR: 0.0005, Elapsed Time: 7502.84 seconds\n",
            "Step 71700/150000, Loss: 5.0586803436279295, Test Loss: 5.0621830224990845, LR: 0.0005, Elapsed Time: 7513.27 seconds\n",
            "Step 71800/150000, Loss: 5.0624367094039915, Test Loss: 5.0649096965789795, LR: 0.0005, Elapsed Time: 7523.73 seconds\n",
            "Step 71900/150000, Loss: 5.056478843688965, Test Loss: 5.053511023521423, LR: 0.0005, Elapsed Time: 7534.23 seconds\n",
            "Step 72000/150000, Loss: 5.047071342468262, Test Loss: 5.0532785058021545, LR: 0.0005, Elapsed Time: 7544.72 seconds\n",
            "Step 72100/150000, Loss: 5.040492033958435, Test Loss: 5.050847828388214, LR: 0.0005, Elapsed Time: 7555.21 seconds\n",
            "Step 72200/150000, Loss: 5.049547748565674, Test Loss: 5.046030402183533, LR: 0.0005, Elapsed Time: 7565.66 seconds\n",
            "Step 72300/150000, Loss: 5.040655527114868, Test Loss: 5.040025115013123, LR: 0.0005, Elapsed Time: 7576.16 seconds\n",
            "Step 72400/150000, Loss: 5.028311057090759, Test Loss: 5.033197104930878, LR: 0.0005, Elapsed Time: 7586.65 seconds\n",
            "Step 72500/150000, Loss: 5.026360702514649, Test Loss: 5.036626815795898, LR: 0.0005, Elapsed Time: 7597.06 seconds\n",
            "Step 72600/150000, Loss: 5.02844470500946, Test Loss: 5.029848277568817, LR: 0.0005, Elapsed Time: 7607.46 seconds\n",
            "Step 72700/150000, Loss: 5.010988907814026, Test Loss: 5.024089813232422, LR: 0.0005, Elapsed Time: 7617.92 seconds\n",
            "Step 72800/150000, Loss: 5.024475946426391, Test Loss: 5.021440744400024, LR: 0.0005, Elapsed Time: 7628.39 seconds\n",
            "Step 72900/150000, Loss: 5.009460544586181, Test Loss: 5.023319959640503, LR: 0.0005, Elapsed Time: 7638.87 seconds\n",
            "Step 73000/150000, Loss: 5.008747835159301, Test Loss: 5.020497381687164, LR: 0.0005, Elapsed Time: 7649.26 seconds\n",
            "Step 73100/150000, Loss: 5.010475697517395, Test Loss: 5.013015627861023, LR: 0.0005, Elapsed Time: 7659.70 seconds\n",
            "Step 73200/150000, Loss: 5.004965076446533, Test Loss: 5.008463144302368, LR: 0.0005, Elapsed Time: 7670.12 seconds\n",
            "Step 73300/150000, Loss: 5.000535912513733, Test Loss: 5.005256116390228, LR: 0.0005, Elapsed Time: 7680.51 seconds\n",
            "Step 73400/150000, Loss: 4.999725069999695, Test Loss: 4.998247385025024, LR: 0.0005, Elapsed Time: 7690.89 seconds\n",
            "Step 73500/150000, Loss: 4.981686358451843, Test Loss: 4.998270750045776, LR: 0.0005, Elapsed Time: 7701.26 seconds\n",
            "Step 73600/150000, Loss: 4.996311163902282, Test Loss: 4.992211997509003, LR: 0.0005, Elapsed Time: 7711.73 seconds\n",
            "Step 73700/150000, Loss: 4.986882858276367, Test Loss: 4.991083979606628, LR: 0.0005, Elapsed Time: 7722.17 seconds\n",
            "Step 73800/150000, Loss: 4.984758992195129, Test Loss: 4.986659586429596, LR: 0.0005, Elapsed Time: 7732.66 seconds\n",
            "Step 73900/150000, Loss: 4.987473340034485, Test Loss: 4.9823867082595825, LR: 0.0005, Elapsed Time: 7743.12 seconds\n",
            "Step 74000/150000, Loss: 4.9866174125671385, Test Loss: 4.980959415435791, LR: 0.0005, Elapsed Time: 7753.65 seconds\n",
            "Step 74100/150000, Loss: 4.97588761806488, Test Loss: 4.983079612255096, LR: 0.0005, Elapsed Time: 7764.09 seconds\n",
            "Step 74200/150000, Loss: 4.970281558036804, Test Loss: 4.973564386367798, LR: 0.0005, Elapsed Time: 7774.54 seconds\n",
            "Step 74300/150000, Loss: 4.9654871320724485, Test Loss: 4.976551711559296, LR: 0.0005, Elapsed Time: 7784.95 seconds\n",
            "Step 74400/150000, Loss: 4.958631463050843, Test Loss: 4.969575524330139, LR: 0.0005, Elapsed Time: 7795.38 seconds\n",
            "Step 74500/150000, Loss: 4.966002478599548, Test Loss: 4.961613774299622, LR: 0.0005, Elapsed Time: 7805.80 seconds\n",
            "Step 74600/150000, Loss: 4.962967591285706, Test Loss: 4.958652496337891, LR: 0.0005, Elapsed Time: 7816.20 seconds\n",
            "Step 74700/150000, Loss: 4.94449866771698, Test Loss: 4.959629416465759, LR: 0.0005, Elapsed Time: 7826.62 seconds\n",
            "Step 74800/150000, Loss: 4.955770788192749, Test Loss: 4.96161276102066, LR: 0.0005, Elapsed Time: 7837.09 seconds\n",
            "Step 74900/150000, Loss: 4.961240215301514, Test Loss: 4.954391121864319, LR: 0.0005, Elapsed Time: 7847.62 seconds\n",
            "Step 75000/150000, Loss: 4.9420176601409915, Test Loss: 4.95599091053009, LR: 0.0005, Elapsed Time: 7858.16 seconds\n",
            "Step 75100/150000, Loss: 4.93077244758606, Test Loss: 4.947518229484558, LR: 0.0005, Elapsed Time: 7868.61 seconds\n",
            "Step 75200/150000, Loss: 4.95118049621582, Test Loss: 4.947926223278046, LR: 0.0005, Elapsed Time: 7879.05 seconds\n",
            "Step 75300/150000, Loss: 4.941116032600402, Test Loss: 4.948946833610535, LR: 0.0005, Elapsed Time: 7889.53 seconds\n",
            "Step 75400/150000, Loss: 4.935917811393738, Test Loss: 4.9424973130226135, LR: 0.0005, Elapsed Time: 7900.00 seconds\n",
            "Step 75500/150000, Loss: 4.93545660495758, Test Loss: 4.941090524196625, LR: 0.0005, Elapsed Time: 7910.41 seconds\n",
            "Step 75600/150000, Loss: 4.921295375823974, Test Loss: 4.934930860996246, LR: 0.0005, Elapsed Time: 7920.88 seconds\n",
            "Step 75700/150000, Loss: 4.931679763793945, Test Loss: 4.931283056735992, LR: 0.0005, Elapsed Time: 7931.33 seconds\n",
            "Step 75800/150000, Loss: 4.928640727996826, Test Loss: 4.932248950004578, LR: 0.0005, Elapsed Time: 7941.79 seconds\n",
            "Step 75900/150000, Loss: 4.927924003601074, Test Loss: 4.93260133266449, LR: 0.0005, Elapsed Time: 7952.21 seconds\n",
            "Step 76000/150000, Loss: 4.924715065956116, Test Loss: 4.926815152168274, LR: 0.0005, Elapsed Time: 7962.61 seconds\n",
            "Step 76100/150000, Loss: 4.925424189567566, Test Loss: 4.919170677661896, LR: 0.0005, Elapsed Time: 7973.01 seconds\n",
            "Step 76200/150000, Loss: 4.925184230804444, Test Loss: 4.922469258308411, LR: 0.0005, Elapsed Time: 7983.44 seconds\n",
            "Step 76300/150000, Loss: 4.914506430625916, Test Loss: 4.91369765996933, LR: 0.0005, Elapsed Time: 7993.87 seconds\n",
            "Step 76400/150000, Loss: 4.91730833530426, Test Loss: 4.91402143239975, LR: 0.0005, Elapsed Time: 8004.26 seconds\n",
            "Step 76500/150000, Loss: 4.913875842094422, Test Loss: 4.910737991333008, LR: 0.0005, Elapsed Time: 8014.66 seconds\n",
            "Step 76600/150000, Loss: 4.921055026054383, Test Loss: 4.909217119216919, LR: 0.0005, Elapsed Time: 8025.09 seconds\n",
            "Step 76700/150000, Loss: 4.8913210582733155, Test Loss: 4.909197807312012, LR: 0.0005, Elapsed Time: 8035.48 seconds\n",
            "Step 76800/150000, Loss: 4.897018671035767, Test Loss: 4.904219090938568, LR: 0.0005, Elapsed Time: 8045.90 seconds\n",
            "Step 76900/150000, Loss: 4.903208332061768, Test Loss: 4.900220036506653, LR: 0.0005, Elapsed Time: 8056.32 seconds\n",
            "Step 77000/150000, Loss: 4.8877551555633545, Test Loss: 4.897697925567627, LR: 0.0005, Elapsed Time: 8066.72 seconds\n",
            "Step 77100/150000, Loss: 4.89857008934021, Test Loss: 4.895069897174835, LR: 0.0005, Elapsed Time: 8077.15 seconds\n",
            "Step 77200/150000, Loss: 4.89669716835022, Test Loss: 4.895212292671204, LR: 0.0005, Elapsed Time: 8087.60 seconds\n",
            "Step 77300/150000, Loss: 4.895123252868652, Test Loss: 4.895405411720276, LR: 0.0005, Elapsed Time: 8098.04 seconds\n",
            "Step 77400/150000, Loss: 4.881409091949463, Test Loss: 4.892144501209259, LR: 0.0005, Elapsed Time: 8108.51 seconds\n",
            "Step 77500/150000, Loss: 4.8801361131668095, Test Loss: 4.888567507266998, LR: 0.0005, Elapsed Time: 8118.98 seconds\n",
            "Step 77600/150000, Loss: 4.883018622398376, Test Loss: 4.883805990219116, LR: 0.0005, Elapsed Time: 8129.43 seconds\n",
            "Step 77700/150000, Loss: 4.891428399085998, Test Loss: 4.884770929813385, LR: 0.0005, Elapsed Time: 8139.89 seconds\n",
            "Step 77800/150000, Loss: 4.891836972236633, Test Loss: 4.883958518505096, LR: 0.0005, Elapsed Time: 8150.32 seconds\n",
            "Step 77900/150000, Loss: 4.873677730560303, Test Loss: 4.881491720676422, LR: 0.0005, Elapsed Time: 8160.81 seconds\n",
            "Step 78000/150000, Loss: 4.87708881855011, Test Loss: 4.875210344791412, LR: 0.0005, Elapsed Time: 8171.30 seconds\n",
            "Step 78100/150000, Loss: 4.865730929374695, Test Loss: 4.874570250511169, LR: 0.0005, Elapsed Time: 8181.81 seconds\n",
            "Step 78200/150000, Loss: 4.875864462852478, Test Loss: 4.872153460979462, LR: 0.0005, Elapsed Time: 8192.30 seconds\n",
            "Step 78300/150000, Loss: 4.8768160438537596, Test Loss: 4.870166897773743, LR: 0.0005, Elapsed Time: 8202.74 seconds\n",
            "Step 78400/150000, Loss: 4.862622671127319, Test Loss: 4.868126332759857, LR: 0.0005, Elapsed Time: 8213.23 seconds\n",
            "Step 78500/150000, Loss: 4.869111142158508, Test Loss: 4.869178414344788, LR: 0.0005, Elapsed Time: 8223.63 seconds\n",
            "Step 78600/150000, Loss: 4.863295907974243, Test Loss: 4.867348253726959, LR: 0.0005, Elapsed Time: 8234.13 seconds\n",
            "Step 78700/150000, Loss: 4.855797481536865, Test Loss: 4.863194048404694, LR: 0.0005, Elapsed Time: 8244.58 seconds\n",
            "Step 78800/150000, Loss: 4.851744694709778, Test Loss: 4.856317698955536, LR: 0.0005, Elapsed Time: 8255.12 seconds\n",
            "Step 78900/150000, Loss: 4.85481614112854, Test Loss: 4.857385039329529, LR: 0.0005, Elapsed Time: 8265.53 seconds\n",
            "Step 79000/150000, Loss: 4.865416345596313, Test Loss: 4.855992913246155, LR: 0.0005, Elapsed Time: 8275.95 seconds\n",
            "Step 79100/150000, Loss: 4.850973930358887, Test Loss: 4.851654231548309, LR: 0.0005, Elapsed Time: 8286.44 seconds\n",
            "Step 79200/150000, Loss: 4.842926735877991, Test Loss: 4.84807151556015, LR: 0.0005, Elapsed Time: 8296.85 seconds\n",
            "Step 79300/150000, Loss: 4.843914189338684, Test Loss: 4.850727438926697, LR: 0.0005, Elapsed Time: 8307.27 seconds\n",
            "Step 79400/150000, Loss: 4.8443244934082035, Test Loss: 4.847101151943207, LR: 0.0005, Elapsed Time: 8317.70 seconds\n",
            "Step 79500/150000, Loss: 4.840743160247802, Test Loss: 4.8469207882881165, LR: 0.0005, Elapsed Time: 8328.13 seconds\n",
            "Step 79600/150000, Loss: 4.846686606407165, Test Loss: 4.850731194019318, LR: 0.0005, Elapsed Time: 8338.54 seconds\n",
            "Step 79700/150000, Loss: 4.834115505218506, Test Loss: 4.837089955806732, LR: 0.0005, Elapsed Time: 8349.02 seconds\n",
            "Step 79800/150000, Loss: 4.845659132003784, Test Loss: 4.837838411331177, LR: 0.0005, Elapsed Time: 8359.49 seconds\n",
            "Step 79900/150000, Loss: 4.835200510025024, Test Loss: 4.844934523105621, LR: 0.0005, Elapsed Time: 8369.94 seconds\n",
            "Step 80000/150000, Loss: 4.832617559432983, Test Loss: 4.837338924407959, LR: 0.0005, Elapsed Time: 8380.40 seconds\n",
            "Step 80100/150000, Loss: 4.839513635635376, Test Loss: 4.835440814495087, LR: 0.0005, Elapsed Time: 8390.82 seconds\n",
            "Step 80200/150000, Loss: 4.8298465394973755, Test Loss: 4.839035391807556, LR: 0.0005, Elapsed Time: 8401.20 seconds\n",
            "Step 80300/150000, Loss: 4.816262397766113, Test Loss: 4.835460960865021, LR: 0.0005, Elapsed Time: 8411.67 seconds\n",
            "Step 80400/150000, Loss: 4.82129798412323, Test Loss: 4.835106730461121, LR: 0.0005, Elapsed Time: 8422.12 seconds\n",
            "Step 80500/150000, Loss: 4.824140167236328, Test Loss: 4.825436234474182, LR: 0.0005, Elapsed Time: 8432.53 seconds\n",
            "Step 80600/150000, Loss: 4.822312550544739, Test Loss: 4.827188849449158, LR: 0.0005, Elapsed Time: 8442.96 seconds\n",
            "Step 80700/150000, Loss: 4.826302876472473, Test Loss: 4.824887692928314, LR: 0.0005, Elapsed Time: 8453.41 seconds\n",
            "Step 80800/150000, Loss: 4.8115832710266115, Test Loss: 4.8238485455513, LR: 0.0005, Elapsed Time: 8463.85 seconds\n",
            "Step 80900/150000, Loss: 4.804930920600891, Test Loss: 4.818974733352661, LR: 0.0005, Elapsed Time: 8474.33 seconds\n",
            "Step 81000/150000, Loss: 4.804895825386048, Test Loss: 4.821969389915466, LR: 0.0005, Elapsed Time: 8484.81 seconds\n",
            "Step 81100/150000, Loss: 4.82920756816864, Test Loss: 4.818613529205322, LR: 0.0005, Elapsed Time: 8495.24 seconds\n",
            "Step 81200/150000, Loss: 4.800021405220032, Test Loss: 4.818397521972656, LR: 0.0005, Elapsed Time: 8505.69 seconds\n",
            "Step 81300/150000, Loss: 4.810849533081055, Test Loss: 4.811725497245789, LR: 0.0005, Elapsed Time: 8516.12 seconds\n",
            "Step 81400/150000, Loss: 4.816287121772766, Test Loss: 4.810984492301941, LR: 0.0005, Elapsed Time: 8526.56 seconds\n",
            "Step 81500/150000, Loss: 4.797459397315979, Test Loss: 4.812243342399597, LR: 0.0005, Elapsed Time: 8537.02 seconds\n",
            "Step 81600/150000, Loss: 4.797677145004273, Test Loss: 4.815264642238617, LR: 0.0005, Elapsed Time: 8547.40 seconds\n",
            "Step 81700/150000, Loss: 4.798846735954284, Test Loss: 4.809240281581879, LR: 0.0005, Elapsed Time: 8557.82 seconds\n",
            "Step 81800/150000, Loss: 4.810451393127441, Test Loss: 4.8037514090538025, LR: 0.0005, Elapsed Time: 8568.23 seconds\n",
            "Step 81900/150000, Loss: 4.797507877349854, Test Loss: 4.803799033164978, LR: 0.0005, Elapsed Time: 8578.68 seconds\n",
            "Step 82000/150000, Loss: 4.80497905254364, Test Loss: 4.803577899932861, LR: 0.0005, Elapsed Time: 8589.10 seconds\n",
            "Step 82100/150000, Loss: 4.792753076553344, Test Loss: 4.799163401126862, LR: 0.0005, Elapsed Time: 8599.56 seconds\n",
            "Step 82200/150000, Loss: 4.796319851875305, Test Loss: 4.79492312669754, LR: 0.0005, Elapsed Time: 8609.96 seconds\n",
            "Step 82300/150000, Loss: 4.801247510910034, Test Loss: 4.792162656784058, LR: 0.0005, Elapsed Time: 8620.40 seconds\n",
            "Step 82400/150000, Loss: 4.79914915561676, Test Loss: 4.796527743339539, LR: 0.0005, Elapsed Time: 8630.90 seconds\n",
            "Step 82500/150000, Loss: 4.789124612808227, Test Loss: 4.792892754077911, LR: 0.0005, Elapsed Time: 8641.39 seconds\n",
            "Step 82600/150000, Loss: 4.786045956611633, Test Loss: 4.797091484069824, LR: 0.0005, Elapsed Time: 8651.86 seconds\n",
            "Step 82700/150000, Loss: 4.782631182670594, Test Loss: 4.79128235578537, LR: 0.0005, Elapsed Time: 8662.35 seconds\n",
            "Step 82800/150000, Loss: 4.794667248725891, Test Loss: 4.784290075302124, LR: 0.0005, Elapsed Time: 8672.84 seconds\n",
            "Step 82900/150000, Loss: 4.785751247406006, Test Loss: 4.7843780517578125, LR: 0.0005, Elapsed Time: 8683.31 seconds\n",
            "Step 83000/150000, Loss: 4.799305410385132, Test Loss: 4.789454162120819, LR: 0.0005, Elapsed Time: 8693.73 seconds\n",
            "Step 83100/150000, Loss: 4.761109509468079, Test Loss: 4.782617628574371, LR: 0.0005, Elapsed Time: 8704.13 seconds\n",
            "Step 83200/150000, Loss: 4.770145978927612, Test Loss: 4.77720183134079, LR: 0.0005, Elapsed Time: 8714.57 seconds\n",
            "Step 83300/150000, Loss: 4.778371877670288, Test Loss: 4.776857376098633, LR: 0.0005, Elapsed Time: 8725.03 seconds\n",
            "Step 83400/150000, Loss: 4.7728280210494995, Test Loss: 4.779247999191284, LR: 0.0005, Elapsed Time: 8735.48 seconds\n",
            "Step 83500/150000, Loss: 4.785246453285217, Test Loss: 4.780354678630829, LR: 0.0005, Elapsed Time: 8745.90 seconds\n",
            "Step 83600/150000, Loss: 4.77032576084137, Test Loss: 4.77236670255661, LR: 0.0005, Elapsed Time: 8756.30 seconds\n",
            "Step 83700/150000, Loss: 4.779432344436645, Test Loss: 4.771672308444977, LR: 0.0005, Elapsed Time: 8766.78 seconds\n",
            "Step 83800/150000, Loss: 4.763528113365173, Test Loss: 4.772262454032898, LR: 0.0005, Elapsed Time: 8777.26 seconds\n",
            "Step 83900/150000, Loss: 4.772539877891541, Test Loss: 4.7759690284729, LR: 0.0005, Elapsed Time: 8787.75 seconds\n",
            "Step 84000/150000, Loss: 4.7568769454956055, Test Loss: 4.7726582288742065, LR: 0.0005, Elapsed Time: 8798.27 seconds\n",
            "Step 84100/150000, Loss: 4.768244895935059, Test Loss: 4.771349132061005, LR: 0.0005, Elapsed Time: 8808.84 seconds\n",
            "Step 84200/150000, Loss: 4.773479900360107, Test Loss: 4.771948516368866, LR: 0.0005, Elapsed Time: 8819.40 seconds\n",
            "Step 84300/150000, Loss: 4.770772023200989, Test Loss: 4.764985084533691, LR: 0.0005, Elapsed Time: 8829.95 seconds\n",
            "Step 84400/150000, Loss: 4.7610893201828, Test Loss: 4.764857113361359, LR: 0.0005, Elapsed Time: 8840.44 seconds\n",
            "Step 84500/150000, Loss: 4.7670999526977536, Test Loss: 4.7675281167030334, LR: 0.0005, Elapsed Time: 8850.95 seconds\n",
            "Step 84600/150000, Loss: 4.754218950271606, Test Loss: 4.761522591114044, LR: 0.0005, Elapsed Time: 8861.48 seconds\n",
            "Step 84700/150000, Loss: 4.761878137588501, Test Loss: 4.763967514038086, LR: 0.0005, Elapsed Time: 8872.01 seconds\n",
            "Step 84800/150000, Loss: 4.757338399887085, Test Loss: 4.75696074962616, LR: 0.0005, Elapsed Time: 8882.48 seconds\n",
            "Step 84900/150000, Loss: 4.740103931427002, Test Loss: 4.757577836513519, LR: 0.0005, Elapsed Time: 8892.93 seconds\n",
            "Step 85000/150000, Loss: 4.7656351709365845, Test Loss: 4.758599102497101, LR: 0.0005, Elapsed Time: 8903.39 seconds\n",
            "Step 85100/150000, Loss: 4.740175333023071, Test Loss: 4.7591472864151, LR: 0.0005, Elapsed Time: 8913.85 seconds\n",
            "Step 85200/150000, Loss: 4.753162198066711, Test Loss: 4.757027328014374, LR: 0.0005, Elapsed Time: 8924.31 seconds\n",
            "Step 85300/150000, Loss: 4.7452677679061885, Test Loss: 4.75495845079422, LR: 0.0005, Elapsed Time: 8934.81 seconds\n",
            "Step 85400/150000, Loss: 4.755494880676269, Test Loss: 4.754721760749817, LR: 0.0005, Elapsed Time: 8945.27 seconds\n",
            "Step 85500/150000, Loss: 4.748078365325927, Test Loss: 4.7513309717178345, LR: 0.0005, Elapsed Time: 8955.80 seconds\n",
            "Step 85600/150000, Loss: 4.754419569969177, Test Loss: 4.743835270404816, LR: 0.0005, Elapsed Time: 8966.33 seconds\n",
            "Step 85700/150000, Loss: 4.747055330276489, Test Loss: 4.748860597610474, LR: 0.0005, Elapsed Time: 8976.81 seconds\n",
            "Step 85800/150000, Loss: 4.744970512390137, Test Loss: 4.743977963924408, LR: 0.0005, Elapsed Time: 8987.34 seconds\n",
            "Step 85900/150000, Loss: 4.73249361038208, Test Loss: 4.746781229972839, LR: 0.0005, Elapsed Time: 8997.79 seconds\n",
            "Step 86000/150000, Loss: 4.743886895179749, Test Loss: 4.740697920322418, LR: 0.0005, Elapsed Time: 9008.26 seconds\n",
            "Step 86100/150000, Loss: 4.736913247108459, Test Loss: 4.746123731136322, LR: 0.0005, Elapsed Time: 9018.71 seconds\n",
            "Step 86200/150000, Loss: 4.729964547157287, Test Loss: 4.739628314971924, LR: 0.0005, Elapsed Time: 9029.17 seconds\n",
            "Step 86300/150000, Loss: 4.742151613235474, Test Loss: 4.7444722056388855, LR: 0.0005, Elapsed Time: 9039.69 seconds\n",
            "Step 86400/150000, Loss: 4.730655908584595, Test Loss: 4.734647810459137, LR: 0.0005, Elapsed Time: 9050.12 seconds\n",
            "Step 86500/150000, Loss: 4.725906023979187, Test Loss: 4.742446005344391, LR: 0.0005, Elapsed Time: 9060.56 seconds\n",
            "Step 86600/150000, Loss: 4.746572585105896, Test Loss: 4.738179326057434, LR: 0.0005, Elapsed Time: 9071.02 seconds\n",
            "Step 86700/150000, Loss: 4.732806754112244, Test Loss: 4.735890448093414, LR: 0.0005, Elapsed Time: 9081.51 seconds\n",
            "Step 86800/150000, Loss: 4.732785940170288, Test Loss: 4.733982443809509, LR: 0.0005, Elapsed Time: 9091.95 seconds\n",
            "Step 86900/150000, Loss: 4.74584538936615, Test Loss: 4.736945807933807, LR: 0.0005, Elapsed Time: 9102.49 seconds\n",
            "Step 87000/150000, Loss: 4.725944318771362, Test Loss: 4.731871545314789, LR: 0.0005, Elapsed Time: 9112.93 seconds\n",
            "Step 87100/150000, Loss: 4.742381439208985, Test Loss: 4.727733254432678, LR: 0.0005, Elapsed Time: 9123.37 seconds\n",
            "Step 87200/150000, Loss: 4.722770009040833, Test Loss: 4.724575638771057, LR: 0.0005, Elapsed Time: 9133.83 seconds\n",
            "Step 87300/150000, Loss: 4.732470784187317, Test Loss: 4.725083649158478, LR: 0.0005, Elapsed Time: 9144.31 seconds\n",
            "Step 87400/150000, Loss: 4.708624377250671, Test Loss: 4.731218636035919, LR: 0.0005, Elapsed Time: 9154.75 seconds\n",
            "Step 87500/150000, Loss: 4.7362952280044555, Test Loss: 4.724602282047272, LR: 0.0005, Elapsed Time: 9165.24 seconds\n",
            "Step 87600/150000, Loss: 4.714593472480774, Test Loss: 4.72144228219986, LR: 0.0005, Elapsed Time: 9175.71 seconds\n",
            "Step 87700/150000, Loss: 4.719215745925903, Test Loss: 4.719575464725494, LR: 0.0005, Elapsed Time: 9186.19 seconds\n",
            "Step 87800/150000, Loss: 4.723467698097229, Test Loss: 4.724494278430939, LR: 0.0005, Elapsed Time: 9196.60 seconds\n",
            "Step 87900/150000, Loss: 4.716660871505737, Test Loss: 4.7248504757881165, LR: 0.0005, Elapsed Time: 9207.01 seconds\n",
            "Step 88000/150000, Loss: 4.723094177246094, Test Loss: 4.7154620885849, LR: 0.0005, Elapsed Time: 9217.45 seconds\n",
            "Step 88100/150000, Loss: 4.715024509429932, Test Loss: 4.714540064334869, LR: 0.0005, Elapsed Time: 9227.88 seconds\n",
            "Step 88200/150000, Loss: 4.724880037307739, Test Loss: 4.719093859195709, LR: 0.0005, Elapsed Time: 9238.31 seconds\n",
            "Step 88300/150000, Loss: 4.713695855140686, Test Loss: 4.723946630954742, LR: 0.0005, Elapsed Time: 9248.74 seconds\n",
            "Step 88400/150000, Loss: 4.706941809654236, Test Loss: 4.7153637409210205, LR: 0.0005, Elapsed Time: 9259.15 seconds\n",
            "Step 88500/150000, Loss: 4.695304732322693, Test Loss: 4.7155386209487915, LR: 0.0005, Elapsed Time: 9269.53 seconds\n",
            "Step 88600/150000, Loss: 4.708195424079895, Test Loss: 4.71875137090683, LR: 0.0005, Elapsed Time: 9279.98 seconds\n",
            "Step 88700/150000, Loss: 4.7044768714904786, Test Loss: 4.71229875087738, LR: 0.0005, Elapsed Time: 9290.49 seconds\n",
            "Step 88800/150000, Loss: 4.696226706504822, Test Loss: 4.712920665740967, LR: 0.0005, Elapsed Time: 9300.89 seconds\n",
            "Step 88900/150000, Loss: 4.704901070594787, Test Loss: 4.712480962276459, LR: 0.0005, Elapsed Time: 9311.29 seconds\n",
            "Step 89000/150000, Loss: 4.718432235717773, Test Loss: 4.705884158611298, LR: 0.0005, Elapsed Time: 9321.67 seconds\n",
            "Step 89100/150000, Loss: 4.702246980667114, Test Loss: 4.705253064632416, LR: 0.0005, Elapsed Time: 9332.07 seconds\n",
            "Step 89200/150000, Loss: 4.703167734146118, Test Loss: 4.7067548632621765, LR: 0.0005, Elapsed Time: 9342.47 seconds\n",
            "Step 89300/150000, Loss: 4.712045331001281, Test Loss: 4.710320353507996, LR: 0.0005, Elapsed Time: 9352.99 seconds\n",
            "Step 89400/150000, Loss: 4.690465898513794, Test Loss: 4.70288223028183, LR: 0.0005, Elapsed Time: 9363.43 seconds\n",
            "Step 89500/150000, Loss: 4.701580586433411, Test Loss: 4.701967239379883, LR: 0.0005, Elapsed Time: 9373.83 seconds\n",
            "Step 89600/150000, Loss: 4.709697279930115, Test Loss: 4.702018618583679, LR: 0.0005, Elapsed Time: 9384.22 seconds\n",
            "Step 89700/150000, Loss: 4.69653694152832, Test Loss: 4.701568067073822, LR: 0.0005, Elapsed Time: 9394.66 seconds\n",
            "Step 89800/150000, Loss: 4.698929047584533, Test Loss: 4.699881494045258, LR: 0.0005, Elapsed Time: 9405.14 seconds\n",
            "Step 89900/150000, Loss: 4.698907523155213, Test Loss: 4.698285818099976, LR: 0.0005, Elapsed Time: 9415.58 seconds\n",
            "Step 90000/150000, Loss: 4.694145135879516, Test Loss: 4.6999791264534, LR: 0.0005, Elapsed Time: 9426.02 seconds\n",
            "Step 90100/150000, Loss: 4.693687815666198, Test Loss: 4.698132336139679, LR: 0.0005, Elapsed Time: 9436.48 seconds\n",
            "Step 90200/150000, Loss: 4.685910682678223, Test Loss: 4.693688452243805, LR: 0.0005, Elapsed Time: 9446.91 seconds\n",
            "Step 90300/150000, Loss: 4.687323231697082, Test Loss: 4.6941112875938416, LR: 0.0005, Elapsed Time: 9457.29 seconds\n",
            "Step 90400/150000, Loss: 4.691967983245849, Test Loss: 4.6985161900520325, LR: 0.0005, Elapsed Time: 9467.68 seconds\n",
            "Step 90500/150000, Loss: 4.685854907035828, Test Loss: 4.692946970462799, LR: 0.0005, Elapsed Time: 9478.07 seconds\n",
            "Step 90600/150000, Loss: 4.683881001472473, Test Loss: 4.694627404212952, LR: 0.0005, Elapsed Time: 9488.47 seconds\n",
            "Step 90700/150000, Loss: 4.6915514850616455, Test Loss: 4.694908678531647, LR: 0.0005, Elapsed Time: 9498.94 seconds\n",
            "Step 90800/150000, Loss: 4.689019637107849, Test Loss: 4.692712366580963, LR: 0.0005, Elapsed Time: 9509.33 seconds\n",
            "Step 90900/150000, Loss: 4.690668997764587, Test Loss: 4.690618216991425, LR: 0.0005, Elapsed Time: 9519.75 seconds\n",
            "Step 91000/150000, Loss: 4.694623308181763, Test Loss: 4.688620090484619, LR: 0.0005, Elapsed Time: 9530.22 seconds\n",
            "Step 91100/150000, Loss: 4.693570137023926, Test Loss: 4.689449310302734, LR: 0.0005, Elapsed Time: 9540.62 seconds\n",
            "Step 91200/150000, Loss: 4.68170777797699, Test Loss: 4.692605435848236, LR: 0.0005, Elapsed Time: 9551.09 seconds\n",
            "Step 91300/150000, Loss: 4.6848638725280765, Test Loss: 4.690497636795044, LR: 0.0005, Elapsed Time: 9561.63 seconds\n",
            "Step 91400/150000, Loss: 4.687048277854919, Test Loss: 4.683878183364868, LR: 0.0005, Elapsed Time: 9572.22 seconds\n",
            "Step 91500/150000, Loss: 4.687886872291565, Test Loss: 4.6849524974823, LR: 0.0005, Elapsed Time: 9582.77 seconds\n",
            "Step 91600/150000, Loss: 4.667567391395568, Test Loss: 4.679482638835907, LR: 0.0005, Elapsed Time: 9593.32 seconds\n",
            "Step 91700/150000, Loss: 4.672996950149536, Test Loss: 4.67811793088913, LR: 0.0005, Elapsed Time: 9603.77 seconds\n",
            "Step 91800/150000, Loss: 4.680358791351319, Test Loss: 4.683247745037079, LR: 0.0005, Elapsed Time: 9614.28 seconds\n",
            "Step 91900/150000, Loss: 4.67656976222992, Test Loss: 4.6828858852386475, LR: 0.0005, Elapsed Time: 9624.75 seconds\n",
            "Step 92000/150000, Loss: 4.674556722640991, Test Loss: 4.68213951587677, LR: 0.0005, Elapsed Time: 9635.16 seconds\n",
            "Step 92100/150000, Loss: 4.676723132133484, Test Loss: 4.6823811531066895, LR: 0.0005, Elapsed Time: 9645.66 seconds\n",
            "Step 92200/150000, Loss: 4.674354152679443, Test Loss: 4.676217079162598, LR: 0.0005, Elapsed Time: 9656.12 seconds\n",
            "Step 92300/150000, Loss: 4.681895899772644, Test Loss: 4.684450745582581, LR: 0.0005, Elapsed Time: 9666.58 seconds\n",
            "Step 92400/150000, Loss: 4.679564142227173, Test Loss: 4.678245842456818, LR: 0.0005, Elapsed Time: 9677.02 seconds\n",
            "Step 92500/150000, Loss: 4.661452374458313, Test Loss: 4.6795238852500916, LR: 0.0005, Elapsed Time: 9687.45 seconds\n",
            "Step 92600/150000, Loss: 4.663585071563721, Test Loss: 4.673865914344788, LR: 0.0005, Elapsed Time: 9697.90 seconds\n",
            "Step 92700/150000, Loss: 4.672209591865539, Test Loss: 4.679299831390381, LR: 0.0005, Elapsed Time: 9708.32 seconds\n",
            "Step 92800/150000, Loss: 4.659889693260193, Test Loss: 4.678571283817291, LR: 0.0005, Elapsed Time: 9718.69 seconds\n",
            "Step 92900/150000, Loss: 4.667638483047486, Test Loss: 4.677476406097412, LR: 0.0005, Elapsed Time: 9729.09 seconds\n",
            "Step 93000/150000, Loss: 4.665724058151245, Test Loss: 4.667974412441254, LR: 0.0005, Elapsed Time: 9739.54 seconds\n",
            "Step 93100/150000, Loss: 4.672451729774475, Test Loss: 4.676164627075195, LR: 0.0005, Elapsed Time: 9749.94 seconds\n",
            "Step 93200/150000, Loss: 4.6606781816482545, Test Loss: 4.671724379062653, LR: 0.0005, Elapsed Time: 9760.35 seconds\n",
            "Step 93300/150000, Loss: 4.650898904800415, Test Loss: 4.670389354228973, LR: 0.0005, Elapsed Time: 9770.72 seconds\n",
            "Step 93400/150000, Loss: 4.659914307594299, Test Loss: 4.672396183013916, LR: 0.0005, Elapsed Time: 9781.21 seconds\n",
            "Step 93500/150000, Loss: 4.674565973281861, Test Loss: 4.668088674545288, LR: 0.0005, Elapsed Time: 9791.69 seconds\n",
            "Step 93600/150000, Loss: 4.657446746826172, Test Loss: 4.6658895611763, LR: 0.0005, Elapsed Time: 9802.15 seconds\n",
            "Step 93700/150000, Loss: 4.664053239822388, Test Loss: 4.665336430072784, LR: 0.0005, Elapsed Time: 9812.61 seconds\n",
            "Step 93800/150000, Loss: 4.662183227539063, Test Loss: 4.6684606075286865, LR: 0.0005, Elapsed Time: 9823.06 seconds\n",
            "Step 93900/150000, Loss: 4.6505805778503415, Test Loss: 4.666433274745941, LR: 0.0005, Elapsed Time: 9833.49 seconds\n",
            "Step 94000/150000, Loss: 4.654260153770447, Test Loss: 4.666564524173737, LR: 0.0005, Elapsed Time: 9843.91 seconds\n",
            "Step 94100/150000, Loss: 4.66292402267456, Test Loss: 4.6626468896865845, LR: 0.0005, Elapsed Time: 9854.41 seconds\n",
            "Step 94200/150000, Loss: 4.660408835411072, Test Loss: 4.663703203201294, LR: 0.0005, Elapsed Time: 9864.92 seconds\n",
            "Step 94300/150000, Loss: 4.645924735069275, Test Loss: 4.655661582946777, LR: 0.0005, Elapsed Time: 9875.46 seconds\n",
            "Step 94400/150000, Loss: 4.650605092048645, Test Loss: 4.658944606781006, LR: 0.0005, Elapsed Time: 9885.95 seconds\n",
            "Step 94500/150000, Loss: 4.6522779512405394, Test Loss: 4.658105671405792, LR: 0.0005, Elapsed Time: 9896.46 seconds\n",
            "Step 94600/150000, Loss: 4.643304872512817, Test Loss: 4.659991383552551, LR: 0.0005, Elapsed Time: 9906.87 seconds\n",
            "Step 94700/150000, Loss: 4.650188775062561, Test Loss: 4.655910968780518, LR: 0.0005, Elapsed Time: 9917.27 seconds\n",
            "Step 94800/150000, Loss: 4.644950227737427, Test Loss: 4.65347558259964, LR: 0.0005, Elapsed Time: 9927.73 seconds\n",
            "Step 94900/150000, Loss: 4.655018644332886, Test Loss: 4.652925968170166, LR: 0.0005, Elapsed Time: 9938.23 seconds\n",
            "Step 95000/150000, Loss: 4.64109251499176, Test Loss: 4.6555516719818115, LR: 0.0005, Elapsed Time: 9948.70 seconds\n",
            "Step 95100/150000, Loss: 4.647016501426696, Test Loss: 4.65079939365387, LR: 0.0005, Elapsed Time: 9959.20 seconds\n",
            "Step 95200/150000, Loss: 4.650451273918152, Test Loss: 4.653127074241638, LR: 0.0005, Elapsed Time: 9969.71 seconds\n",
            "Step 95300/150000, Loss: 4.637032742500305, Test Loss: 4.653068959712982, LR: 0.0005, Elapsed Time: 9980.21 seconds\n",
            "Step 95400/150000, Loss: 4.643475980758667, Test Loss: 4.649152159690857, LR: 0.0005, Elapsed Time: 9990.67 seconds\n",
            "Step 95500/150000, Loss: 4.635945873260498, Test Loss: 4.651387095451355, LR: 0.0005, Elapsed Time: 10001.12 seconds\n",
            "Step 95600/150000, Loss: 4.642398982048035, Test Loss: 4.652744114398956, LR: 0.0005, Elapsed Time: 10011.59 seconds\n",
            "Step 95700/150000, Loss: 4.6379948282241825, Test Loss: 4.652747809886932, LR: 0.0005, Elapsed Time: 10022.07 seconds\n",
            "Step 95800/150000, Loss: 4.651347227096558, Test Loss: 4.646367132663727, LR: 0.0005, Elapsed Time: 10032.51 seconds\n",
            "Step 95900/150000, Loss: 4.638998565673828, Test Loss: 4.645727097988129, LR: 0.0005, Elapsed Time: 10042.93 seconds\n",
            "Step 96000/150000, Loss: 4.644238095283509, Test Loss: 4.646568834781647, LR: 0.0005, Elapsed Time: 10053.35 seconds\n",
            "Step 96100/150000, Loss: 4.637891778945923, Test Loss: 4.647608697414398, LR: 0.0005, Elapsed Time: 10063.81 seconds\n",
            "Step 96200/150000, Loss: 4.633581829071045, Test Loss: 4.646522760391235, LR: 0.0005, Elapsed Time: 10074.22 seconds\n",
            "Step 96300/150000, Loss: 4.62905830860138, Test Loss: 4.643860340118408, LR: 0.0005, Elapsed Time: 10084.81 seconds\n",
            "Step 96400/150000, Loss: 4.638504371643067, Test Loss: 4.641965210437775, LR: 0.0005, Elapsed Time: 10095.45 seconds\n",
            "Step 96500/150000, Loss: 4.643032631874084, Test Loss: 4.642145156860352, LR: 0.0005, Elapsed Time: 10106.03 seconds\n",
            "Step 96600/150000, Loss: 4.6287583112716675, Test Loss: 4.6412633061409, LR: 0.0005, Elapsed Time: 10116.47 seconds\n",
            "Step 96700/150000, Loss: 4.6401792335510255, Test Loss: 4.640094876289368, LR: 0.0005, Elapsed Time: 10126.91 seconds\n",
            "Step 96800/150000, Loss: 4.640670771598816, Test Loss: 4.6432560086250305, LR: 0.0005, Elapsed Time: 10137.32 seconds\n",
            "Step 96900/150000, Loss: 4.625831456184387, Test Loss: 4.636908531188965, LR: 0.0005, Elapsed Time: 10147.70 seconds\n",
            "Step 97000/150000, Loss: 4.619618358612061, Test Loss: 4.639626145362854, LR: 0.0005, Elapsed Time: 10158.12 seconds\n",
            "Step 97100/150000, Loss: 4.636686840057373, Test Loss: 4.643510103225708, LR: 0.0005, Elapsed Time: 10168.53 seconds\n",
            "Step 97200/150000, Loss: 4.632449345588684, Test Loss: 4.639756202697754, LR: 0.0005, Elapsed Time: 10179.03 seconds\n",
            "Step 97300/150000, Loss: 4.6224003553390505, Test Loss: 4.635087072849274, LR: 0.0005, Elapsed Time: 10189.55 seconds\n",
            "Step 97400/150000, Loss: 4.628466601371765, Test Loss: 4.634986162185669, LR: 0.0005, Elapsed Time: 10200.03 seconds\n",
            "Step 97500/150000, Loss: 4.624435496330261, Test Loss: 4.632292091846466, LR: 0.0005, Elapsed Time: 10210.50 seconds\n",
            "Step 97600/150000, Loss: 4.631333756446838, Test Loss: 4.633491098880768, LR: 0.0005, Elapsed Time: 10221.03 seconds\n",
            "Step 97700/150000, Loss: 4.632350621223449, Test Loss: 4.635118305683136, LR: 0.0005, Elapsed Time: 10231.54 seconds\n",
            "Step 97800/150000, Loss: 4.623516960144043, Test Loss: 4.6315866112709045, LR: 0.0005, Elapsed Time: 10241.99 seconds\n",
            "Step 97900/150000, Loss: 4.634884152412415, Test Loss: 4.630384981632233, LR: 0.0005, Elapsed Time: 10252.47 seconds\n",
            "Step 98000/150000, Loss: 4.6223526239395145, Test Loss: 4.631523549556732, LR: 0.0005, Elapsed Time: 10262.89 seconds\n",
            "Step 98100/150000, Loss: 4.633681287765503, Test Loss: 4.631156325340271, LR: 0.0005, Elapsed Time: 10273.32 seconds\n",
            "Step 98200/150000, Loss: 4.614708132743836, Test Loss: 4.632968544960022, LR: 0.0005, Elapsed Time: 10283.72 seconds\n",
            "Step 98300/150000, Loss: 4.636079020500183, Test Loss: 4.632040321826935, LR: 0.0005, Elapsed Time: 10294.18 seconds\n",
            "Step 98400/150000, Loss: 4.636147599220276, Test Loss: 4.629549622535706, LR: 0.0005, Elapsed Time: 10304.65 seconds\n",
            "Step 98500/150000, Loss: 4.629519305229187, Test Loss: 4.630109131336212, LR: 0.0005, Elapsed Time: 10315.11 seconds\n",
            "Step 98600/150000, Loss: 4.611019201278687, Test Loss: 4.628794252872467, LR: 0.0005, Elapsed Time: 10325.52 seconds\n",
            "Step 98700/150000, Loss: 4.616982908248901, Test Loss: 4.627687573432922, LR: 0.0005, Elapsed Time: 10335.96 seconds\n",
            "Step 98800/150000, Loss: 4.620314059257507, Test Loss: 4.628237307071686, LR: 0.0005, Elapsed Time: 10346.46 seconds\n",
            "Step 98900/150000, Loss: 4.618053684234619, Test Loss: 4.625787377357483, LR: 0.0005, Elapsed Time: 10356.96 seconds\n",
            "Step 99000/150000, Loss: 4.62555016040802, Test Loss: 4.622941672801971, LR: 0.0005, Elapsed Time: 10367.43 seconds\n",
            "Step 99100/150000, Loss: 4.621173882484436, Test Loss: 4.628370583057404, LR: 0.0005, Elapsed Time: 10377.93 seconds\n",
            "Step 99200/150000, Loss: 4.617442541122436, Test Loss: 4.626085937023163, LR: 0.0005, Elapsed Time: 10388.38 seconds\n",
            "Step 99300/150000, Loss: 4.617632808685303, Test Loss: 4.62497866153717, LR: 0.0005, Elapsed Time: 10398.84 seconds\n",
            "Step 99400/150000, Loss: 4.6072718524932865, Test Loss: 4.622231721878052, LR: 0.0005, Elapsed Time: 10409.31 seconds\n",
            "Step 99500/150000, Loss: 4.624561305046082, Test Loss: 4.622986972332001, LR: 0.0005, Elapsed Time: 10419.73 seconds\n",
            "Step 99600/150000, Loss: 4.619499030113221, Test Loss: 4.619997501373291, LR: 0.0005, Elapsed Time: 10430.15 seconds\n",
            "Step 99700/150000, Loss: 4.623346285820007, Test Loss: 4.616527736186981, LR: 0.0005, Elapsed Time: 10440.63 seconds\n",
            "Step 99800/150000, Loss: 4.619135618209839, Test Loss: 4.618362128734589, LR: 0.0005, Elapsed Time: 10451.10 seconds\n",
            "Step 99900/150000, Loss: 4.611783108711243, Test Loss: 4.626369416713715, LR: 0.0005, Elapsed Time: 10461.53 seconds\n",
            "Step 100000/150000, Loss: 4.6088929367065425, Test Loss: 4.621346056461334, LR: 0.0005, Elapsed Time: 10471.96 seconds\n",
            "Saving model checkpoint at step 100000\n",
            "Step 100100/150000, Loss: 4.6205266046524045, Test Loss: 4.621374487876892, LR: 0.0005, Elapsed Time: 10482.52 seconds\n",
            "Step 100200/150000, Loss: 4.612310762405396, Test Loss: 4.618714928627014, LR: 0.0005, Elapsed Time: 10492.95 seconds\n",
            "Step 100300/150000, Loss: 4.613575110435486, Test Loss: 4.614742815494537, LR: 0.0005, Elapsed Time: 10503.43 seconds\n",
            "Step 100400/150000, Loss: 4.607941718101501, Test Loss: 4.619343996047974, LR: 0.0005, Elapsed Time: 10513.84 seconds\n",
            "Step 100500/150000, Loss: 4.610837459564209, Test Loss: 4.612621366977692, LR: 0.0005, Elapsed Time: 10524.25 seconds\n",
            "Step 100600/150000, Loss: 4.60339282989502, Test Loss: 4.612646639347076, LR: 0.0005, Elapsed Time: 10534.72 seconds\n",
            "Step 100700/150000, Loss: 4.605064792633057, Test Loss: 4.610822558403015, LR: 0.0005, Elapsed Time: 10545.18 seconds\n",
            "Step 100800/150000, Loss: 4.616553387641907, Test Loss: 4.615645110607147, LR: 0.0005, Elapsed Time: 10555.62 seconds\n",
            "Step 100900/150000, Loss: 4.60834499835968, Test Loss: 4.611374258995056, LR: 0.0005, Elapsed Time: 10566.09 seconds\n",
            "Step 101000/150000, Loss: 4.604138746261596, Test Loss: 4.610837936401367, LR: 0.0005, Elapsed Time: 10576.60 seconds\n",
            "Step 101100/150000, Loss: 4.595677237510682, Test Loss: 4.616147518157959, LR: 0.0005, Elapsed Time: 10587.13 seconds\n",
            "Step 101200/150000, Loss: 4.610685257911682, Test Loss: 4.608050048351288, LR: 0.0005, Elapsed Time: 10597.55 seconds\n",
            "Step 101300/150000, Loss: 4.596573357582092, Test Loss: 4.61631852388382, LR: 0.0005, Elapsed Time: 10608.07 seconds\n",
            "Step 101400/150000, Loss: 4.6035781717300415, Test Loss: 4.610188841819763, LR: 0.0005, Elapsed Time: 10618.62 seconds\n",
            "Step 101500/150000, Loss: 4.611757030487061, Test Loss: 4.608919441699982, LR: 0.0005, Elapsed Time: 10629.08 seconds\n",
            "Step 101600/150000, Loss: 4.601540565490723, Test Loss: 4.614411234855652, LR: 0.0005, Elapsed Time: 10639.51 seconds\n",
            "Step 101700/150000, Loss: 4.6011603307724, Test Loss: 4.608978509902954, LR: 0.0005, Elapsed Time: 10649.98 seconds\n",
            "Step 101800/150000, Loss: 4.606809692382813, Test Loss: 4.606192767620087, LR: 0.0005, Elapsed Time: 10660.42 seconds\n",
            "Step 101900/150000, Loss: 4.5982136726379395, Test Loss: 4.608429670333862, LR: 0.0005, Elapsed Time: 10670.89 seconds\n",
            "Step 102000/150000, Loss: 4.606473655700683, Test Loss: 4.607306778430939, LR: 0.0005, Elapsed Time: 10681.35 seconds\n",
            "Step 102100/150000, Loss: 4.600328321456909, Test Loss: 4.608515739440918, LR: 0.0005, Elapsed Time: 10691.84 seconds\n",
            "Step 102200/150000, Loss: 4.583628311157226, Test Loss: 4.609188795089722, LR: 0.0005, Elapsed Time: 10702.28 seconds\n",
            "Step 102300/150000, Loss: 4.594375777244568, Test Loss: 4.604035496711731, LR: 0.0005, Elapsed Time: 10712.78 seconds\n",
            "Step 102400/150000, Loss: 4.598370118141174, Test Loss: 4.608293652534485, LR: 0.0005, Elapsed Time: 10723.29 seconds\n",
            "Step 102500/150000, Loss: 4.592981562614441, Test Loss: 4.60769259929657, LR: 0.0005, Elapsed Time: 10733.73 seconds\n",
            "Step 102600/150000, Loss: 4.600937805175781, Test Loss: 4.600850403308868, LR: 0.0005, Elapsed Time: 10744.20 seconds\n",
            "Step 102700/150000, Loss: 4.587853412628174, Test Loss: 4.601161420345306, LR: 0.0005, Elapsed Time: 10754.57 seconds\n",
            "Step 102800/150000, Loss: 4.586189322471618, Test Loss: 4.607200562953949, LR: 0.0005, Elapsed Time: 10765.05 seconds\n",
            "Step 102900/150000, Loss: 4.583870034217835, Test Loss: 4.603475332260132, LR: 0.0005, Elapsed Time: 10775.48 seconds\n",
            "Step 103000/150000, Loss: 4.6077166938781735, Test Loss: 4.603028476238251, LR: 0.0005, Elapsed Time: 10785.89 seconds\n",
            "Step 103100/150000, Loss: 4.578835482597351, Test Loss: 4.600737929344177, LR: 0.0005, Elapsed Time: 10796.28 seconds\n",
            "Step 103200/150000, Loss: 4.607456932067871, Test Loss: 4.5980218052864075, LR: 0.0005, Elapsed Time: 10806.75 seconds\n",
            "Step 103300/150000, Loss: 4.596224827766418, Test Loss: 4.598835349082947, LR: 0.0005, Elapsed Time: 10817.22 seconds\n",
            "Step 103400/150000, Loss: 4.581506223678589, Test Loss: 4.600177109241486, LR: 0.0005, Elapsed Time: 10827.71 seconds\n",
            "Step 103500/150000, Loss: 4.578043236732483, Test Loss: 4.599627673625946, LR: 0.0005, Elapsed Time: 10838.20 seconds\n",
            "Step 103600/150000, Loss: 4.5954876899719235, Test Loss: 4.597754061222076, LR: 0.0005, Elapsed Time: 10848.68 seconds\n",
            "Step 103700/150000, Loss: 4.589528551101685, Test Loss: 4.603450775146484, LR: 0.0005, Elapsed Time: 10859.21 seconds\n",
            "Step 103800/150000, Loss: 4.589122161865235, Test Loss: 4.59776383638382, LR: 0.0005, Elapsed Time: 10869.65 seconds\n",
            "Step 103900/150000, Loss: 4.591271123886108, Test Loss: 4.5991591811180115, LR: 0.0005, Elapsed Time: 10880.08 seconds\n",
            "Step 104000/150000, Loss: 4.58751446723938, Test Loss: 4.59814590215683, LR: 0.0005, Elapsed Time: 10890.55 seconds\n",
            "Step 104100/150000, Loss: 4.589705247879028, Test Loss: 4.593555808067322, LR: 0.0005, Elapsed Time: 10901.10 seconds\n",
            "Step 104200/150000, Loss: 4.590589661598205, Test Loss: 4.593409478664398, LR: 0.0005, Elapsed Time: 10911.61 seconds\n",
            "Step 104300/150000, Loss: 4.589026546478271, Test Loss: 4.588950991630554, LR: 0.0005, Elapsed Time: 10922.09 seconds\n",
            "Step 104400/150000, Loss: 4.585087246894837, Test Loss: 4.5934566259384155, LR: 0.0005, Elapsed Time: 10932.57 seconds\n",
            "Step 104500/150000, Loss: 4.581825428009033, Test Loss: 4.590896666049957, LR: 0.0005, Elapsed Time: 10943.00 seconds\n",
            "Step 104600/150000, Loss: 4.57651686668396, Test Loss: 4.590027153491974, LR: 0.0005, Elapsed Time: 10953.45 seconds\n",
            "Step 104700/150000, Loss: 4.594912619590759, Test Loss: 4.591535449028015, LR: 0.0005, Elapsed Time: 10963.98 seconds\n",
            "Step 104800/150000, Loss: 4.588951959609985, Test Loss: 4.589764475822449, LR: 0.0005, Elapsed Time: 10974.44 seconds\n",
            "Step 104900/150000, Loss: 4.589153542518615, Test Loss: 4.588797032833099, LR: 0.0005, Elapsed Time: 10984.85 seconds\n",
            "Step 105000/150000, Loss: 4.562139172554016, Test Loss: 4.594941318035126, LR: 0.0005, Elapsed Time: 10995.38 seconds\n",
            "Step 105100/150000, Loss: 4.578693151473999, Test Loss: 4.586896657943726, LR: 0.0005, Elapsed Time: 11005.88 seconds\n",
            "Step 105200/150000, Loss: 4.5765238809585576, Test Loss: 4.584712326526642, LR: 0.0005, Elapsed Time: 11016.40 seconds\n",
            "Step 105300/150000, Loss: 4.5896961212158205, Test Loss: 4.586831510066986, LR: 0.0005, Elapsed Time: 11026.87 seconds\n",
            "Step 105400/150000, Loss: 4.580262274742126, Test Loss: 4.590433776378632, LR: 0.0005, Elapsed Time: 11037.32 seconds\n",
            "Step 105500/150000, Loss: 4.582105946540833, Test Loss: 4.585211396217346, LR: 0.0005, Elapsed Time: 11047.71 seconds\n",
            "Step 105600/150000, Loss: 4.580722970962524, Test Loss: 4.584090828895569, LR: 0.0005, Elapsed Time: 11058.15 seconds\n",
            "Step 105700/150000, Loss: 4.578978352546692, Test Loss: 4.588916778564453, LR: 0.0005, Elapsed Time: 11068.58 seconds\n",
            "Step 105800/150000, Loss: 4.567564072608948, Test Loss: 4.588899314403534, LR: 0.0005, Elapsed Time: 11078.99 seconds\n",
            "Step 105900/150000, Loss: 4.573529272079468, Test Loss: 4.5820518136024475, LR: 0.0005, Elapsed Time: 11089.40 seconds\n",
            "Step 106000/150000, Loss: 4.581840133666992, Test Loss: 4.587289273738861, LR: 0.0005, Elapsed Time: 11099.79 seconds\n",
            "Step 106100/150000, Loss: 4.583699650764466, Test Loss: 4.585439503192902, LR: 0.0005, Elapsed Time: 11110.26 seconds\n",
            "Step 106200/150000, Loss: 4.584826011657714, Test Loss: 4.588802814483643, LR: 0.0005, Elapsed Time: 11120.69 seconds\n",
            "Step 106300/150000, Loss: 4.575369172096252, Test Loss: 4.5852895975112915, LR: 0.0005, Elapsed Time: 11131.10 seconds\n",
            "Step 106400/150000, Loss: 4.57934070110321, Test Loss: 4.578591227531433, LR: 0.0005, Elapsed Time: 11141.61 seconds\n",
            "Step 106500/150000, Loss: 4.56543420791626, Test Loss: 4.580428600311279, LR: 0.0005, Elapsed Time: 11152.05 seconds\n",
            "Step 106600/150000, Loss: 4.577886719703674, Test Loss: 4.583413362503052, LR: 0.0005, Elapsed Time: 11162.51 seconds\n",
            "Step 106700/150000, Loss: 4.567098631858825, Test Loss: 4.582394361495972, LR: 0.0005, Elapsed Time: 11173.00 seconds\n",
            "Step 106800/150000, Loss: 4.562952451705932, Test Loss: 4.584019482135773, LR: 0.0005, Elapsed Time: 11183.46 seconds\n",
            "Step 106900/150000, Loss: 4.5743641710281375, Test Loss: 4.5805052518844604, LR: 0.0005, Elapsed Time: 11193.93 seconds\n",
            "Step 107000/150000, Loss: 4.56983811378479, Test Loss: 4.58242803812027, LR: 0.0005, Elapsed Time: 11204.42 seconds\n",
            "Step 107100/150000, Loss: 4.569725260734558, Test Loss: 4.579241335391998, LR: 0.0005, Elapsed Time: 11214.88 seconds\n",
            "Step 107200/150000, Loss: 4.562703409194946, Test Loss: 4.579744279384613, LR: 0.0005, Elapsed Time: 11225.42 seconds\n",
            "Step 107300/150000, Loss: 4.57162504196167, Test Loss: 4.580317854881287, LR: 0.0005, Elapsed Time: 11235.93 seconds\n",
            "Step 107400/150000, Loss: 4.577780499458313, Test Loss: 4.582172513008118, LR: 0.0005, Elapsed Time: 11246.40 seconds\n",
            "Step 107500/150000, Loss: 4.577816696166992, Test Loss: 4.573326230049133, LR: 0.0005, Elapsed Time: 11256.87 seconds\n",
            "Step 107600/150000, Loss: 4.56252010345459, Test Loss: 4.576704025268555, LR: 0.0005, Elapsed Time: 11267.41 seconds\n",
            "Step 107700/150000, Loss: 4.562086844444275, Test Loss: 4.577490568161011, LR: 0.0005, Elapsed Time: 11277.84 seconds\n",
            "Step 107800/150000, Loss: 4.558477053642273, Test Loss: 4.581203401088715, LR: 0.0005, Elapsed Time: 11288.37 seconds\n",
            "Step 107900/150000, Loss: 4.567818946838379, Test Loss: 4.574652671813965, LR: 0.0005, Elapsed Time: 11298.83 seconds\n",
            "Step 108000/150000, Loss: 4.572022051811218, Test Loss: 4.573378562927246, LR: 0.0005, Elapsed Time: 11309.26 seconds\n",
            "Step 108100/150000, Loss: 4.555449562072754, Test Loss: 4.573389649391174, LR: 0.0005, Elapsed Time: 11319.70 seconds\n",
            "Step 108200/150000, Loss: 4.5733958339691165, Test Loss: 4.575595676898956, LR: 0.0005, Elapsed Time: 11330.22 seconds\n",
            "Step 108300/150000, Loss: 4.551444945335388, Test Loss: 4.568536460399628, LR: 0.0005, Elapsed Time: 11340.69 seconds\n",
            "Step 108400/150000, Loss: 4.553409218788147, Test Loss: 4.568656146526337, LR: 0.0005, Elapsed Time: 11351.10 seconds\n",
            "Step 108500/150000, Loss: 4.574968419075012, Test Loss: 4.570334851741791, LR: 0.0005, Elapsed Time: 11361.55 seconds\n",
            "Step 108600/150000, Loss: 4.567140502929687, Test Loss: 4.569110691547394, LR: 0.0005, Elapsed Time: 11371.99 seconds\n",
            "Step 108700/150000, Loss: 4.570112390518188, Test Loss: 4.570746004581451, LR: 0.0005, Elapsed Time: 11382.39 seconds\n",
            "Step 108800/150000, Loss: 4.569946856498718, Test Loss: 4.56686133146286, LR: 0.0005, Elapsed Time: 11392.81 seconds\n",
            "Step 108900/150000, Loss: 4.559395890235901, Test Loss: 4.565597832202911, LR: 0.0005, Elapsed Time: 11403.26 seconds\n",
            "Step 109000/150000, Loss: 4.580220856666565, Test Loss: 4.564338445663452, LR: 0.0005, Elapsed Time: 11413.78 seconds\n",
            "Step 109100/150000, Loss: 4.552979741096497, Test Loss: 4.5680171847343445, LR: 0.0005, Elapsed Time: 11424.31 seconds\n",
            "Step 109200/150000, Loss: 4.567926630973816, Test Loss: 4.56605076789856, LR: 0.0005, Elapsed Time: 11434.79 seconds\n",
            "Step 109300/150000, Loss: 4.549776530265808, Test Loss: 4.565725564956665, LR: 0.0005, Elapsed Time: 11445.26 seconds\n",
            "Step 109400/150000, Loss: 4.5639936876297, Test Loss: 4.566766262054443, LR: 0.0005, Elapsed Time: 11455.72 seconds\n",
            "Step 109500/150000, Loss: 4.552523670196533, Test Loss: 4.565463423728943, LR: 0.0005, Elapsed Time: 11466.23 seconds\n",
            "Step 109600/150000, Loss: 4.556932649612427, Test Loss: 4.5612112283706665, LR: 0.0005, Elapsed Time: 11476.67 seconds\n",
            "Step 109700/150000, Loss: 4.562925319671631, Test Loss: 4.562435030937195, LR: 0.0005, Elapsed Time: 11487.08 seconds\n",
            "Step 109800/150000, Loss: 4.55955801486969, Test Loss: 4.561127066612244, LR: 0.0005, Elapsed Time: 11497.51 seconds\n",
            "Step 109900/150000, Loss: 4.563456506729126, Test Loss: 4.565692245960236, LR: 0.0005, Elapsed Time: 11507.93 seconds\n",
            "Step 110000/150000, Loss: 4.553442587852478, Test Loss: 4.563076615333557, LR: 0.0005, Elapsed Time: 11518.37 seconds\n",
            "Step 110100/150000, Loss: 4.566080374717712, Test Loss: 4.5667847990989685, LR: 0.0005, Elapsed Time: 11528.86 seconds\n",
            "Step 110200/150000, Loss: 4.5564982700347905, Test Loss: 4.567052006721497, LR: 0.0005, Elapsed Time: 11539.33 seconds\n",
            "Step 110300/150000, Loss: 4.546115689277649, Test Loss: 4.5663028955459595, LR: 0.0005, Elapsed Time: 11549.79 seconds\n",
            "Step 110400/150000, Loss: 4.536957292556763, Test Loss: 4.56752336025238, LR: 0.0005, Elapsed Time: 11560.26 seconds\n",
            "Step 110500/150000, Loss: 4.555437207221985, Test Loss: 4.560078024864197, LR: 0.0005, Elapsed Time: 11570.75 seconds\n",
            "Step 110600/150000, Loss: 4.543961219787597, Test Loss: 4.557436466217041, LR: 0.0005, Elapsed Time: 11581.20 seconds\n",
            "Step 110700/150000, Loss: 4.5470106887817385, Test Loss: 4.564876675605774, LR: 0.0005, Elapsed Time: 11591.65 seconds\n",
            "Step 110800/150000, Loss: 4.558285331726074, Test Loss: 4.562499105930328, LR: 0.0005, Elapsed Time: 11602.10 seconds\n",
            "Step 110900/150000, Loss: 4.555693101882935, Test Loss: 4.55800998210907, LR: 0.0005, Elapsed Time: 11612.48 seconds\n",
            "Step 111000/150000, Loss: 4.548618221282959, Test Loss: 4.5634554624557495, LR: 0.0005, Elapsed Time: 11622.91 seconds\n",
            "Step 111100/150000, Loss: 4.55680094242096, Test Loss: 4.56287008523941, LR: 0.0005, Elapsed Time: 11633.32 seconds\n",
            "Step 111200/150000, Loss: 4.551384043693543, Test Loss: 4.562257647514343, LR: 0.0005, Elapsed Time: 11643.76 seconds\n",
            "Step 111300/150000, Loss: 4.547683753967285, Test Loss: 4.566716313362122, LR: 0.0005, Elapsed Time: 11654.22 seconds\n",
            "Step 111400/150000, Loss: 4.550557055473328, Test Loss: 4.5603147149086, LR: 0.0005, Elapsed Time: 11664.66 seconds\n",
            "Step 111500/150000, Loss: 4.553681507110595, Test Loss: 4.558698534965515, LR: 0.0005, Elapsed Time: 11675.12 seconds\n",
            "Step 111600/150000, Loss: 4.541778049468994, Test Loss: 4.560165047645569, LR: 0.0005, Elapsed Time: 11685.57 seconds\n",
            "Step 111700/150000, Loss: 4.5513324499130245, Test Loss: 4.559481084346771, LR: 0.0005, Elapsed Time: 11696.03 seconds\n",
            "Step 111800/150000, Loss: 4.5364692640304565, Test Loss: 4.535502731800079, LR: 0.00015, Elapsed Time: 11706.47 seconds\n",
            "Step 111900/150000, Loss: 4.518524746894837, Test Loss: 4.532015383243561, LR: 0.00015, Elapsed Time: 11716.92 seconds\n",
            "Step 112000/150000, Loss: 4.5207448577880855, Test Loss: 4.5303186774253845, LR: 0.00015, Elapsed Time: 11727.33 seconds\n",
            "Step 112100/150000, Loss: 4.51040702342987, Test Loss: 4.5277997851371765, LR: 0.00015, Elapsed Time: 11737.81 seconds\n",
            "Step 112200/150000, Loss: 4.509240312576294, Test Loss: 4.525943577289581, LR: 0.00015, Elapsed Time: 11748.23 seconds\n",
            "Step 112300/150000, Loss: 4.517040462493896, Test Loss: 4.5255022048950195, LR: 0.00015, Elapsed Time: 11758.67 seconds\n",
            "Step 112400/150000, Loss: 4.503537411689758, Test Loss: 4.5238776206970215, LR: 0.00015, Elapsed Time: 11769.11 seconds\n",
            "Step 112500/150000, Loss: 4.513156633377076, Test Loss: 4.524541139602661, LR: 0.00015, Elapsed Time: 11779.63 seconds\n",
            "Step 112600/150000, Loss: 4.520106949806213, Test Loss: 4.524295151233673, LR: 0.00015, Elapsed Time: 11790.08 seconds\n",
            "Step 112700/150000, Loss: 4.515898613929749, Test Loss: 4.522246837615967, LR: 0.00015, Elapsed Time: 11800.56 seconds\n",
            "Step 112800/150000, Loss: 4.515511326789856, Test Loss: 4.523564517498016, LR: 0.00015, Elapsed Time: 11811.07 seconds\n",
            "Step 112900/150000, Loss: 4.518942203521728, Test Loss: 4.519757807254791, LR: 0.00015, Elapsed Time: 11821.55 seconds\n",
            "Step 113000/150000, Loss: 4.514722218513489, Test Loss: 4.519052863121033, LR: 0.00015, Elapsed Time: 11832.10 seconds\n",
            "Step 113100/150000, Loss: 4.509721164703369, Test Loss: 4.521239936351776, LR: 0.00015, Elapsed Time: 11842.53 seconds\n",
            "Step 113200/150000, Loss: 4.514523415565491, Test Loss: 4.517209410667419, LR: 0.00015, Elapsed Time: 11852.95 seconds\n",
            "Step 113300/150000, Loss: 4.504203109741211, Test Loss: 4.5186790227890015, LR: 0.00015, Elapsed Time: 11863.43 seconds\n",
            "Step 113400/150000, Loss: 4.517420325279236, Test Loss: 4.519274652004242, LR: 0.00015, Elapsed Time: 11873.94 seconds\n",
            "Step 113500/150000, Loss: 4.504851264953613, Test Loss: 4.516611814498901, LR: 0.00015, Elapsed Time: 11884.45 seconds\n",
            "Step 113600/150000, Loss: 4.499229264259339, Test Loss: 4.519327878952026, LR: 0.00015, Elapsed Time: 11894.91 seconds\n",
            "Step 113700/150000, Loss: 4.510981035232544, Test Loss: 4.518710136413574, LR: 0.00015, Elapsed Time: 11905.39 seconds\n",
            "Step 113800/150000, Loss: 4.503222732543946, Test Loss: 4.516289055347443, LR: 0.00015, Elapsed Time: 11915.81 seconds\n",
            "Step 113900/150000, Loss: 4.503435482978821, Test Loss: 4.518833577632904, LR: 0.00015, Elapsed Time: 11926.27 seconds\n",
            "Step 114000/150000, Loss: 4.512168221473694, Test Loss: 4.515090346336365, LR: 0.00015, Elapsed Time: 11936.83 seconds\n",
            "Step 114100/150000, Loss: 4.510672211647034, Test Loss: 4.517912328243256, LR: 0.00015, Elapsed Time: 11947.29 seconds\n",
            "Step 114200/150000, Loss: 4.507784543037414, Test Loss: 4.517097115516663, LR: 0.00015, Elapsed Time: 11957.75 seconds\n",
            "Step 114300/150000, Loss: 4.508707365989685, Test Loss: 4.515001535415649, LR: 0.00015, Elapsed Time: 11968.15 seconds\n",
            "Step 114400/150000, Loss: 4.49879629611969, Test Loss: 4.515933692455292, LR: 0.00015, Elapsed Time: 11978.64 seconds\n",
            "Step 114500/150000, Loss: 4.496595015525818, Test Loss: 4.516028642654419, LR: 0.00015, Elapsed Time: 11989.07 seconds\n",
            "Step 114600/150000, Loss: 4.504826817512512, Test Loss: 4.514825880527496, LR: 0.00015, Elapsed Time: 11999.46 seconds\n",
            "Step 114700/150000, Loss: 4.496674246788025, Test Loss: 4.514963269233704, LR: 0.00015, Elapsed Time: 12009.93 seconds\n",
            "Step 114800/150000, Loss: 4.501419844627381, Test Loss: 4.5139800906181335, LR: 0.00015, Elapsed Time: 12020.37 seconds\n",
            "Step 114900/150000, Loss: 4.499157304763794, Test Loss: 4.513362109661102, LR: 0.00015, Elapsed Time: 12030.82 seconds\n",
            "Step 115000/150000, Loss: 4.50982714176178, Test Loss: 4.512733638286591, LR: 0.00015, Elapsed Time: 12041.31 seconds\n",
            "Step 115100/150000, Loss: 4.498891859054566, Test Loss: 4.513752996921539, LR: 0.00015, Elapsed Time: 12051.70 seconds\n",
            "Step 115200/150000, Loss: 4.483005495071411, Test Loss: 4.513491630554199, LR: 0.00015, Elapsed Time: 12062.22 seconds\n",
            "Step 115300/150000, Loss: 4.501119575500488, Test Loss: 4.5148725509643555, LR: 0.00015, Elapsed Time: 12072.70 seconds\n",
            "Step 115400/150000, Loss: 4.516231217384338, Test Loss: 4.511253774166107, LR: 0.00015, Elapsed Time: 12083.17 seconds\n",
            "Step 115500/150000, Loss: 4.4928572416305546, Test Loss: 4.5139641761779785, LR: 0.00015, Elapsed Time: 12093.67 seconds\n",
            "Step 115600/150000, Loss: 4.503080163002014, Test Loss: 4.511976182460785, LR: 0.00015, Elapsed Time: 12104.18 seconds\n",
            "Step 115700/150000, Loss: 4.497336449623108, Test Loss: 4.514186918735504, LR: 0.00015, Elapsed Time: 12114.62 seconds\n",
            "Step 115800/150000, Loss: 4.493121776580811, Test Loss: 4.511273682117462, LR: 0.00015, Elapsed Time: 12125.06 seconds\n",
            "Step 115900/150000, Loss: 4.499014234542846, Test Loss: 4.51170802116394, LR: 0.00015, Elapsed Time: 12135.51 seconds\n",
            "Step 116000/150000, Loss: 4.507682685852051, Test Loss: 4.510825276374817, LR: 0.00015, Elapsed Time: 12145.93 seconds\n",
            "Step 116100/150000, Loss: 4.495392971038818, Test Loss: 4.511817634105682, LR: 0.00015, Elapsed Time: 12156.34 seconds\n",
            "Step 116200/150000, Loss: 4.490530600547791, Test Loss: 4.51213264465332, LR: 0.00015, Elapsed Time: 12166.71 seconds\n",
            "Step 116300/150000, Loss: 4.494541583061218, Test Loss: 4.509191930294037, LR: 0.00015, Elapsed Time: 12177.15 seconds\n",
            "Step 116400/150000, Loss: 4.494622631072998, Test Loss: 4.512215793132782, LR: 0.00015, Elapsed Time: 12187.64 seconds\n",
            "Step 116500/150000, Loss: 4.487378377914428, Test Loss: 4.512327611446381, LR: 0.00015, Elapsed Time: 12198.14 seconds\n",
            "Step 116600/150000, Loss: 4.498601627349854, Test Loss: 4.509634017944336, LR: 0.00015, Elapsed Time: 12208.63 seconds\n",
            "Step 116700/150000, Loss: 4.485839328765869, Test Loss: 4.5126954317092896, LR: 0.00015, Elapsed Time: 12219.08 seconds\n",
            "Step 116800/150000, Loss: 4.50567843914032, Test Loss: 4.509118378162384, LR: 0.00015, Elapsed Time: 12229.56 seconds\n",
            "Step 116900/150000, Loss: 4.484252982139587, Test Loss: 4.51077675819397, LR: 0.00015, Elapsed Time: 12240.02 seconds\n",
            "Step 117000/150000, Loss: 4.489790840148926, Test Loss: 4.507184207439423, LR: 0.00015, Elapsed Time: 12250.50 seconds\n",
            "Step 117100/150000, Loss: 4.498467578887939, Test Loss: 4.507408678531647, LR: 0.00015, Elapsed Time: 12261.00 seconds\n",
            "Step 117200/150000, Loss: 4.4810103416442875, Test Loss: 4.509073972702026, LR: 0.00015, Elapsed Time: 12271.56 seconds\n",
            "Step 117300/150000, Loss: 4.499067540168762, Test Loss: 4.506733477115631, LR: 0.00015, Elapsed Time: 12282.05 seconds\n",
            "Step 117400/150000, Loss: 4.484221906661987, Test Loss: 4.506816387176514, LR: 0.00015, Elapsed Time: 12292.49 seconds\n",
            "Step 117500/150000, Loss: 4.491930375099182, Test Loss: 4.507956087589264, LR: 0.00015, Elapsed Time: 12302.97 seconds\n",
            "Step 117600/150000, Loss: 4.490207738876343, Test Loss: 4.509123265743256, LR: 0.00015, Elapsed Time: 12313.45 seconds\n",
            "Step 117700/150000, Loss: 4.496786093711853, Test Loss: 4.506335556507111, LR: 0.00015, Elapsed Time: 12323.85 seconds\n",
            "Step 117800/150000, Loss: 4.4916459083557125, Test Loss: 4.508045315742493, LR: 0.00015, Elapsed Time: 12334.30 seconds\n",
            "Step 117900/150000, Loss: 4.495825591087342, Test Loss: 4.505767822265625, LR: 0.00015, Elapsed Time: 12344.86 seconds\n",
            "Step 118000/150000, Loss: 4.4876113748550415, Test Loss: 4.507592618465424, LR: 0.00015, Elapsed Time: 12355.37 seconds\n",
            "Step 118100/150000, Loss: 4.4838976526260375, Test Loss: 4.5074891448020935, LR: 0.00015, Elapsed Time: 12365.89 seconds\n",
            "Step 118200/150000, Loss: 4.4878209733963015, Test Loss: 4.506527900695801, LR: 0.00015, Elapsed Time: 12376.35 seconds\n",
            "Step 118300/150000, Loss: 4.493080282211304, Test Loss: 4.50555282831192, LR: 0.00015, Elapsed Time: 12386.79 seconds\n",
            "Step 118400/150000, Loss: 4.490910849571228, Test Loss: 4.505158960819244, LR: 0.00015, Elapsed Time: 12397.24 seconds\n",
            "Step 118500/150000, Loss: 4.487466378211975, Test Loss: 4.505689859390259, LR: 0.00015, Elapsed Time: 12407.69 seconds\n",
            "Step 118600/150000, Loss: 4.498301229476929, Test Loss: 4.507144093513489, LR: 0.00015, Elapsed Time: 12418.08 seconds\n",
            "Step 118700/150000, Loss: 4.49314293384552, Test Loss: 4.507704555988312, LR: 0.00015, Elapsed Time: 12428.62 seconds\n",
            "Step 118800/150000, Loss: 4.477272725105285, Test Loss: 4.504107117652893, LR: 0.00015, Elapsed Time: 12439.10 seconds\n",
            "Step 118900/150000, Loss: 4.4820624399185185, Test Loss: 4.50462132692337, LR: 0.00015, Elapsed Time: 12449.61 seconds\n",
            "Step 119000/150000, Loss: 4.488964409828186, Test Loss: 4.505551874637604, LR: 0.00015, Elapsed Time: 12460.13 seconds\n",
            "Step 119100/150000, Loss: 4.489337587356568, Test Loss: 4.505482196807861, LR: 0.00015, Elapsed Time: 12470.53 seconds\n",
            "Step 119200/150000, Loss: 4.482102084159851, Test Loss: 4.504363834857941, LR: 0.00015, Elapsed Time: 12481.03 seconds\n",
            "Step 119300/150000, Loss: 4.482666277885437, Test Loss: 4.5006866455078125, LR: 0.00015, Elapsed Time: 12491.53 seconds\n",
            "Step 119400/150000, Loss: 4.483446817398072, Test Loss: 4.504553198814392, LR: 0.00015, Elapsed Time: 12502.05 seconds\n",
            "Step 119500/150000, Loss: 4.4945578956604, Test Loss: 4.5012388825416565, LR: 0.00015, Elapsed Time: 12512.50 seconds\n",
            "Step 119600/150000, Loss: 4.487843675613403, Test Loss: 4.505428194999695, LR: 0.00015, Elapsed Time: 12522.97 seconds\n",
            "Step 119700/150000, Loss: 4.485524296760559, Test Loss: 4.502618730068207, LR: 0.00015, Elapsed Time: 12533.42 seconds\n",
            "Step 119800/150000, Loss: 4.4955757093429565, Test Loss: 4.504160702228546, LR: 0.00015, Elapsed Time: 12543.93 seconds\n",
            "Step 119900/150000, Loss: 4.487319149971008, Test Loss: 4.50249582529068, LR: 0.00015, Elapsed Time: 12554.38 seconds\n",
            "Step 120000/150000, Loss: 4.488677215576172, Test Loss: 4.502407371997833, LR: 0.00015, Elapsed Time: 12564.81 seconds\n",
            "Step 120100/150000, Loss: 4.4847621870040895, Test Loss: 4.505232512950897, LR: 0.00015, Elapsed Time: 12575.29 seconds\n",
            "Step 120200/150000, Loss: 4.4907989406585695, Test Loss: 4.504931449890137, LR: 0.00015, Elapsed Time: 12585.82 seconds\n",
            "Step 120300/150000, Loss: 4.500872626304626, Test Loss: 4.504347264766693, LR: 0.00015, Elapsed Time: 12596.24 seconds\n",
            "Step 120400/150000, Loss: 4.486850256919861, Test Loss: 4.5039098262786865, LR: 0.00015, Elapsed Time: 12606.64 seconds\n",
            "Step 120500/150000, Loss: 4.479715437889099, Test Loss: 4.494948208332062, LR: 4.4999999999999996e-05, Elapsed Time: 12617.06 seconds\n",
            "Step 120600/150000, Loss: 4.467232046127319, Test Loss: 4.492970585823059, LR: 4.4999999999999996e-05, Elapsed Time: 12627.57 seconds\n",
            "Step 120700/150000, Loss: 4.481060471534729, Test Loss: 4.492671012878418, LR: 4.4999999999999996e-05, Elapsed Time: 12638.05 seconds\n",
            "Step 120800/150000, Loss: 4.475752177238465, Test Loss: 4.492815673351288, LR: 4.4999999999999996e-05, Elapsed Time: 12648.52 seconds\n",
            "Step 120900/150000, Loss: 4.480569381713867, Test Loss: 4.491977334022522, LR: 4.4999999999999996e-05, Elapsed Time: 12658.96 seconds\n",
            "Step 121000/150000, Loss: 4.477535028457641, Test Loss: 4.492493689060211, LR: 4.4999999999999996e-05, Elapsed Time: 12669.47 seconds\n",
            "Step 121100/150000, Loss: 4.479924092292785, Test Loss: 4.492965757846832, LR: 4.4999999999999996e-05, Elapsed Time: 12679.92 seconds\n",
            "Step 121200/150000, Loss: 4.467280168533325, Test Loss: 4.49102509021759, LR: 4.4999999999999996e-05, Elapsed Time: 12690.37 seconds\n",
            "Step 121300/150000, Loss: 4.471936130523682, Test Loss: 4.491993844509125, LR: 4.4999999999999996e-05, Elapsed Time: 12700.88 seconds\n",
            "Step 121400/150000, Loss: 4.4817187690734865, Test Loss: 4.4911975264549255, LR: 4.4999999999999996e-05, Elapsed Time: 12711.33 seconds\n",
            "Step 121500/150000, Loss: 4.480138850212097, Test Loss: 4.491039872169495, LR: 4.4999999999999996e-05, Elapsed Time: 12721.85 seconds\n",
            "Step 121600/150000, Loss: 4.4733587169647215, Test Loss: 4.491002678871155, LR: 4.4999999999999996e-05, Elapsed Time: 12732.32 seconds\n",
            "Step 121700/150000, Loss: 4.485110955238342, Test Loss: 4.4903857707977295, LR: 4.4999999999999996e-05, Elapsed Time: 12742.90 seconds\n",
            "Step 121800/150000, Loss: 4.468965845108032, Test Loss: 4.491197228431702, LR: 4.4999999999999996e-05, Elapsed Time: 12753.44 seconds\n",
            "Step 121900/150000, Loss: 4.4744036722183225, Test Loss: 4.490458071231842, LR: 4.4999999999999996e-05, Elapsed Time: 12763.95 seconds\n",
            "Step 122000/150000, Loss: 4.477947340011597, Test Loss: 4.489441335201263, LR: 4.4999999999999996e-05, Elapsed Time: 12774.40 seconds\n",
            "Step 122100/150000, Loss: 4.467560367584229, Test Loss: 4.4898547530174255, LR: 4.4999999999999996e-05, Elapsed Time: 12784.84 seconds\n",
            "Step 122200/150000, Loss: 4.473015084266662, Test Loss: 4.489317953586578, LR: 4.4999999999999996e-05, Elapsed Time: 12795.26 seconds\n",
            "Step 122300/150000, Loss: 4.47404993057251, Test Loss: 4.489753901958466, LR: 4.4999999999999996e-05, Elapsed Time: 12805.70 seconds\n",
            "Step 122400/150000, Loss: 4.469119501113892, Test Loss: 4.489037930965424, LR: 4.4999999999999996e-05, Elapsed Time: 12816.16 seconds\n",
            "Step 122500/150000, Loss: 4.463781461715699, Test Loss: 4.48970240354538, LR: 4.4999999999999996e-05, Elapsed Time: 12826.62 seconds\n",
            "Step 122600/150000, Loss: 4.474469270706177, Test Loss: 4.488738775253296, LR: 4.4999999999999996e-05, Elapsed Time: 12837.12 seconds\n",
            "Step 122700/150000, Loss: 4.480641741752624, Test Loss: 4.488793611526489, LR: 4.4999999999999996e-05, Elapsed Time: 12847.72 seconds\n",
            "Step 122800/150000, Loss: 4.468992614746094, Test Loss: 4.488360524177551, LR: 4.4999999999999996e-05, Elapsed Time: 12858.30 seconds\n",
            "Step 122900/150000, Loss: 4.465169644355774, Test Loss: 4.488807201385498, LR: 4.4999999999999996e-05, Elapsed Time: 12868.76 seconds\n",
            "Step 123000/150000, Loss: 4.461304631233215, Test Loss: 4.488252580165863, LR: 4.4999999999999996e-05, Elapsed Time: 12879.26 seconds\n",
            "Step 123100/150000, Loss: 4.4740800952911375, Test Loss: 4.488720893859863, LR: 4.4999999999999996e-05, Elapsed Time: 12889.87 seconds\n",
            "Step 123200/150000, Loss: 4.464904737472534, Test Loss: 4.489350318908691, LR: 4.4999999999999996e-05, Elapsed Time: 12900.50 seconds\n",
            "Step 123300/150000, Loss: 4.4693335437774655, Test Loss: 4.487623810768127, LR: 4.4999999999999996e-05, Elapsed Time: 12910.99 seconds\n",
            "Step 123400/150000, Loss: 4.477683815956116, Test Loss: 4.488196551799774, LR: 4.4999999999999996e-05, Elapsed Time: 12921.48 seconds\n",
            "Step 123500/150000, Loss: 4.467153158187866, Test Loss: 4.4890522956848145, LR: 4.4999999999999996e-05, Elapsed Time: 12931.95 seconds\n",
            "Step 123600/150000, Loss: 4.467233591079712, Test Loss: 4.487945854663849, LR: 4.4999999999999996e-05, Elapsed Time: 12942.46 seconds\n",
            "Step 123700/150000, Loss: 4.47087414264679, Test Loss: 4.488249361515045, LR: 4.4999999999999996e-05, Elapsed Time: 12953.04 seconds\n",
            "Step 123800/150000, Loss: 4.472932424545288, Test Loss: 4.488454639911652, LR: 4.4999999999999996e-05, Elapsed Time: 12963.50 seconds\n",
            "Step 123900/150000, Loss: 4.463984227180481, Test Loss: 4.489133954048157, LR: 4.4999999999999996e-05, Elapsed Time: 12974.00 seconds\n",
            "Step 124000/150000, Loss: 4.468763785362244, Test Loss: 4.488923013210297, LR: 4.4999999999999996e-05, Elapsed Time: 12984.46 seconds\n",
            "Step 124100/150000, Loss: 4.454857597351074, Test Loss: 4.487793505191803, LR: 4.4999999999999996e-05, Elapsed Time: 12994.95 seconds\n",
            "Step 124200/150000, Loss: 4.460699634552002, Test Loss: 4.487112522125244, LR: 4.4999999999999996e-05, Elapsed Time: 13005.50 seconds\n",
            "Step 124300/150000, Loss: 4.472416114807129, Test Loss: 4.488667547702789, LR: 4.4999999999999996e-05, Elapsed Time: 13016.02 seconds\n",
            "Step 124400/150000, Loss: 4.463651685714722, Test Loss: 4.48861426115036, LR: 4.4999999999999996e-05, Elapsed Time: 13026.52 seconds\n",
            "Step 124500/150000, Loss: 4.466560091972351, Test Loss: 4.487222909927368, LR: 4.4999999999999996e-05, Elapsed Time: 13037.10 seconds\n",
            "Step 124600/150000, Loss: 4.466427540779113, Test Loss: 4.487075567245483, LR: 4.4999999999999996e-05, Elapsed Time: 13047.66 seconds\n",
            "Step 124700/150000, Loss: 4.451697969436646, Test Loss: 4.48666250705719, LR: 4.4999999999999996e-05, Elapsed Time: 13058.18 seconds\n",
            "Step 124800/150000, Loss: 4.469199962615967, Test Loss: 4.4868868589401245, LR: 4.4999999999999996e-05, Elapsed Time: 13068.60 seconds\n",
            "Step 124900/150000, Loss: 4.4719459390640255, Test Loss: 4.487867295742035, LR: 4.4999999999999996e-05, Elapsed Time: 13079.05 seconds\n",
            "Step 125000/150000, Loss: 4.454464921951294, Test Loss: 4.487796068191528, LR: 4.4999999999999996e-05, Elapsed Time: 13089.44 seconds\n",
            "Step 125100/150000, Loss: 4.476164207458496, Test Loss: 4.487152814865112, LR: 4.4999999999999996e-05, Elapsed Time: 13099.85 seconds\n",
            "Step 125200/150000, Loss: 4.4643251419067385, Test Loss: 4.487168371677399, LR: 4.4999999999999996e-05, Elapsed Time: 13110.33 seconds\n",
            "Step 125300/150000, Loss: 4.455333132743835, Test Loss: 4.487185955047607, LR: 4.4999999999999996e-05, Elapsed Time: 13120.79 seconds\n",
            "Step 125400/150000, Loss: 4.456692519187928, Test Loss: 4.488124787807465, LR: 4.4999999999999996e-05, Elapsed Time: 13131.18 seconds\n",
            "Step 125500/150000, Loss: 4.470191869735718, Test Loss: 4.486340284347534, LR: 4.4999999999999996e-05, Elapsed Time: 13141.66 seconds\n",
            "Step 125600/150000, Loss: 4.458254261016846, Test Loss: 4.486109793186188, LR: 4.4999999999999996e-05, Elapsed Time: 13152.08 seconds\n",
            "Step 125700/150000, Loss: 4.467713441848755, Test Loss: 4.486522376537323, LR: 4.4999999999999996e-05, Elapsed Time: 13162.48 seconds\n",
            "Step 125800/150000, Loss: 4.465572094917297, Test Loss: 4.485868692398071, LR: 4.4999999999999996e-05, Elapsed Time: 13172.92 seconds\n",
            "Step 125900/150000, Loss: 4.460412192344665, Test Loss: 4.486004710197449, LR: 4.4999999999999996e-05, Elapsed Time: 13183.39 seconds\n",
            "Step 126000/150000, Loss: 4.471394152641296, Test Loss: 4.485243856906891, LR: 4.4999999999999996e-05, Elapsed Time: 13193.90 seconds\n",
            "Step 126100/150000, Loss: 4.462978682518005, Test Loss: 4.485128104686737, LR: 4.4999999999999996e-05, Elapsed Time: 13204.37 seconds\n",
            "Step 126200/150000, Loss: 4.47186026096344, Test Loss: 4.48541784286499, LR: 4.4999999999999996e-05, Elapsed Time: 13214.81 seconds\n",
            "Step 126300/150000, Loss: 4.456851267814637, Test Loss: 4.485319554805756, LR: 4.4999999999999996e-05, Elapsed Time: 13225.21 seconds\n",
            "Step 126400/150000, Loss: 4.45381320476532, Test Loss: 4.485398888587952, LR: 4.4999999999999996e-05, Elapsed Time: 13235.64 seconds\n",
            "Step 126500/150000, Loss: 4.469040369987487, Test Loss: 4.485400378704071, LR: 4.4999999999999996e-05, Elapsed Time: 13246.14 seconds\n",
            "Step 126600/150000, Loss: 4.461485404968261, Test Loss: 4.484817206859589, LR: 4.4999999999999996e-05, Elapsed Time: 13256.63 seconds\n",
            "Step 126700/150000, Loss: 4.470855569839477, Test Loss: 4.484742999076843, LR: 4.4999999999999996e-05, Elapsed Time: 13267.01 seconds\n",
            "Step 126800/150000, Loss: 4.4615496635437015, Test Loss: 4.485148310661316, LR: 4.4999999999999996e-05, Elapsed Time: 13277.56 seconds\n",
            "Step 126900/150000, Loss: 4.4453039693832395, Test Loss: 4.486326456069946, LR: 4.4999999999999996e-05, Elapsed Time: 13288.02 seconds\n",
            "Step 127000/150000, Loss: 4.45172233581543, Test Loss: 4.485834777355194, LR: 4.4999999999999996e-05, Elapsed Time: 13298.53 seconds\n",
            "Step 127100/150000, Loss: 4.4639071226119995, Test Loss: 4.48524683713913, LR: 4.4999999999999996e-05, Elapsed Time: 13309.03 seconds\n",
            "Step 127200/150000, Loss: 4.468339996337891, Test Loss: 4.484718501567841, LR: 4.4999999999999996e-05, Elapsed Time: 13319.47 seconds\n",
            "Step 127300/150000, Loss: 4.455747904777527, Test Loss: 4.485554397106171, LR: 4.4999999999999996e-05, Elapsed Time: 13329.92 seconds\n",
            "Step 127400/150000, Loss: 4.465098557472229, Test Loss: 4.4841853976249695, LR: 4.4999999999999996e-05, Elapsed Time: 13340.36 seconds\n",
            "Step 127500/150000, Loss: 4.458192391395569, Test Loss: 4.484564423561096, LR: 4.4999999999999996e-05, Elapsed Time: 13350.80 seconds\n",
            "Step 127600/150000, Loss: 4.457617545127869, Test Loss: 4.484224617481232, LR: 4.4999999999999996e-05, Elapsed Time: 13361.20 seconds\n",
            "Step 127700/150000, Loss: 4.448497743606567, Test Loss: 4.4840662479400635, LR: 4.4999999999999996e-05, Elapsed Time: 13371.59 seconds\n",
            "Step 127800/150000, Loss: 4.460550222396851, Test Loss: 4.484978675842285, LR: 4.4999999999999996e-05, Elapsed Time: 13382.02 seconds\n",
            "Step 127900/150000, Loss: 4.466691884994507, Test Loss: 4.484026849269867, LR: 4.4999999999999996e-05, Elapsed Time: 13392.42 seconds\n",
            "Step 128000/150000, Loss: 4.467158412933349, Test Loss: 4.484635889530182, LR: 4.4999999999999996e-05, Elapsed Time: 13402.81 seconds\n",
            "Step 128100/150000, Loss: 4.45674994468689, Test Loss: 4.483771324157715, LR: 4.4999999999999996e-05, Elapsed Time: 13413.20 seconds\n",
            "Step 128200/150000, Loss: 4.464687805175782, Test Loss: 4.484904766082764, LR: 4.4999999999999996e-05, Elapsed Time: 13423.67 seconds\n",
            "Step 128300/150000, Loss: 4.461249589920044, Test Loss: 4.483905732631683, LR: 4.4999999999999996e-05, Elapsed Time: 13434.14 seconds\n",
            "Step 128400/150000, Loss: 4.450422458648681, Test Loss: 4.483882963657379, LR: 4.4999999999999996e-05, Elapsed Time: 13444.63 seconds\n",
            "Step 128500/150000, Loss: 4.45587944984436, Test Loss: 4.484372675418854, LR: 4.4999999999999996e-05, Elapsed Time: 13455.08 seconds\n",
            "Step 128600/150000, Loss: 4.449896788597107, Test Loss: 4.481815874576569, LR: 1.3499999999999998e-05, Elapsed Time: 13465.55 seconds\n",
            "Step 128700/150000, Loss: 4.449651756286621, Test Loss: 4.481597542762756, LR: 1.3499999999999998e-05, Elapsed Time: 13476.02 seconds\n",
            "Step 128800/150000, Loss: 4.455052027702331, Test Loss: 4.480638802051544, LR: 1.3499999999999998e-05, Elapsed Time: 13486.46 seconds\n",
            "Step 128900/150000, Loss: 4.451883678436279, Test Loss: 4.480772316455841, LR: 1.3499999999999998e-05, Elapsed Time: 13496.92 seconds\n",
            "Step 129000/150000, Loss: 4.447164330482483, Test Loss: 4.481145143508911, LR: 1.3499999999999998e-05, Elapsed Time: 13507.40 seconds\n",
            "Step 129100/150000, Loss: 4.452170372009277, Test Loss: 4.4813355803489685, LR: 1.3499999999999998e-05, Elapsed Time: 13517.80 seconds\n",
            "Step 129200/150000, Loss: 4.4543700742721555, Test Loss: 4.480516850948334, LR: 1.3499999999999998e-05, Elapsed Time: 13528.29 seconds\n",
            "Step 129300/150000, Loss: 4.455539760589599, Test Loss: 4.480832159519196, LR: 1.3499999999999998e-05, Elapsed Time: 13538.76 seconds\n",
            "Step 129400/150000, Loss: 4.459345836639404, Test Loss: 4.480493187904358, LR: 1.3499999999999998e-05, Elapsed Time: 13549.28 seconds\n",
            "Step 129500/150000, Loss: 4.4450619554519655, Test Loss: 4.480683922767639, LR: 1.3499999999999998e-05, Elapsed Time: 13559.78 seconds\n",
            "Step 129600/150000, Loss: 4.44326199054718, Test Loss: 4.4805707931518555, LR: 1.3499999999999998e-05, Elapsed Time: 13570.26 seconds\n",
            "Step 129700/150000, Loss: 4.4537488460540775, Test Loss: 4.480657458305359, LR: 1.3499999999999998e-05, Elapsed Time: 13580.76 seconds\n",
            "Step 129800/150000, Loss: 4.4470742893218995, Test Loss: 4.480579197406769, LR: 1.3499999999999998e-05, Elapsed Time: 13591.30 seconds\n",
            "Step 129900/150000, Loss: 4.454203004837036, Test Loss: 4.480512380599976, LR: 1.3499999999999998e-05, Elapsed Time: 13601.76 seconds\n",
            "Step 130000/150000, Loss: 4.444230542182923, Test Loss: 4.479863166809082, LR: 5e-06, Elapsed Time: 13612.22 seconds\n",
            "Step 130100/150000, Loss: 4.453616781234741, Test Loss: 4.479707479476929, LR: 5e-06, Elapsed Time: 13622.69 seconds\n",
            "Step 130200/150000, Loss: 4.4402565050125125, Test Loss: 4.4796125292778015, LR: 5e-06, Elapsed Time: 13633.12 seconds\n",
            "Step 130300/150000, Loss: 4.440785131454468, Test Loss: 4.479307174682617, LR: 5e-06, Elapsed Time: 13643.58 seconds\n",
            "Step 130400/150000, Loss: 4.464026141166687, Test Loss: 4.479389727115631, LR: 5e-06, Elapsed Time: 13654.05 seconds\n",
            "Step 130500/150000, Loss: 4.446814670562744, Test Loss: 4.479382872581482, LR: 5e-06, Elapsed Time: 13664.48 seconds\n",
            "Step 130600/150000, Loss: 4.458519659042358, Test Loss: 4.479211151599884, LR: 5e-06, Elapsed Time: 13674.86 seconds\n",
            "Step 130700/150000, Loss: 4.453226027488708, Test Loss: 4.479368686676025, LR: 5e-06, Elapsed Time: 13685.26 seconds\n",
            "Step 130800/150000, Loss: 4.451517472267151, Test Loss: 4.479521751403809, LR: 5e-06, Elapsed Time: 13695.67 seconds\n",
            "Step 130900/150000, Loss: 4.453995113372803, Test Loss: 4.479201138019562, LR: 5e-06, Elapsed Time: 13706.10 seconds\n",
            "Step 131000/150000, Loss: 4.44236234664917, Test Loss: 4.479627728462219, LR: 5e-06, Elapsed Time: 13716.50 seconds\n",
            "Step 131100/150000, Loss: 4.4512608623504635, Test Loss: 4.479277670383453, LR: 5e-06, Elapsed Time: 13726.89 seconds\n",
            "Step 131200/150000, Loss: 4.446147084236145, Test Loss: 4.479288280010223, LR: 5e-06, Elapsed Time: 13737.33 seconds\n",
            "Step 131300/150000, Loss: 4.445587630271912, Test Loss: 4.47925478219986, LR: 5e-06, Elapsed Time: 13747.78 seconds\n",
            "Step 131400/150000, Loss: 4.441020140647888, Test Loss: 4.479016423225403, LR: 5e-06, Elapsed Time: 13758.20 seconds\n",
            "Step 131500/150000, Loss: 4.447850136756897, Test Loss: 4.478977560997009, LR: 5e-06, Elapsed Time: 13768.61 seconds\n",
            "Step 131600/150000, Loss: 4.441596531867981, Test Loss: 4.479114472866058, LR: 5e-06, Elapsed Time: 13779.01 seconds\n",
            "Step 131700/150000, Loss: 4.446668605804444, Test Loss: 4.479244112968445, LR: 5e-06, Elapsed Time: 13789.47 seconds\n",
            "Step 131800/150000, Loss: 4.444085536003112, Test Loss: 4.479072630405426, LR: 5e-06, Elapsed Time: 13799.92 seconds\n",
            "Step 131900/150000, Loss: 4.445896315574646, Test Loss: 4.479025602340698, LR: 5e-06, Elapsed Time: 13810.36 seconds\n",
            "Step 132000/150000, Loss: 4.448441534042359, Test Loss: 4.479126989841461, LR: 5e-06, Elapsed Time: 13820.80 seconds\n",
            "Step 132100/150000, Loss: 4.441824765205383, Test Loss: 4.479188501834869, LR: 5e-06, Elapsed Time: 13831.20 seconds\n",
            "Step 132200/150000, Loss: 4.4359630107879635, Test Loss: 4.479049980640411, LR: 5e-06, Elapsed Time: 13841.64 seconds\n",
            "Step 132300/150000, Loss: 4.424449138641357, Test Loss: 4.479227006435394, LR: 5e-06, Elapsed Time: 13852.08 seconds\n",
            "Step 132400/150000, Loss: 4.4394665670394895, Test Loss: 4.479380905628204, LR: 5e-06, Elapsed Time: 13862.52 seconds\n",
            "Step 132500/150000, Loss: 4.438440752029419, Test Loss: 4.479095220565796, LR: 5e-06, Elapsed Time: 13872.95 seconds\n",
            "Step 132600/150000, Loss: 4.431824889183044, Test Loss: 4.479286074638367, LR: 5e-06, Elapsed Time: 13883.39 seconds\n",
            "Step 132700/150000, Loss: 4.448132047653198, Test Loss: 4.479146838188171, LR: 5e-06, Elapsed Time: 13893.82 seconds\n",
            "Step 132800/150000, Loss: 4.435203351974487, Test Loss: 4.479044258594513, LR: 5e-06, Elapsed Time: 13904.26 seconds\n",
            "Step 132900/150000, Loss: 4.444718046188354, Test Loss: 4.479381024837494, LR: 5e-06, Elapsed Time: 13914.74 seconds\n",
            "Step 133000/150000, Loss: 4.446352291107178, Test Loss: 4.479088723659515, LR: 5e-06, Elapsed Time: 13925.17 seconds\n",
            "Step 133100/150000, Loss: 4.434353976249695, Test Loss: 4.4789891839027405, LR: 5e-06, Elapsed Time: 13935.61 seconds\n",
            "Step 133200/150000, Loss: 4.429741969108582, Test Loss: 4.479423403739929, LR: 5e-06, Elapsed Time: 13946.13 seconds\n",
            "Step 133300/150000, Loss: 4.447629985809326, Test Loss: 4.479048132896423, LR: 5e-06, Elapsed Time: 13956.61 seconds\n",
            "Step 133400/150000, Loss: 4.436065187454224, Test Loss: 4.4789658188819885, LR: 5e-06, Elapsed Time: 13967.12 seconds\n",
            "Step 133500/150000, Loss: 4.433476228713989, Test Loss: 4.479179739952087, LR: 5e-06, Elapsed Time: 13977.64 seconds\n",
            "Step 133600/150000, Loss: 4.446922926902771, Test Loss: 4.478704750537872, LR: 5e-06, Elapsed Time: 13988.11 seconds\n",
            "Step 133700/150000, Loss: 4.4583496522903445, Test Loss: 4.4789846539497375, LR: 5e-06, Elapsed Time: 13998.60 seconds\n",
            "Step 133800/150000, Loss: 4.453595714569092, Test Loss: 4.478910565376282, LR: 5e-06, Elapsed Time: 14009.07 seconds\n",
            "Step 133900/150000, Loss: 4.45616678237915, Test Loss: 4.478927254676819, LR: 5e-06, Elapsed Time: 14019.56 seconds\n",
            "Step 134000/150000, Loss: 4.451961078643799, Test Loss: 4.478913426399231, LR: 5e-06, Elapsed Time: 14029.95 seconds\n",
            "Step 134100/150000, Loss: 4.451156849861145, Test Loss: 4.478847563266754, LR: 5e-06, Elapsed Time: 14040.36 seconds\n",
            "Step 134200/150000, Loss: 4.452733325958252, Test Loss: 4.479070961475372, LR: 5e-06, Elapsed Time: 14050.78 seconds\n",
            "Step 134300/150000, Loss: 4.450558843612671, Test Loss: 4.479029715061188, LR: 5e-06, Elapsed Time: 14061.25 seconds\n",
            "Step 134400/150000, Loss: 4.458207788467408, Test Loss: 4.478677034378052, LR: 5e-06, Elapsed Time: 14071.67 seconds\n",
            "Step 134500/150000, Loss: 4.465290102958679, Test Loss: 4.4786606431007385, LR: 5e-06, Elapsed Time: 14082.11 seconds\n",
            "Step 134600/150000, Loss: 4.460664262771607, Test Loss: 4.478908956050873, LR: 5e-06, Elapsed Time: 14092.55 seconds\n",
            "Step 134700/150000, Loss: 4.466694478988647, Test Loss: 4.4786266684532166, LR: 5e-06, Elapsed Time: 14102.98 seconds\n",
            "Step 134800/150000, Loss: 4.458388533592224, Test Loss: 4.478971481323242, LR: 5e-06, Elapsed Time: 14113.50 seconds\n",
            "Step 134900/150000, Loss: 4.457404327392578, Test Loss: 4.478647589683533, LR: 5e-06, Elapsed Time: 14123.95 seconds\n",
            "Step 135000/150000, Loss: 4.457415971755982, Test Loss: 4.478706359863281, LR: 5e-06, Elapsed Time: 14134.42 seconds\n",
            "Step 135100/150000, Loss: 4.467681994438172, Test Loss: 4.478459000587463, LR: 5e-06, Elapsed Time: 14144.93 seconds\n",
            "Step 135200/150000, Loss: 4.45432888507843, Test Loss: 4.478470325469971, LR: 5e-06, Elapsed Time: 14155.37 seconds\n",
            "Step 135300/150000, Loss: 4.460258412361145, Test Loss: 4.478568494319916, LR: 5e-06, Elapsed Time: 14165.84 seconds\n",
            "Step 135400/150000, Loss: 4.450377073287964, Test Loss: 4.478768587112427, LR: 5e-06, Elapsed Time: 14176.34 seconds\n",
            "Step 135500/150000, Loss: 4.450869164466858, Test Loss: 4.478575527667999, LR: 5e-06, Elapsed Time: 14186.80 seconds\n",
            "Step 135600/150000, Loss: 4.46503155708313, Test Loss: 4.47839629650116, LR: 5e-06, Elapsed Time: 14197.30 seconds\n",
            "Step 135700/150000, Loss: 4.448193969726563, Test Loss: 4.478398025035858, LR: 5e-06, Elapsed Time: 14207.79 seconds\n",
            "Step 135800/150000, Loss: 4.461751093864441, Test Loss: 4.478470742702484, LR: 5e-06, Elapsed Time: 14218.25 seconds\n",
            "Step 135900/150000, Loss: 4.451924252510071, Test Loss: 4.478416204452515, LR: 5e-06, Elapsed Time: 14228.67 seconds\n",
            "Step 136000/150000, Loss: 4.459493503570557, Test Loss: 4.478494107723236, LR: 5e-06, Elapsed Time: 14239.13 seconds\n",
            "Step 136100/150000, Loss: 4.459863262176514, Test Loss: 4.478583514690399, LR: 5e-06, Elapsed Time: 14249.64 seconds\n",
            "Step 136200/150000, Loss: 4.460433435440064, Test Loss: 4.478476822376251, LR: 5e-06, Elapsed Time: 14260.04 seconds\n",
            "Step 136300/150000, Loss: 4.44530463218689, Test Loss: 4.478290855884552, LR: 5e-06, Elapsed Time: 14270.53 seconds\n",
            "Step 136400/150000, Loss: 4.452734093666077, Test Loss: 4.4783191084861755, LR: 5e-06, Elapsed Time: 14280.94 seconds\n",
            "Step 136500/150000, Loss: 4.451989979743957, Test Loss: 4.478296458721161, LR: 5e-06, Elapsed Time: 14291.42 seconds\n",
            "Step 136600/150000, Loss: 4.450274305343628, Test Loss: 4.47839891910553, LR: 5e-06, Elapsed Time: 14301.82 seconds\n",
            "Step 136700/150000, Loss: 4.457493181228638, Test Loss: 4.478432476520538, LR: 5e-06, Elapsed Time: 14312.28 seconds\n",
            "Step 136800/150000, Loss: 4.4551367712020875, Test Loss: 4.478426277637482, LR: 5e-06, Elapsed Time: 14322.68 seconds\n",
            "Step 136900/150000, Loss: 4.451749324798584, Test Loss: 4.478342950344086, LR: 5e-06, Elapsed Time: 14333.09 seconds\n",
            "Step 137000/150000, Loss: 4.455621113777161, Test Loss: 4.478265762329102, LR: 5e-06, Elapsed Time: 14343.49 seconds\n",
            "Step 137100/150000, Loss: 4.445135159492493, Test Loss: 4.478258669376373, LR: 5e-06, Elapsed Time: 14353.98 seconds\n",
            "Step 137200/150000, Loss: 4.457284317016602, Test Loss: 4.478313326835632, LR: 5e-06, Elapsed Time: 14364.43 seconds\n",
            "Step 137300/150000, Loss: 4.463478903770447, Test Loss: 4.478327512741089, LR: 5e-06, Elapsed Time: 14374.88 seconds\n",
            "Step 137400/150000, Loss: 4.448733448982239, Test Loss: 4.4785667061805725, LR: 5e-06, Elapsed Time: 14385.32 seconds\n",
            "Step 137500/150000, Loss: 4.44860643863678, Test Loss: 4.4781622886657715, LR: 5e-06, Elapsed Time: 14395.73 seconds\n",
            "Step 137600/150000, Loss: 4.455298166275025, Test Loss: 4.478264391422272, LR: 5e-06, Elapsed Time: 14406.15 seconds\n",
            "Step 137700/150000, Loss: 4.448502583503723, Test Loss: 4.47821718454361, LR: 5e-06, Elapsed Time: 14416.56 seconds\n",
            "Step 137800/150000, Loss: 4.460841007232666, Test Loss: 4.47849828004837, LR: 5e-06, Elapsed Time: 14426.97 seconds\n",
            "Step 137900/150000, Loss: 4.4515567111969, Test Loss: 4.4783583879470825, LR: 5e-06, Elapsed Time: 14437.57 seconds\n",
            "Step 138000/150000, Loss: 4.449024171829223, Test Loss: 4.478369414806366, LR: 5e-06, Elapsed Time: 14448.18 seconds\n",
            "Step 138100/150000, Loss: 4.449520902633667, Test Loss: 4.478293001651764, LR: 5e-06, Elapsed Time: 14458.67 seconds\n",
            "Step 138200/150000, Loss: 4.449461722373963, Test Loss: 4.4785783886909485, LR: 5e-06, Elapsed Time: 14469.19 seconds\n",
            "Step 138300/150000, Loss: 4.4479807043075565, Test Loss: 4.47832727432251, LR: 5e-06, Elapsed Time: 14479.85 seconds\n",
            "Step 138400/150000, Loss: 4.449244651794434, Test Loss: 4.478243708610535, LR: 5e-06, Elapsed Time: 14490.64 seconds\n",
            "Step 138500/150000, Loss: 4.449891104698181, Test Loss: 4.478472411632538, LR: 5e-06, Elapsed Time: 14501.49 seconds\n",
            "Step 138600/150000, Loss: 4.440348033905029, Test Loss: 4.478163540363312, LR: 5e-06, Elapsed Time: 14512.34 seconds\n",
            "Step 138700/150000, Loss: 4.459986472129822, Test Loss: 4.4785425662994385, LR: 5e-06, Elapsed Time: 14523.20 seconds\n",
            "Step 138800/150000, Loss: 4.440903420448303, Test Loss: 4.478104412555695, LR: 5e-06, Elapsed Time: 14533.95 seconds\n",
            "Step 138900/150000, Loss: 4.448002452850342, Test Loss: 4.478087842464447, LR: 5e-06, Elapsed Time: 14544.64 seconds\n",
            "Step 139000/150000, Loss: 4.452141966819763, Test Loss: 4.47807502746582, LR: 5e-06, Elapsed Time: 14555.26 seconds\n",
            "Step 139100/150000, Loss: 4.442619466781617, Test Loss: 4.478442847728729, LR: 5e-06, Elapsed Time: 14565.88 seconds\n",
            "Step 139200/150000, Loss: 4.446925511360169, Test Loss: 4.4781383872032166, LR: 5e-06, Elapsed Time: 14576.39 seconds\n",
            "Step 139300/150000, Loss: 4.4426392078399655, Test Loss: 4.478105306625366, LR: 5e-06, Elapsed Time: 14586.88 seconds\n",
            "Step 139400/150000, Loss: 4.45004243850708, Test Loss: 4.47863233089447, LR: 5e-06, Elapsed Time: 14597.33 seconds\n",
            "Step 139500/150000, Loss: 4.443422069549561, Test Loss: 4.4782615303993225, LR: 5e-06, Elapsed Time: 14607.82 seconds\n",
            "Step 139600/150000, Loss: 4.459392757415771, Test Loss: 4.478220582008362, LR: 5e-06, Elapsed Time: 14618.30 seconds\n",
            "Step 139700/150000, Loss: 4.44310875415802, Test Loss: 4.478226840496063, LR: 5e-06, Elapsed Time: 14628.86 seconds\n",
            "Step 139800/150000, Loss: 4.4535003185272215, Test Loss: 4.478256821632385, LR: 5e-06, Elapsed Time: 14639.29 seconds\n",
            "Step 139900/150000, Loss: 4.444937100410462, Test Loss: 4.47806715965271, LR: 5e-06, Elapsed Time: 14649.77 seconds\n",
            "Step 140000/150000, Loss: 4.439912452697754, Test Loss: 4.478237509727478, LR: 5e-06, Elapsed Time: 14660.23 seconds\n",
            "Step 140100/150000, Loss: 4.445317759513855, Test Loss: 4.478129148483276, LR: 5e-06, Elapsed Time: 14670.70 seconds\n",
            "Step 140200/150000, Loss: 4.451802768707275, Test Loss: 4.47803395986557, LR: 5e-06, Elapsed Time: 14681.10 seconds\n",
            "Step 140300/150000, Loss: 4.439867057800293, Test Loss: 4.47809910774231, LR: 5e-06, Elapsed Time: 14691.52 seconds\n",
            "Step 140400/150000, Loss: 4.449755959510803, Test Loss: 4.4781341552734375, LR: 5e-06, Elapsed Time: 14701.92 seconds\n",
            "Step 140500/150000, Loss: 4.456595873832702, Test Loss: 4.4780572056770325, LR: 5e-06, Elapsed Time: 14712.31 seconds\n",
            "Step 140600/150000, Loss: 4.445584444999695, Test Loss: 4.4781962633132935, LR: 5e-06, Elapsed Time: 14722.73 seconds\n",
            "Step 140700/150000, Loss: 4.431318507194519, Test Loss: 4.477784752845764, LR: 5e-06, Elapsed Time: 14733.22 seconds\n",
            "Step 140800/150000, Loss: 4.451277441978455, Test Loss: 4.478279292583466, LR: 5e-06, Elapsed Time: 14743.77 seconds\n",
            "Step 140900/150000, Loss: 4.442976865768433, Test Loss: 4.478408694267273, LR: 5e-06, Elapsed Time: 14754.26 seconds\n",
            "Step 141000/150000, Loss: 4.4473585033416745, Test Loss: 4.478288233280182, LR: 5e-06, Elapsed Time: 14764.71 seconds\n",
            "Step 141100/150000, Loss: 4.439138388633728, Test Loss: 4.478205859661102, LR: 5e-06, Elapsed Time: 14775.22 seconds\n",
            "Step 141200/150000, Loss: 4.4387941074371335, Test Loss: 4.478100299835205, LR: 5e-06, Elapsed Time: 14785.73 seconds\n",
            "Step 141300/150000, Loss: 4.445084519386292, Test Loss: 4.47817325592041, LR: 5e-06, Elapsed Time: 14796.24 seconds\n",
            "Step 141400/150000, Loss: 4.445789151191711, Test Loss: 4.4779932498931885, LR: 5e-06, Elapsed Time: 14806.74 seconds\n",
            "Step 141500/150000, Loss: 4.453622083663941, Test Loss: 4.477752864360809, LR: 5e-06, Elapsed Time: 14817.23 seconds\n",
            "Step 141600/150000, Loss: 4.439001026153565, Test Loss: 4.477730393409729, LR: 5e-06, Elapsed Time: 14827.68 seconds\n",
            "Step 141700/150000, Loss: 4.4461033821105955, Test Loss: 4.478102266788483, LR: 5e-06, Elapsed Time: 14838.18 seconds\n",
            "Step 141800/150000, Loss: 4.451633529663086, Test Loss: 4.478155732154846, LR: 5e-06, Elapsed Time: 14848.74 seconds\n",
            "Step 141900/150000, Loss: 4.444929718971252, Test Loss: 4.478077471256256, LR: 5e-06, Elapsed Time: 14859.22 seconds\n",
            "Step 142000/150000, Loss: 4.444979481697082, Test Loss: 4.477997601032257, LR: 5e-06, Elapsed Time: 14869.78 seconds\n",
            "Step 142100/150000, Loss: 4.446121497154236, Test Loss: 4.478193759918213, LR: 5e-06, Elapsed Time: 14880.35 seconds\n",
            "Step 142200/150000, Loss: 4.458120980262756, Test Loss: 4.478077828884125, LR: 5e-06, Elapsed Time: 14890.84 seconds\n",
            "Step 142300/150000, Loss: 4.444281477928161, Test Loss: 4.477860331535339, LR: 5e-06, Elapsed Time: 14901.30 seconds\n",
            "Step 142400/150000, Loss: 4.455064721107483, Test Loss: 4.478033363819122, LR: 5e-06, Elapsed Time: 14911.77 seconds\n",
            "Step 142500/150000, Loss: 4.454940156936646, Test Loss: 4.477924644947052, LR: 5e-06, Elapsed Time: 14922.29 seconds\n",
            "Step 142600/150000, Loss: 4.453886032104492, Test Loss: 4.4777637124061584, LR: 5e-06, Elapsed Time: 14932.83 seconds\n",
            "Step 142700/150000, Loss: 4.457716779708862, Test Loss: 4.477853059768677, LR: 5e-06, Elapsed Time: 14943.33 seconds\n",
            "Step 142800/150000, Loss: 4.461301555633545, Test Loss: 4.477818489074707, LR: 5e-06, Elapsed Time: 14953.83 seconds\n",
            "Step 142900/150000, Loss: 4.463912057876587, Test Loss: 4.47791588306427, LR: 5e-06, Elapsed Time: 14964.39 seconds\n",
            "Step 143000/150000, Loss: 4.451261177062988, Test Loss: 4.47802460193634, LR: 5e-06, Elapsed Time: 14974.82 seconds\n",
            "Step 143100/150000, Loss: 4.450143213272095, Test Loss: 4.477888524532318, LR: 5e-06, Elapsed Time: 14985.29 seconds\n",
            "Step 143200/150000, Loss: 4.457161793708801, Test Loss: 4.477930963039398, LR: 5e-06, Elapsed Time: 14995.81 seconds\n",
            "Step 143300/150000, Loss: 4.4597063684463505, Test Loss: 4.477966725826263, LR: 5e-06, Elapsed Time: 15006.31 seconds\n",
            "Step 143400/150000, Loss: 4.463327369689941, Test Loss: 4.477759897708893, LR: 5e-06, Elapsed Time: 15016.80 seconds\n",
            "Step 143500/150000, Loss: 4.461476716995239, Test Loss: 4.4779563546180725, LR: 5e-06, Elapsed Time: 15027.30 seconds\n",
            "Step 143600/150000, Loss: 4.459793868064881, Test Loss: 4.477846682071686, LR: 5e-06, Elapsed Time: 15037.87 seconds\n",
            "Step 143700/150000, Loss: 4.44737868309021, Test Loss: 4.477720677852631, LR: 5e-06, Elapsed Time: 15048.34 seconds\n",
            "Step 143800/150000, Loss: 4.4579430150985715, Test Loss: 4.477664887905121, LR: 5e-06, Elapsed Time: 15058.84 seconds\n",
            "Step 143900/150000, Loss: 4.460228509902954, Test Loss: 4.477552592754364, LR: 5e-06, Elapsed Time: 15069.33 seconds\n",
            "Step 144000/150000, Loss: 4.455417423248291, Test Loss: 4.477795600891113, LR: 5e-06, Elapsed Time: 15079.79 seconds\n",
            "Step 144100/150000, Loss: 4.453219032287597, Test Loss: 4.477742433547974, LR: 5e-06, Elapsed Time: 15090.25 seconds\n",
            "Step 144200/150000, Loss: 4.458648386001587, Test Loss: 4.477952420711517, LR: 5e-06, Elapsed Time: 15100.76 seconds\n",
            "Step 144300/150000, Loss: 4.44669912815094, Test Loss: 4.477637588977814, LR: 5e-06, Elapsed Time: 15111.29 seconds\n",
            "Step 144400/150000, Loss: 4.44953161239624, Test Loss: 4.47789466381073, LR: 5e-06, Elapsed Time: 15121.81 seconds\n",
            "Step 144500/150000, Loss: 4.455354633331299, Test Loss: 4.477940082550049, LR: 5e-06, Elapsed Time: 15132.31 seconds\n",
            "Step 144600/150000, Loss: 4.4668526935577395, Test Loss: 4.477588534355164, LR: 5e-06, Elapsed Time: 15142.88 seconds\n",
            "Step 144700/150000, Loss: 4.454244751930236, Test Loss: 4.477806568145752, LR: 5e-06, Elapsed Time: 15153.41 seconds\n",
            "Step 144800/150000, Loss: 4.442389678955078, Test Loss: 4.477844536304474, LR: 5e-06, Elapsed Time: 15163.93 seconds\n",
            "Step 144900/150000, Loss: 4.451730027198791, Test Loss: 4.477531850337982, LR: 5e-06, Elapsed Time: 15174.41 seconds\n",
            "Step 145000/150000, Loss: 4.448106727600098, Test Loss: 4.477595031261444, LR: 5e-06, Elapsed Time: 15184.89 seconds\n",
            "Step 145100/150000, Loss: 4.453576111793518, Test Loss: 4.47765439748764, LR: 5e-06, Elapsed Time: 15195.36 seconds\n",
            "Step 145200/150000, Loss: 4.452858099937439, Test Loss: 4.47746068239212, LR: 5e-06, Elapsed Time: 15205.81 seconds\n",
            "Step 145300/150000, Loss: 4.456533522605896, Test Loss: 4.4775460958480835, LR: 5e-06, Elapsed Time: 15216.31 seconds\n",
            "Step 145400/150000, Loss: 4.455827317237854, Test Loss: 4.477603614330292, LR: 5e-06, Elapsed Time: 15226.78 seconds\n",
            "Step 145500/150000, Loss: 4.451658873558045, Test Loss: 4.4774357080459595, LR: 5e-06, Elapsed Time: 15237.24 seconds\n",
            "Step 145600/150000, Loss: 4.447464776039124, Test Loss: 4.477752447128296, LR: 5e-06, Elapsed Time: 15247.78 seconds\n",
            "Step 145700/150000, Loss: 4.46410873413086, Test Loss: 4.477738380432129, LR: 5e-06, Elapsed Time: 15258.27 seconds\n",
            "Step 145800/150000, Loss: 4.448485341072082, Test Loss: 4.4776571393013, LR: 5e-06, Elapsed Time: 15268.75 seconds\n",
            "Step 145900/150000, Loss: 4.440375475883484, Test Loss: 4.477726697921753, LR: 5e-06, Elapsed Time: 15279.20 seconds\n",
            "Step 146000/150000, Loss: 4.446234831809997, Test Loss: 4.477728307247162, LR: 5e-06, Elapsed Time: 15289.62 seconds\n",
            "Step 146100/150000, Loss: 4.4494277858734135, Test Loss: 4.477596640586853, LR: 5e-06, Elapsed Time: 15300.07 seconds\n",
            "Step 146200/150000, Loss: 4.448816304206848, Test Loss: 4.477940499782562, LR: 5e-06, Elapsed Time: 15310.56 seconds\n",
            "Step 146300/150000, Loss: 4.453729848861695, Test Loss: 4.477714240550995, LR: 5e-06, Elapsed Time: 15321.09 seconds\n",
            "Step 146400/150000, Loss: 4.447273626327514, Test Loss: 4.47768098115921, LR: 5e-06, Elapsed Time: 15331.65 seconds\n",
            "Step 146500/150000, Loss: 4.444137463569641, Test Loss: 4.477687835693359, LR: 5e-06, Elapsed Time: 15342.18 seconds\n",
            "Step 146600/150000, Loss: 4.4381750249862675, Test Loss: 4.477806985378265, LR: 5e-06, Elapsed Time: 15352.66 seconds\n",
            "Step 146700/150000, Loss: 4.463432188034058, Test Loss: 4.477578282356262, LR: 5e-06, Elapsed Time: 15363.18 seconds\n",
            "Step 146800/150000, Loss: 4.44898639202118, Test Loss: 4.477652311325073, LR: 5e-06, Elapsed Time: 15373.75 seconds\n",
            "Step 146900/150000, Loss: 4.446751656532288, Test Loss: 4.477584898471832, LR: 5e-06, Elapsed Time: 15384.22 seconds\n",
            "Step 147000/150000, Loss: 4.455843539237976, Test Loss: 4.477757453918457, LR: 5e-06, Elapsed Time: 15394.79 seconds\n",
            "Step 147100/150000, Loss: 4.44806161403656, Test Loss: 4.47775000333786, LR: 5e-06, Elapsed Time: 15405.38 seconds\n",
            "Step 147200/150000, Loss: 4.436804265975952, Test Loss: 4.478011071681976, LR: 5e-06, Elapsed Time: 15415.84 seconds\n",
            "Step 147300/150000, Loss: 4.447864966392517, Test Loss: 4.477620720863342, LR: 5e-06, Elapsed Time: 15426.33 seconds\n",
            "Step 147400/150000, Loss: 4.45115873336792, Test Loss: 4.477466642856598, LR: 5e-06, Elapsed Time: 15436.77 seconds\n",
            "Step 147500/150000, Loss: 4.442023587226868, Test Loss: 4.4776612520217896, LR: 5e-06, Elapsed Time: 15447.24 seconds\n",
            "Step 147600/150000, Loss: 4.458568711280822, Test Loss: 4.477674961090088, LR: 5e-06, Elapsed Time: 15457.75 seconds\n",
            "Step 147700/150000, Loss: 4.445251317024231, Test Loss: 4.477506756782532, LR: 5e-06, Elapsed Time: 15468.26 seconds\n",
            "Step 147800/150000, Loss: 4.444106588363647, Test Loss: 4.477727293968201, LR: 5e-06, Elapsed Time: 15478.74 seconds\n",
            "Step 147900/150000, Loss: 4.455885577201843, Test Loss: 4.477564752101898, LR: 5e-06, Elapsed Time: 15489.28 seconds\n",
            "Step 148000/150000, Loss: 4.4548672103881835, Test Loss: 4.47752833366394, LR: 5e-06, Elapsed Time: 15499.78 seconds\n",
            "Step 148100/150000, Loss: 4.4496666526794435, Test Loss: 4.477524757385254, LR: 5e-06, Elapsed Time: 15510.30 seconds\n",
            "Step 148200/150000, Loss: 4.444072656631469, Test Loss: 4.477586030960083, LR: 5e-06, Elapsed Time: 15520.83 seconds\n",
            "Step 148300/150000, Loss: 4.43893572807312, Test Loss: 4.477665185928345, LR: 5e-06, Elapsed Time: 15531.29 seconds\n",
            "Step 148400/150000, Loss: 4.450906119346619, Test Loss: 4.4776611328125, LR: 5e-06, Elapsed Time: 15541.79 seconds\n",
            "Step 148500/150000, Loss: 4.449767684936523, Test Loss: 4.47739976644516, LR: 5e-06, Elapsed Time: 15552.28 seconds\n",
            "Step 148600/150000, Loss: 4.458567199707031, Test Loss: 4.47760272026062, LR: 5e-06, Elapsed Time: 15562.77 seconds\n",
            "Step 148700/150000, Loss: 4.432878155708313, Test Loss: 4.477497458457947, LR: 5e-06, Elapsed Time: 15573.21 seconds\n",
            "Step 148800/150000, Loss: 4.435658230781555, Test Loss: 4.4777191281318665, LR: 5e-06, Elapsed Time: 15583.67 seconds\n",
            "Step 148900/150000, Loss: 4.444917263984681, Test Loss: 4.477657496929169, LR: 5e-06, Elapsed Time: 15594.08 seconds\n",
            "Step 149000/150000, Loss: 4.44398530960083, Test Loss: 4.477760970592499, LR: 5e-06, Elapsed Time: 15604.50 seconds\n",
            "Step 149100/150000, Loss: 4.45354805469513, Test Loss: 4.47755765914917, LR: 5e-06, Elapsed Time: 15614.99 seconds\n",
            "Step 149200/150000, Loss: 4.445557546615601, Test Loss: 4.477566242218018, LR: 5e-06, Elapsed Time: 15625.53 seconds\n",
            "Step 149300/150000, Loss: 4.447668471336365, Test Loss: 4.477498114109039, LR: 5e-06, Elapsed Time: 15636.07 seconds\n",
            "Step 149400/150000, Loss: 4.440193943977356, Test Loss: 4.477578401565552, LR: 5e-06, Elapsed Time: 15646.57 seconds\n",
            "Step 149500/150000, Loss: 4.445153255462646, Test Loss: 4.4774104952812195, LR: 5e-06, Elapsed Time: 15657.05 seconds\n",
            "Step 149600/150000, Loss: 4.432530446052551, Test Loss: 4.477606415748596, LR: 5e-06, Elapsed Time: 15667.58 seconds\n",
            "Step 149700/150000, Loss: 4.444754137992859, Test Loss: 4.477676808834076, LR: 5e-06, Elapsed Time: 15678.08 seconds\n",
            "Step 149800/150000, Loss: 4.454492030143737, Test Loss: 4.477569818496704, LR: 5e-06, Elapsed Time: 15688.60 seconds\n",
            "Step 149900/150000, Loss: 4.450811109542847, Test Loss: 4.477332055568695, LR: 5e-06, Elapsed Time: 15699.09 seconds\n",
            "Step 150000/150000, Loss: 4.440347175598145, Test Loss: 4.477678954601288, LR: 5e-06, Elapsed Time: 15709.59 seconds\n",
            "Saving model checkpoint at step 150000\n"
          ]
        }
      ],
      "source": [
        "# Example config:\n",
        "batch_size = 64\n",
        "sequence_len = 128\n",
        "num_steps = 150000\n",
        "accumulation_steps = 100\n",
        "\n",
        "\n",
        "# Reload the train and test datasets\n",
        "train_ds = load_dataset(\"parquet\", data_files=\"cnn_dailymail_train.parquet\", split=\"train\")\n",
        "test_ds = load_dataset(\"parquet\", data_files=\"cnn_dailymail_test.parquet\", split=\"train\")\n",
        "\n",
        "# Convert dataset to PyTorch format\n",
        "train_ds.set_format(\"torch\", columns=[\"input\", \"target\"])\n",
        "test_ds.set_format(\"torch\", columns=[\"input\", \"target\"])\n",
        "\n",
        "# Create DataLoaders for training and testing\n",
        "train_dataloader = cycle(DataLoader(train_ds, batch_size=batch_size, shuffle=False))\n",
        "test_dataloader = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "config = GPTConfig(\n",
        "    vocab_size=hf_tokenizer.vocab_size,\n",
        "    n_layer=8,   # fewer layers for a quick demo\n",
        "    n_head=8,\n",
        "    n_embd=128,\n",
        "    seq_len=sequence_len,\n",
        ")\n",
        "\n",
        "# Create the GPT model\n",
        "model = GPTModel(config)\n",
        "\n",
        "use_existing_model = os.path.exists(\"./pretrain_final.pth\")\n",
        "# Check if pre-trained model exists\n",
        "if use_existing_model:\n",
        "    model = torch.load(\"./pretrain_final.pth\")\n",
        "    print(\"Loaded pre-trained model from ./pretrain_final.pth, skipping training loop.\")\n",
        "\n",
        "else:\n",
        "    # Define the optimizer\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
        "\n",
        "\n",
        "    # Define Scheduler\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',factor=0.3, patience=10, min_lr=5e-6, threshold=1e-4)\n",
        "\n",
        "\n",
        "    # Training loop\n",
        "    losses = []\n",
        "    test_losses = []\n",
        "    accumulator = 0\n",
        "    accumulator_loss = 0\n",
        "    start_time = time.time()\n",
        "    for i in range(num_steps):\n",
        "        model.train()\n",
        "        example = next(train_dataloader)\n",
        "        train_input = example[\"input\"].to(device)\n",
        "        train_target = example[\"target\"].to(device)\n",
        "\n",
        "        logits = model(train_input)\n",
        "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), train_target.view(-1))\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        # Update weights\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        accumulator += 1\n",
        "        accumulator_loss += loss.item()\n",
        "\n",
        "        \n",
        "        if accumulator >= accumulation_steps:\n",
        "            losses.append(accumulator_loss / accumulation_steps)\n",
        "            accumulator = 0\n",
        "            accumulator_loss = 0\n",
        "            model.eval()\n",
        "            test_loss = 0\n",
        "            test_accumulator = 0\n",
        "            with torch.no_grad():\n",
        "                for test_example in test_dataloader:\n",
        "                    test_input = test_example[\"input\"].to(device)\n",
        "                    test_target = test_example[\"target\"].to(device)\n",
        "                    test_logits = model(test_input)\n",
        "                    test_loss += F.cross_entropy(test_logits.view(-1, test_logits.size(-1)), test_target.view(-1)).item()\n",
        "                    test_accumulator += 1\n",
        "                test_losses.append(test_loss / test_accumulator)\n",
        "                elapsed_time = time.time() - start_time\n",
        "                print(f\"Step {i+1}/{num_steps}, Loss: {losses[-1]}, Test Loss: {test_losses[-1]}, LR: {optimizer.param_groups[0]['lr']}, Elapsed Time: {elapsed_time:.2f} seconds\")\n",
        "                test_dataloader = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
        "                scheduler.step(test_losses[-1])\n",
        "\n",
        "   \n",
        "        if (i+1) % 50000 == 0:\n",
        "            # Save the model checkpoint\n",
        "            print(f\"Saving model checkpoint at step {i+1}\")\n",
        "            torch.save(model, f\"./model_checkpoint_{i}.pt\")\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "if use_existing_model:\n",
        "    print(\"Existing model used, no loss curves shown.\")\n",
        "    plt.imshow(plt.imread(\"./loss_curve.png\"))\n",
        "else:\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(losses, label=\"Train Loss\", color='blue')\n",
        "    plt.plot(test_losses, label=\"Test Loss\", color='red')\n",
        "    plt.xlabel('Checkpoint')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Test Loss Over Time')\n",
        "    plt.grid(True, linestyle='--', alpha=0.7)\n",
        "    plt.legend()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not use_existing_model:\n",
        "    torch.save(model, f\"./pretrain_final.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Inferencia del modelo pre-entrenado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Podemos ver que el modelo es capaz de producir inglés legible y que la mayoría de las palabras tienen sentido. Sin embargo, sus limitaciones de tamaño hacen que no sea tan sólido como los modelos más grandes. Aun así, es lo suficientemente bueno como para ver unos *sparks* de comprensión de lenguaje."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "def inference(prompt,torch_model, max_new_tokens):\n",
        "    torch_model.eval()\n",
        "    with torch.no_grad():\n",
        "        tokens = hf_tokenizer.encode(prompt)\n",
        "        for _ in range(max_new_tokens):\n",
        "            num_tokens = len(tokens)\n",
        "            tokens_padded = tokens + [hf_tokenizer.eos_token_id] * (config.seq_len - num_tokens)\n",
        "            tokens_padded = torch.tensor(tokens_padded).unsqueeze(0).to(device)\n",
        "            logits = torch_model(tokens_padded)\n",
        "            probabilities = torch.softmax(logits[0, num_tokens-1, :], dim=-1)\n",
        "            predicted_token = torch.multinomial(probabilities, 1).item()\n",
        "            tokens.append(predicted_token)\n",
        "        return hf_tokenizer.decode(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-02-15 09:21:39.354292: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-02-15 09:21:39.354348: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-02-15 09:21:39.355365: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-02-15 09:21:39.361722: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-02-15 09:21:40.186448: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted: The president signed a bill to pass the bailout policy, and that's not necessarily in the Democratic Republic.\" Obama seems to have his closest\n",
            "Predicted: There was a large division in some kind disguised battles to film Chinese. I voted myself again, but I think the Internet's terms\n",
            "Predicted: Reports are showing that their advances in optimal traffic culture where each athlete's shift is allowable using drones, an 8-strong\n"
          ]
        }
      ],
      "source": [
        "print(\"Predicted:\", inference(\"The president signed a bill to pass\", model, max_new_tokens=20))\n",
        "print(\"Predicted:\", inference(\"There was a large division in\", model, max_new_tokens=20))\n",
        "print(\"Predicted:\", inference(\"Reports are showing that\", model, max_new_tokens=20))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Supervised Fine-tuning (SFT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para hacer el modelo más usable, tomamos el modelo preentrenado y lo hacemos pasar por un proceso llamado **ajuste fino supervisado**. Este proceso consiste en usar conjuntos de datos de texto supervisados y de alta calidad para lograr que el modelo responda como queremos.\n",
        "\n",
        "Para esto podemos usar el conjunto de datos [Fact Q&A](https://huggingface.co/datasets/rubenroy/GammaCorpus-Fact-QA-450k?library=datasets) de Hugging Face. Este conjunto de datos consiste en ejemplos de preguntas y respuestas que son cortos, lo cual es bueno para nuestro caso de uso ya que tenemos una ventana de contexto pequeña de 128 tokens.\n",
        "\n",
        "El ajuste fino supervisado es donde podemos introducir “etiquetas” y otros tipos de tokens de texto que pueden ayudar al modelo a entender diferentes roles en el texto. Para nuestro conjunto de datos, tendremos una etiqueta de “question” y una de “answer”. Agregaremos todo esto cuando creemos nuestro conjunto de datos y también durante la inferencia cuando un usuario envíe una consulta. También añadimos tokens eos para finalizar o rellenar los ejemplos que no ocupen toda la ventana de contexto.\n",
        "\n",
        "Después de este *fine-tuning*, tendremos un LLM al que se le pueda hacer una pregunta y obtener una respuesta (una manera usual de interacción con los LLMs populares)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "549555c37b1542e48ab5fa9ca9d505bd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/2.01k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenized supervised fine tuning dataset does not exist locally... Generating and saving to disk.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cdb10b25365b4047a0e6de7b48027d9a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "55cc8f8162154955b6324ce778983fd2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Creating parquet from Arrow format:   0%|          | 0/440 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9a30cb2183314fc4a08bd1449afee090",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Saved 440000 train rows and 500 test rows for supervised fine tuning.\n"
          ]
        }
      ],
      "source": [
        "# Load dataset in streaming mode\n",
        "sft_ds = load_dataset(\"rubenroy/GammaCorpus-Fact-QA-450k\", split=\"train\", streaming=True)\n",
        "\n",
        "def check_sft_dataset_exists():\n",
        "    try:\n",
        "        # Attempt to load the dataset with reuse_cache_if_exists mode\n",
        "        load_dataset(\"parquet\", data_files=\"fact_qa_train.parquet\", split=\"train\")\n",
        "        load_dataset(\"parquet\", data_files=\"fact_qa_test.parquet\", split=\"train\")\n",
        "        return True\n",
        "    except FileNotFoundError:\n",
        "        return False\n",
        "    \n",
        "if not check_sft_dataset_exists():\n",
        "    print(\"Tokenized supervised fine tuning dataset does not exist locally... Generating and saving to disk.\")\n",
        "\n",
        "    def tokenize_and_chunk(dataset, tokenizer, chunk_size=512, rows=1000):\n",
        "        \"\"\"\n",
        "        Tokenizes and chunks the dataset into fixed-length 512-token segments.\n",
        "        The 'target' sequence is shifted left by 1 token.\n",
        "        Stops after generating `train_rows + test_rows` tokenized chunks.\n",
        "        \"\"\"\n",
        "        row_count = 0\n",
        "\n",
        "        for example in dataset:\n",
        "            question_plus_answer = \"<Question>\" + example[\"question\"] + \"</Question>\" + \"<Answer>\" + example[\"answer\"] + \"</Answer>\"\n",
        "            input_tokens = tokenizer(question_plus_answer, truncation=False, padding=False)['input_ids']\n",
        "\n",
        "            if row_count >= rows:\n",
        "                return\n",
        "\n",
        "            if len(input_tokens) >= chunk_size:\n",
        "                continue\n",
        "            else:\n",
        "                input_tokens = input_tokens +[tokenizer.eos_token_id] * (chunk_size - len(input_tokens))\n",
        "            \n",
        "            target_tokens = input_tokens[1:] + [tokenizer.eos_token_id]  # Shifted by 1 token\n",
        "\n",
        "            yield {\n",
        "                \"input\": input_tokens, \n",
        "                \"target\": target_tokens\n",
        "            }\n",
        "            \n",
        "            row_count += 1\n",
        "\n",
        "    # Set the max number of rows for training and testing\n",
        "    TRAIN_ROWS = 440000  # Adjust as needed\n",
        "    TEST_ROWS = 500   # Adjust as needed\n",
        "    CHUNK_SIZE = 128\n",
        "\n",
        "    # Convert generator to a Hugging Face Dataset\n",
        "    tokenized_sft_dataset = Dataset.from_generator(lambda: tokenize_and_chunk(sft_ds, hf_tokenizer,chunk_size=CHUNK_SIZE, rows=TRAIN_ROWS + TEST_ROWS))\n",
        "\n",
        "    # Split the dataset into `train` and `test`\n",
        "    sft_dataset_splits = tokenized_sft_dataset.train_test_split(train_size=TRAIN_ROWS, test_size=TEST_ROWS, seed=42)\n",
        "\n",
        "    # Save to disk\n",
        "    sft_dataset_splits[\"train\"].to_parquet(\"fact_qa_train.parquet\")\n",
        "    sft_dataset_splits[\"test\"].to_parquet(\"fact_qa_test.parquet\")\n",
        "\n",
        "    print(f\"✅ Saved {TRAIN_ROWS} train rows and {TEST_ROWS} test rows for supervised fine tuning.\")\n",
        "else:\n",
        "    print(\"SFT Tokenized dataset already exists locally.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Fine Tuning Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "849e965af11d46ce91568c53d8074005",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1a4b86491b7341abb97b34d6b86e093b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 100/50000, Loss: 1.9918309170007706, Test Loss: 0.7444852367043495\n",
            "Step 200/50000, Loss: 0.7175035107135773, Test Loss: 0.7121347188949585\n",
            "Step 300/50000, Loss: 0.7018899911642075, Test Loss: 0.6949615925550461\n",
            "Step 400/50000, Loss: 0.6908353179693222, Test Loss: 0.6887291222810745\n",
            "Step 500/50000, Loss: 0.6768182325363159, Test Loss: 0.6792757734656334\n",
            "Step 600/50000, Loss: 0.674177388548851, Test Loss: 0.6729135736823082\n",
            "Step 700/50000, Loss: 0.6727178591489792, Test Loss: 0.665991447865963\n",
            "Step 800/50000, Loss: 0.6694402080774308, Test Loss: 0.6644480600953102\n",
            "Step 900/50000, Loss: 0.6582337802648545, Test Loss: 0.6569188311696053\n",
            "Step 1000/50000, Loss: 0.6548500311374664, Test Loss: 0.6566447466611862\n",
            "Step 1100/50000, Loss: 0.6548229521512985, Test Loss: 0.651200458407402\n",
            "Step 1200/50000, Loss: 0.6486632919311524, Test Loss: 0.6475077718496323\n",
            "Step 1300/50000, Loss: 0.6484963500499725, Test Loss: 0.6474942564964294\n",
            "Step 1400/50000, Loss: 0.6471871453523635, Test Loss: 0.6445949599146843\n",
            "Step 1500/50000, Loss: 0.6443393552303314, Test Loss: 0.6420401781797409\n",
            "Step 1600/50000, Loss: 0.633151044845581, Test Loss: 0.638265810906887\n",
            "Step 1700/50000, Loss: 0.6382915496826171, Test Loss: 0.6368600502610207\n",
            "Step 1800/50000, Loss: 0.6347232925891876, Test Loss: 0.6353018656373024\n",
            "Step 1900/50000, Loss: 0.6338496947288513, Test Loss: 0.6344203874468803\n",
            "Step 2000/50000, Loss: 0.6316756427288055, Test Loss: 0.6306689009070396\n",
            "Step 2100/50000, Loss: 0.6319619745016098, Test Loss: 0.6294878572225571\n",
            "Step 2200/50000, Loss: 0.6291478699445725, Test Loss: 0.6268564462661743\n",
            "Step 2300/50000, Loss: 0.6278823328018188, Test Loss: 0.6255254969000816\n",
            "Step 2400/50000, Loss: 0.623234384059906, Test Loss: 0.6254716143012047\n",
            "Step 2500/50000, Loss: 0.6190310460329056, Test Loss: 0.6225540265440941\n",
            "Step 2600/50000, Loss: 0.6188549196720123, Test Loss: 0.6221257895231247\n",
            "Step 2700/50000, Loss: 0.618628158569336, Test Loss: 0.6187321320176125\n",
            "Step 2800/50000, Loss: 0.6215020614862442, Test Loss: 0.6191798150539398\n",
            "Step 2900/50000, Loss: 0.6181986683607101, Test Loss: 0.6164458692073822\n",
            "Step 3000/50000, Loss: 0.618198910355568, Test Loss: 0.6175436750054359\n",
            "Step 3100/50000, Loss: 0.6129893887043, Test Loss: 0.6151383370161057\n",
            "Step 3200/50000, Loss: 0.6147611361742019, Test Loss: 0.6149737536907196\n",
            "Step 3300/50000, Loss: 0.6152183496952057, Test Loss: 0.614312969148159\n",
            "Step 3400/50000, Loss: 0.6159565663337707, Test Loss: 0.6127408817410469\n",
            "Step 3500/50000, Loss: 0.6132823014259339, Test Loss: 0.611700989305973\n",
            "Step 3600/50000, Loss: 0.6138676339387894, Test Loss: 0.6131153330206871\n",
            "Step 3700/50000, Loss: 0.6062165397405624, Test Loss: 0.6110232546925545\n",
            "Step 3800/50000, Loss: 0.6079108506441117, Test Loss: 0.6089049205183983\n",
            "Step 3900/50000, Loss: 0.6074112200737, Test Loss: 0.6070513054728508\n",
            "Step 4000/50000, Loss: 0.6106110924482345, Test Loss: 0.6049295142292976\n",
            "Step 4100/50000, Loss: 0.6047671067714692, Test Loss: 0.6041082292795181\n",
            "Step 4200/50000, Loss: 0.603420569896698, Test Loss: 0.6057098358869553\n",
            "Step 4300/50000, Loss: 0.6009371656179429, Test Loss: 0.6042213067412376\n",
            "Step 4400/50000, Loss: 0.6017094248533249, Test Loss: 0.604745663702488\n",
            "Step 4500/50000, Loss: 0.6014189845323563, Test Loss: 0.6009880006313324\n",
            "Step 4600/50000, Loss: 0.5976266038417816, Test Loss: 0.6004352197051048\n",
            "Step 4700/50000, Loss: 0.6051703834533692, Test Loss: 0.5990288332104683\n",
            "Step 4800/50000, Loss: 0.5987521249055863, Test Loss: 0.5995767191052437\n",
            "Step 4900/50000, Loss: 0.605491629242897, Test Loss: 0.5985585302114487\n",
            "Step 5000/50000, Loss: 0.5989030289649964, Test Loss: 0.5991090014576912\n",
            "Step 5100/50000, Loss: 0.5965026852488517, Test Loss: 0.597358226776123\n",
            "Step 5200/50000, Loss: 0.5994638687372208, Test Loss: 0.5964501574635506\n",
            "Step 5300/50000, Loss: 0.5930718868970871, Test Loss: 0.5972273796796799\n",
            "Step 5400/50000, Loss: 0.5987280166149139, Test Loss: 0.5952593758702278\n",
            "Step 5500/50000, Loss: 0.5906244492530823, Test Loss: 0.5944551900029182\n",
            "Step 5600/50000, Loss: 0.5958823591470719, Test Loss: 0.5944562777876854\n",
            "Step 5700/50000, Loss: 0.5881155526638031, Test Loss: 0.5947108417749405\n",
            "Step 5800/50000, Loss: 0.5935657703876496, Test Loss: 0.593703106045723\n",
            "Step 5900/50000, Loss: 0.59330351293087, Test Loss: 0.5939234718680382\n",
            "Step 6000/50000, Loss: 0.5902789396047592, Test Loss: 0.59233009070158\n",
            "Step 6100/50000, Loss: 0.588716431260109, Test Loss: 0.5923418253660202\n",
            "Step 6200/50000, Loss: 0.5928794878721237, Test Loss: 0.5913825184106827\n",
            "Step 6300/50000, Loss: 0.5914193224906922, Test Loss: 0.5902153626084328\n",
            "Step 6400/50000, Loss: 0.5833790856599808, Test Loss: 0.588626816868782\n",
            "Step 6500/50000, Loss: 0.5877368372678756, Test Loss: 0.5877366214990616\n",
            "Step 6600/50000, Loss: 0.5860869419574738, Test Loss: 0.5863989442586899\n",
            "Step 6700/50000, Loss: 0.589739373922348, Test Loss: 0.5867156386375427\n",
            "Step 6800/50000, Loss: 0.584526817202568, Test Loss: 0.5867301002144814\n",
            "Step 6900/50000, Loss: 0.5822766584157943, Test Loss: 0.5880885794758797\n",
            "Step 7000/50000, Loss: 0.577407209277153, Test Loss: 0.5861862972378731\n",
            "Step 7100/50000, Loss: 0.5744632133841514, Test Loss: 0.5860717371106148\n",
            "Step 7200/50000, Loss: 0.5751043623685836, Test Loss: 0.5863938108086586\n",
            "Step 7300/50000, Loss: 0.5748037856817245, Test Loss: 0.5855845585465431\n",
            "Step 7400/50000, Loss: 0.572316969037056, Test Loss: 0.5851596668362617\n",
            "Step 7500/50000, Loss: 0.5747124922275543, Test Loss: 0.5847618877887726\n",
            "Step 7600/50000, Loss: 0.5798290985822677, Test Loss: 0.5830075964331627\n",
            "Step 7700/50000, Loss: 0.5753655230998993, Test Loss: 0.5843382328748703\n",
            "Step 7800/50000, Loss: 0.5709194296598434, Test Loss: 0.582453265786171\n",
            "Step 7900/50000, Loss: 0.5761319142580033, Test Loss: 0.5819704532623291\n",
            "Step 8000/50000, Loss: 0.5711049169301987, Test Loss: 0.5836983248591423\n",
            "Step 8100/50000, Loss: 0.5715294700860977, Test Loss: 0.5828280597925186\n",
            "Step 8200/50000, Loss: 0.5724949318170548, Test Loss: 0.5819578990340233\n",
            "Step 8300/50000, Loss: 0.5741658109426498, Test Loss: 0.581995002925396\n",
            "Step 8400/50000, Loss: 0.5692359441518784, Test Loss: 0.5819196477532387\n",
            "Step 8500/50000, Loss: 0.5626497274637222, Test Loss: 0.5805854797363281\n",
            "Step 8600/50000, Loss: 0.5717644435167313, Test Loss: 0.5816004872322083\n",
            "Step 8700/50000, Loss: 0.5690776839852333, Test Loss: 0.5807487517595291\n",
            "Step 8800/50000, Loss: 0.567734204530716, Test Loss: 0.5793915018439293\n",
            "Step 8900/50000, Loss: 0.5671832323074341, Test Loss: 0.5800510048866272\n",
            "Step 9000/50000, Loss: 0.5706066074967384, Test Loss: 0.5805618986487389\n",
            "Step 9100/50000, Loss: 0.5674847972393036, Test Loss: 0.5779681727290154\n",
            "Step 9200/50000, Loss: 0.5652343165874482, Test Loss: 0.57679333537817\n",
            "Step 9300/50000, Loss: 0.5636638420820236, Test Loss: 0.578231044113636\n",
            "Step 9400/50000, Loss: 0.5642386451363564, Test Loss: 0.5772436782717705\n",
            "Step 9500/50000, Loss: 0.5624971750378609, Test Loss: 0.578190840780735\n",
            "Step 9600/50000, Loss: 0.5634752601385117, Test Loss: 0.5764676332473755\n",
            "Step 9700/50000, Loss: 0.5636484289169311, Test Loss: 0.5761144608259201\n",
            "Step 9800/50000, Loss: 0.5656767743825912, Test Loss: 0.5751572251319885\n",
            "Step 9900/50000, Loss: 0.5646402412652969, Test Loss: 0.576744981110096\n",
            "Step 10000/50000, Loss: 0.5584480208158493, Test Loss: 0.5752606242895126\n",
            "Step 10100/50000, Loss: 0.5675463330745697, Test Loss: 0.5746448859572411\n",
            "Step 10200/50000, Loss: 0.5639113080501557, Test Loss: 0.5766838267445564\n",
            "Step 10300/50000, Loss: 0.565152844786644, Test Loss: 0.575394481420517\n",
            "Step 10400/50000, Loss: 0.5642486420273781, Test Loss: 0.57430499792099\n",
            "Step 10500/50000, Loss: 0.5635545629262925, Test Loss: 0.5738035812973976\n",
            "Step 10600/50000, Loss: 0.560040000975132, Test Loss: 0.5750302597880363\n",
            "Step 10700/50000, Loss: 0.5596969667077064, Test Loss: 0.57220808416605\n",
            "Step 10800/50000, Loss: 0.5574226033687592, Test Loss: 0.5730143934488297\n",
            "Step 10900/50000, Loss: 0.5647035917639732, Test Loss: 0.5714382529258728\n",
            "Step 11000/50000, Loss: 0.5587136927247047, Test Loss: 0.5711110085248947\n",
            "Step 11100/50000, Loss: 0.5587834417819977, Test Loss: 0.5726886764168739\n",
            "Step 11200/50000, Loss: 0.5549559888243675, Test Loss: 0.5714552626013756\n",
            "Step 11300/50000, Loss: 0.5565311759710312, Test Loss: 0.5713993534445763\n",
            "Step 11400/50000, Loss: 0.5563825571537018, Test Loss: 0.5699047222733498\n",
            "Step 11500/50000, Loss: 0.5576359283924103, Test Loss: 0.5679070949554443\n",
            "Step 11600/50000, Loss: 0.5607616451382637, Test Loss: 0.5699220821261406\n",
            "Step 11700/50000, Loss: 0.5581514132022858, Test Loss: 0.5714012905955315\n",
            "Step 11800/50000, Loss: 0.5621607759594918, Test Loss: 0.5677169635891914\n",
            "Step 11900/50000, Loss: 0.55369149684906, Test Loss: 0.5713604018092155\n",
            "Step 12000/50000, Loss: 0.5588549131155014, Test Loss: 0.5696694105863571\n",
            "Step 12100/50000, Loss: 0.5542031788825988, Test Loss: 0.5705981105566025\n",
            "Step 12200/50000, Loss: 0.5551470720767975, Test Loss: 0.568854846060276\n",
            "Step 12300/50000, Loss: 0.5562813901901245, Test Loss: 0.5683741867542267\n",
            "Step 12400/50000, Loss: 0.5507404124736786, Test Loss: 0.5689040198922157\n",
            "Step 12500/50000, Loss: 0.5562013146281243, Test Loss: 0.5684254467487335\n",
            "Step 12600/50000, Loss: 0.5528185081481933, Test Loss: 0.5685967206954956\n",
            "Step 12700/50000, Loss: 0.5524562358856201, Test Loss: 0.5687995627522469\n",
            "Step 12800/50000, Loss: 0.5539699363708496, Test Loss: 0.5666004568338394\n",
            "Step 12900/50000, Loss: 0.5513373532891274, Test Loss: 0.5677912011742592\n",
            "Step 13000/50000, Loss: 0.5518678990006447, Test Loss: 0.5673673525452614\n",
            "Step 13100/50000, Loss: 0.5557357114553452, Test Loss: 0.5658706501126289\n",
            "Step 13200/50000, Loss: 0.5515711814165115, Test Loss: 0.565628170967102\n",
            "Step 13300/50000, Loss: 0.550505211353302, Test Loss: 0.5668040290474892\n",
            "Step 13400/50000, Loss: 0.5481249782443046, Test Loss: 0.5661885440349579\n",
            "Step 13500/50000, Loss: 0.5528215989470482, Test Loss: 0.5625173598527908\n",
            "Step 13600/50000, Loss: 0.5528467190265656, Test Loss: 0.5631937682628632\n",
            "Step 13700/50000, Loss: 0.5483042612671852, Test Loss: 0.5636199936270714\n",
            "Step 13800/50000, Loss: 0.5488942724466324, Test Loss: 0.565242625772953\n",
            "Step 13900/50000, Loss: 0.542830813229084, Test Loss: 0.5647081807255745\n",
            "Step 14000/50000, Loss: 0.544424757361412, Test Loss: 0.5652362257242203\n",
            "Step 14100/50000, Loss: 0.5432074779272079, Test Loss: 0.5628474578261375\n",
            "Step 14200/50000, Loss: 0.5443048387765884, Test Loss: 0.5640399232506752\n",
            "Step 14300/50000, Loss: 0.5409858584403991, Test Loss: 0.5641464665532112\n",
            "Step 14400/50000, Loss: 0.5432823747396469, Test Loss: 0.5631680116057396\n",
            "Step 14500/50000, Loss: 0.5496816590428353, Test Loss: 0.56195417791605\n",
            "Step 14600/50000, Loss: 0.5455321884155273, Test Loss: 0.5633310079574585\n",
            "Step 14700/50000, Loss: 0.5394514006376266, Test Loss: 0.5619406774640083\n",
            "Step 14800/50000, Loss: 0.5472288265824318, Test Loss: 0.5640913099050522\n",
            "Step 14900/50000, Loss: 0.5410517767071724, Test Loss: 0.5626183301210403\n",
            "Step 15000/50000, Loss: 0.5427042940258979, Test Loss: 0.5630702450871468\n",
            "Step 15100/50000, Loss: 0.5432297945022583, Test Loss: 0.5624691918492317\n",
            "Step 15200/50000, Loss: 0.5445645186305046, Test Loss: 0.5646524205803871\n",
            "Step 15300/50000, Loss: 0.5373612320423127, Test Loss: 0.5627492591738701\n",
            "Step 15400/50000, Loss: 0.5364279320836067, Test Loss: 0.5618307366967201\n",
            "Step 15500/50000, Loss: 0.5434803596138954, Test Loss: 0.5621660053730011\n",
            "Step 15600/50000, Loss: 0.5427698168158531, Test Loss: 0.5622566640377045\n",
            "Step 15700/50000, Loss: 0.539301550090313, Test Loss: 0.5627597570419312\n",
            "Step 15800/50000, Loss: 0.5386711174249649, Test Loss: 0.562005452811718\n",
            "Step 15900/50000, Loss: 0.5415481841564178, Test Loss: 0.5643602684140205\n",
            "Step 16000/50000, Loss: 0.5412552654743195, Test Loss: 0.5606033578515053\n",
            "Step 16100/50000, Loss: 0.5379377207159997, Test Loss: 0.5603309124708176\n",
            "Step 16200/50000, Loss: 0.5377152219414711, Test Loss: 0.560099333524704\n",
            "Step 16300/50000, Loss: 0.5371793606877326, Test Loss: 0.5596804171800613\n",
            "Step 16400/50000, Loss: 0.5350770393013954, Test Loss: 0.5591178834438324\n",
            "Step 16500/50000, Loss: 0.5388319265842437, Test Loss: 0.5582670271396637\n",
            "Step 16600/50000, Loss: 0.5378912803530693, Test Loss: 0.5595279708504677\n",
            "Step 16700/50000, Loss: 0.5402527329325676, Test Loss: 0.557677835226059\n",
            "Step 16800/50000, Loss: 0.5356794854998589, Test Loss: 0.5578664466738701\n",
            "Step 16900/50000, Loss: 0.5346369329094887, Test Loss: 0.5596128329634666\n",
            "Step 17000/50000, Loss: 0.54222346752882, Test Loss: 0.5585969537496567\n",
            "Step 17100/50000, Loss: 0.539298540353775, Test Loss: 0.5581124350428581\n",
            "Step 17200/50000, Loss: 0.5376360580325127, Test Loss: 0.5601217895746231\n",
            "Step 17300/50000, Loss: 0.5395399758219719, Test Loss: 0.559946745634079\n",
            "Step 17400/50000, Loss: 0.5386925312876701, Test Loss: 0.5598364993929863\n",
            "Step 17500/50000, Loss: 0.5348520314693451, Test Loss: 0.5595741048455238\n",
            "Step 17600/50000, Loss: 0.5292650026082992, Test Loss: 0.5579413771629333\n",
            "Step 17700/50000, Loss: 0.5398921331763268, Test Loss: 0.5583313331007957\n",
            "Step 17800/50000, Loss: 0.5357465851306915, Test Loss: 0.5585442036390305\n",
            "Step 17900/50000, Loss: 0.5327663189172744, Test Loss: 0.5510242432355881\n",
            "Step 18000/50000, Loss: 0.5271280682086945, Test Loss: 0.5502513647079468\n",
            "Step 18100/50000, Loss: 0.5259345364570618, Test Loss: 0.5491451099514961\n",
            "Step 18200/50000, Loss: 0.5271320742368698, Test Loss: 0.5489896535873413\n",
            "Step 18300/50000, Loss: 0.5238709262013436, Test Loss: 0.5480611175298691\n",
            "Step 18400/50000, Loss: 0.5273246216773987, Test Loss: 0.5474121198058128\n",
            "Step 18500/50000, Loss: 0.5263362589478493, Test Loss: 0.5471702739596367\n",
            "Step 18600/50000, Loss: 0.5279832828044891, Test Loss: 0.547274611890316\n",
            "Step 18700/50000, Loss: 0.5278452935814858, Test Loss: 0.5464454367756844\n",
            "Step 18800/50000, Loss: 0.5205520689487457, Test Loss: 0.5468438267707825\n",
            "Step 18900/50000, Loss: 0.525918051302433, Test Loss: 0.5463645756244659\n",
            "Step 19000/50000, Loss: 0.5218250566720962, Test Loss: 0.5465811118483543\n",
            "Step 19100/50000, Loss: 0.5228285375237465, Test Loss: 0.545909933745861\n",
            "Step 19200/50000, Loss: 0.524276040494442, Test Loss: 0.5463402941823006\n",
            "Step 19300/50000, Loss: 0.5219045123457908, Test Loss: 0.5459508374333382\n",
            "Step 19400/50000, Loss: 0.5209808105230331, Test Loss: 0.5453898906707764\n",
            "Step 19500/50000, Loss: 0.5214326724410057, Test Loss: 0.5455401465296745\n",
            "Step 19600/50000, Loss: 0.5179504960775375, Test Loss: 0.5457349643111229\n",
            "Step 19700/50000, Loss: 0.5227244329452515, Test Loss: 0.5457464009523392\n",
            "Step 19800/50000, Loss: 0.5178977763652801, Test Loss: 0.5452375411987305\n",
            "Step 19900/50000, Loss: 0.5201017183065414, Test Loss: 0.5451173186302185\n",
            "Step 20000/50000, Loss: 0.5228851708769798, Test Loss: 0.5447744950652122\n",
            "Step 20100/50000, Loss: 0.5185097491741181, Test Loss: 0.5446213185787201\n",
            "Step 20200/50000, Loss: 0.5180230981111527, Test Loss: 0.5443410575389862\n",
            "Step 20300/50000, Loss: 0.517382781803608, Test Loss: 0.5442386493086815\n",
            "Step 20400/50000, Loss: 0.5202216759324074, Test Loss: 0.5437059476971626\n",
            "Step 20500/50000, Loss: 0.5196962520480156, Test Loss: 0.5435793101787567\n",
            "Step 20600/50000, Loss: 0.5123674601316452, Test Loss: 0.5442237555980682\n",
            "Step 20700/50000, Loss: 0.5200866732001305, Test Loss: 0.5439315959811211\n",
            "Step 20800/50000, Loss: 0.5091942158341408, Test Loss: 0.543663926422596\n",
            "Step 20900/50000, Loss: 0.5112863254547119, Test Loss: 0.5440888926386833\n",
            "Step 21000/50000, Loss: 0.5135010626912117, Test Loss: 0.5429307892918587\n",
            "Step 21100/50000, Loss: 0.5110966017842293, Test Loss: 0.5440293699502945\n",
            "Step 21200/50000, Loss: 0.5097229021787644, Test Loss: 0.5440555810928345\n",
            "Step 21300/50000, Loss: 0.5112406870722771, Test Loss: 0.5441801026463509\n",
            "Step 21400/50000, Loss: 0.5192759984731674, Test Loss: 0.543305054306984\n",
            "Step 21500/50000, Loss: 0.5122659048438072, Test Loss: 0.5429277196526527\n",
            "Step 21600/50000, Loss: 0.5052849891781807, Test Loss: 0.5427471697330475\n",
            "Step 21700/50000, Loss: 0.5136335334181785, Test Loss: 0.5432612597942352\n",
            "Step 21800/50000, Loss: 0.5077440929412842, Test Loss: 0.5436762794852257\n",
            "Step 21900/50000, Loss: 0.509901857972145, Test Loss: 0.5442083030939102\n",
            "Step 22000/50000, Loss: 0.5127878683805466, Test Loss: 0.5434536635875702\n",
            "Step 22100/50000, Loss: 0.5088658592104912, Test Loss: 0.5432475730776787\n",
            "Step 22200/50000, Loss: 0.503545526266098, Test Loss: 0.5430518910288811\n",
            "Step 22300/50000, Loss: 0.509440864622593, Test Loss: 0.5440312474966049\n",
            "Step 22400/50000, Loss: 0.504312878549099, Test Loss: 0.5440322384238243\n",
            "Step 22500/50000, Loss: 0.5105202877521515, Test Loss: 0.5433548018336296\n",
            "Step 22600/50000, Loss: 0.5068865966796875, Test Loss: 0.543488435447216\n",
            "Step 22700/50000, Loss: 0.5061589232087136, Test Loss: 0.543672688305378\n",
            "Step 22800/50000, Loss: 0.5050221633911133, Test Loss: 0.54234429448843\n",
            "Step 22900/50000, Loss: 0.5060675710439682, Test Loss: 0.5421250462532043\n",
            "Step 23000/50000, Loss: 0.5028709018230438, Test Loss: 0.5417612642049789\n",
            "Step 23100/50000, Loss: 0.5007539334893226, Test Loss: 0.5417554974555969\n",
            "Step 23200/50000, Loss: 0.5032428222894668, Test Loss: 0.5419873222708702\n",
            "Step 23300/50000, Loss: 0.4976426234841347, Test Loss: 0.5417612642049789\n",
            "Step 23400/50000, Loss: 0.5037221732735634, Test Loss: 0.5416203439235687\n",
            "Step 23500/50000, Loss: 0.5026455554366112, Test Loss: 0.541393369436264\n",
            "Step 23600/50000, Loss: 0.5041168466210365, Test Loss: 0.5412039160728455\n",
            "Step 23700/50000, Loss: 0.499170041680336, Test Loss: 0.5414468348026276\n",
            "Step 23800/50000, Loss: 0.49886377930641174, Test Loss: 0.5415316671133041\n",
            "Step 23900/50000, Loss: 0.5008612561225891, Test Loss: 0.5415767431259155\n",
            "Step 24000/50000, Loss: 0.5027919802069664, Test Loss: 0.541602335870266\n",
            "Step 24100/50000, Loss: 0.4981607446074486, Test Loss: 0.5416673794388771\n",
            "Step 24200/50000, Loss: 0.5020068144798279, Test Loss: 0.5414736419916153\n",
            "Step 24300/50000, Loss: 0.49790079534053805, Test Loss: 0.5420249551534653\n",
            "Step 24400/50000, Loss: 0.4945983332395554, Test Loss: 0.5420288145542145\n",
            "Step 24500/50000, Loss: 0.4918954423069954, Test Loss: 0.5416672825813293\n",
            "Step 24600/50000, Loss: 0.4987477654218674, Test Loss: 0.5413290336728096\n",
            "Step 24700/50000, Loss: 0.4991921219229698, Test Loss: 0.5409770160913467\n",
            "Step 24800/50000, Loss: 0.5160313996672631, Test Loss: 0.5407248362898827\n",
            "Step 24900/50000, Loss: 0.5089896723628045, Test Loss: 0.5407268553972244\n",
            "Step 25000/50000, Loss: 0.5133795315027236, Test Loss: 0.540549747645855\n",
            "Step 25100/50000, Loss: 0.5142266032099724, Test Loss: 0.5403367578983307\n",
            "Step 25200/50000, Loss: 0.5106305554509163, Test Loss: 0.5404777601361275\n",
            "Step 25300/50000, Loss: 0.514853127002716, Test Loss: 0.5402576178312302\n",
            "Step 25400/50000, Loss: 0.5116986194252968, Test Loss: 0.540235199034214\n",
            "Step 25500/50000, Loss: 0.5178939509391784, Test Loss: 0.5401464626193047\n",
            "Step 25600/50000, Loss: 0.5128523615002633, Test Loss: 0.5403817817568779\n",
            "Step 25700/50000, Loss: 0.5107192620635033, Test Loss: 0.5402729734778404\n",
            "Step 25800/50000, Loss: 0.5151638248562813, Test Loss: 0.540238045156002\n",
            "Step 25900/50000, Loss: 0.5079279285669327, Test Loss: 0.5405196249485016\n",
            "Step 26000/50000, Loss: 0.5137662106752395, Test Loss: 0.5403135642409325\n",
            "Step 26100/50000, Loss: 0.5081922098994255, Test Loss: 0.5401801690459251\n",
            "Step 26200/50000, Loss: 0.5133265718817711, Test Loss: 0.5400106459856033\n",
            "Step 26300/50000, Loss: 0.5071106451749802, Test Loss: 0.5401815176010132\n",
            "Step 26400/50000, Loss: 0.5103961524367332, Test Loss: 0.5401192381978035\n",
            "Step 26500/50000, Loss: 0.5102493834495544, Test Loss: 0.5405233800411224\n",
            "Step 26600/50000, Loss: 0.5107258838415146, Test Loss: 0.540264330804348\n",
            "Step 26700/50000, Loss: 0.5076950311660766, Test Loss: 0.5403445139527321\n",
            "Step 26800/50000, Loss: 0.508688750565052, Test Loss: 0.5403786599636078\n",
            "Step 26900/50000, Loss: 0.5121384325623513, Test Loss: 0.5402709916234016\n",
            "Step 27000/50000, Loss: 0.5072655525803565, Test Loss: 0.5402552932500839\n",
            "Step 27100/50000, Loss: 0.5062821558117867, Test Loss: 0.5403524935245514\n",
            "Step 27200/50000, Loss: 0.5098704776167869, Test Loss: 0.5401039719581604\n",
            "Step 27300/50000, Loss: 0.5111740103363991, Test Loss: 0.5400716066360474\n",
            "Step 27400/50000, Loss: 0.506802773475647, Test Loss: 0.5399189367890358\n",
            "Step 27500/50000, Loss: 0.5019376620650291, Test Loss: 0.5398764163255692\n",
            "Step 27600/50000, Loss: 0.5078087207674981, Test Loss: 0.539827011525631\n",
            "Step 27700/50000, Loss: 0.49983825743198396, Test Loss: 0.5397513061761856\n",
            "Step 27800/50000, Loss: 0.501948290169239, Test Loss: 0.5397870093584061\n",
            "Step 27900/50000, Loss: 0.503223777115345, Test Loss: 0.5396886020898819\n",
            "Step 28000/50000, Loss: 0.4988864180445671, Test Loss: 0.5397669076919556\n",
            "Step 28100/50000, Loss: 0.5000418615341187, Test Loss: 0.539934940636158\n",
            "Step 28200/50000, Loss: 0.5037692028284073, Test Loss: 0.5399342402815819\n",
            "Step 28300/50000, Loss: 0.5065342092514038, Test Loss: 0.5399300828576088\n",
            "Step 28400/50000, Loss: 0.49868389159440996, Test Loss: 0.5400566309690475\n",
            "Step 28500/50000, Loss: 0.49928132712841033, Test Loss: 0.5400507599115372\n",
            "Step 28600/50000, Loss: 0.5010258322954178, Test Loss: 0.5399254709482193\n",
            "Step 28700/50000, Loss: 0.49798482537269595, Test Loss: 0.540141187608242\n",
            "Step 28800/50000, Loss: 0.5000904050469398, Test Loss: 0.5402135774493217\n",
            "Step 28900/50000, Loss: 0.5013855120539665, Test Loss: 0.5401042699813843\n",
            "Step 29000/50000, Loss: 0.4986867892742157, Test Loss: 0.5401381179690361\n",
            "Step 29100/50000, Loss: 0.49214051008224485, Test Loss: 0.5402148738503456\n",
            "Step 29200/50000, Loss: 0.4973500269651413, Test Loss: 0.5402180105447769\n",
            "Step 29300/50000, Loss: 0.4951749128103256, Test Loss: 0.5403081923723221\n",
            "Step 29400/50000, Loss: 0.49519805639982223, Test Loss: 0.5403984859585762\n",
            "Step 29500/50000, Loss: 0.4949836692214012, Test Loss: 0.5403021425008774\n",
            "Step 29600/50000, Loss: 0.49874858170747755, Test Loss: 0.5402622520923615\n",
            "Step 29700/50000, Loss: 0.5020810437202453, Test Loss: 0.5400966182351112\n",
            "Step 29800/50000, Loss: 0.5026862496137618, Test Loss: 0.5399602949619293\n",
            "Step 29900/50000, Loss: 0.5002500921487808, Test Loss: 0.5399456769227982\n",
            "Step 30000/50000, Loss: 0.4972523659467697, Test Loss: 0.5399295464158058\n",
            "Step 30100/50000, Loss: 0.49687450855970383, Test Loss: 0.540132001042366\n",
            "Step 30200/50000, Loss: 0.4967585051059723, Test Loss: 0.5400266796350479\n",
            "Step 30300/50000, Loss: 0.5007252091169357, Test Loss: 0.5401216298341751\n",
            "Step 30400/50000, Loss: 0.49998929440975187, Test Loss: 0.5400102064013481\n",
            "Step 30500/50000, Loss: 0.49902217119932174, Test Loss: 0.5399828031659126\n",
            "Step 30600/50000, Loss: 0.4965375950932503, Test Loss: 0.5401788353919983\n",
            "Step 30700/50000, Loss: 0.49841643542051317, Test Loss: 0.5401521772146225\n",
            "Step 30800/50000, Loss: 0.49873902708292006, Test Loss: 0.5401478558778763\n",
            "Step 30900/50000, Loss: 0.4986496239900589, Test Loss: 0.5402649715542793\n",
            "Step 31000/50000, Loss: 0.4976932209730148, Test Loss: 0.540209136903286\n",
            "Step 31100/50000, Loss: 0.4981874457001686, Test Loss: 0.5402702316641808\n",
            "Step 31200/50000, Loss: 0.4924463939666748, Test Loss: 0.5404428169131279\n",
            "Step 31300/50000, Loss: 0.4926974037289619, Test Loss: 0.5405407398939133\n",
            "Step 31400/50000, Loss: 0.4927178320288658, Test Loss: 0.5404405742883682\n",
            "Step 31500/50000, Loss: 0.4961189603805542, Test Loss: 0.5402893200516701\n",
            "Step 31600/50000, Loss: 0.502119165956974, Test Loss: 0.5400217920541763\n",
            "Step 31700/50000, Loss: 0.5106057554483414, Test Loss: 0.539872981607914\n",
            "Step 31800/50000, Loss: 0.5088860255479812, Test Loss: 0.5397540032863617\n",
            "Step 31900/50000, Loss: 0.5092998299002648, Test Loss: 0.5397043526172638\n",
            "Step 32000/50000, Loss: 0.5100585091114044, Test Loss: 0.5396628379821777\n",
            "Step 32100/50000, Loss: 0.508292889893055, Test Loss: 0.5396415516734123\n",
            "Step 32200/50000, Loss: 0.514079284965992, Test Loss: 0.5395453348755836\n",
            "Step 32300/50000, Loss: 0.5094090312719345, Test Loss: 0.5395669266581535\n",
            "Step 32400/50000, Loss: 0.5140978255867958, Test Loss: 0.5395563840866089\n",
            "Step 32500/50000, Loss: 0.5108561027050018, Test Loss: 0.5395280569791794\n",
            "Step 32600/50000, Loss: 0.508008286356926, Test Loss: 0.5395460426807404\n",
            "Step 32700/50000, Loss: 0.5117560002207756, Test Loss: 0.5395319536328316\n",
            "Step 32800/50000, Loss: 0.5059553095698357, Test Loss: 0.5396299511194229\n",
            "Step 32900/50000, Loss: 0.5118668919801712, Test Loss: 0.5395850613713264\n",
            "Step 33000/50000, Loss: 0.5050633817911148, Test Loss: 0.5395438820123672\n",
            "Step 33100/50000, Loss: 0.5107882875204086, Test Loss: 0.5395044758915901\n",
            "Step 33200/50000, Loss: 0.5029929068684578, Test Loss: 0.5395789965987206\n",
            "Step 33300/50000, Loss: 0.5088842526078224, Test Loss: 0.5396023169159889\n",
            "Step 33400/50000, Loss: 0.5078984281420708, Test Loss: 0.5396974086761475\n",
            "Step 33500/50000, Loss: 0.5050894203782081, Test Loss: 0.5396722480654716\n",
            "Step 33600/50000, Loss: 0.5062958505749703, Test Loss: 0.5396731942892075\n",
            "Step 33700/50000, Loss: 0.5092896395921707, Test Loss: 0.5396324619650841\n",
            "Step 33800/50000, Loss: 0.5085639423131942, Test Loss: 0.5396186858415604\n",
            "Step 33900/50000, Loss: 0.5020746979117393, Test Loss: 0.5396337956190109\n",
            "Step 34000/50000, Loss: 0.5059955576062203, Test Loss: 0.539700098335743\n",
            "Step 34100/50000, Loss: 0.5049130553007126, Test Loss: 0.5396822318434715\n",
            "Step 34200/50000, Loss: 0.5080492258071899, Test Loss: 0.539713479578495\n",
            "Step 34300/50000, Loss: 0.5059011596441269, Test Loss: 0.539629191160202\n",
            "Step 34400/50000, Loss: 0.5042031505703926, Test Loss: 0.5396181717514992\n",
            "Step 34500/50000, Loss: 0.5040303328633309, Test Loss: 0.5395740866661072\n",
            "Step 34600/50000, Loss: 0.5003925260901451, Test Loss: 0.5395526438951492\n",
            "Step 34700/50000, Loss: 0.5008617109060287, Test Loss: 0.5396481305360794\n",
            "Step 34800/50000, Loss: 0.5013332226872445, Test Loss: 0.5395035743713379\n",
            "Step 34900/50000, Loss: 0.4993538862466812, Test Loss: 0.5396741777658463\n",
            "Step 35000/50000, Loss: 0.49984502136707304, Test Loss: 0.5397976487874985\n",
            "Step 35100/50000, Loss: 0.5055657151341438, Test Loss: 0.5397628992795944\n",
            "Step 35200/50000, Loss: 0.5025000894069671, Test Loss: 0.5398406684398651\n",
            "Step 35300/50000, Loss: 0.4979968535900116, Test Loss: 0.5399135574698448\n",
            "Step 35400/50000, Loss: 0.5027144092321396, Test Loss: 0.5398437827825546\n",
            "Step 35500/50000, Loss: 0.49810495495796203, Test Loss: 0.5398181229829788\n",
            "Step 35600/50000, Loss: 0.49869739830493925, Test Loss: 0.539912611246109\n",
            "Step 35700/50000, Loss: 0.4993435364961624, Test Loss: 0.540031224489212\n",
            "Step 35800/50000, Loss: 0.5010734874010087, Test Loss: 0.539951279759407\n",
            "Step 35900/50000, Loss: 0.4961173927783966, Test Loss: 0.5399811267852783\n",
            "Step 36000/50000, Loss: 0.49067836165428164, Test Loss: 0.5400380566716194\n",
            "Step 36100/50000, Loss: 0.4983882322907448, Test Loss: 0.5400507599115372\n",
            "Step 36200/50000, Loss: 0.49570639073848727, Test Loss: 0.5401374697685242\n",
            "Step 36300/50000, Loss: 0.49497527778148653, Test Loss: 0.5402403101325035\n",
            "Step 36400/50000, Loss: 0.4943352746963501, Test Loss: 0.5401946380734444\n",
            "Step 36500/50000, Loss: 0.5013378807902336, Test Loss: 0.5400375798344612\n",
            "Step 36600/50000, Loss: 0.5009453481435776, Test Loss: 0.5398918315768242\n",
            "Step 36700/50000, Loss: 0.49999902069568636, Test Loss: 0.5398625582456589\n",
            "Step 36800/50000, Loss: 0.4986932471394539, Test Loss: 0.5397756099700928\n",
            "Step 36900/50000, Loss: 0.4988508015871048, Test Loss: 0.5398235246539116\n",
            "Step 37000/50000, Loss: 0.49667459309101103, Test Loss: 0.5399428531527519\n",
            "Step 37100/50000, Loss: 0.4973614087700844, Test Loss: 0.5398567691445351\n",
            "Step 37200/50000, Loss: 0.4985528939962387, Test Loss: 0.5399526283144951\n",
            "Step 37300/50000, Loss: 0.5001018795371056, Test Loss: 0.5398374199867249\n",
            "Step 37400/50000, Loss: 0.4983680948615074, Test Loss: 0.5398883894085884\n",
            "Step 37500/50000, Loss: 0.49329759150743485, Test Loss: 0.5400660261511803\n",
            "Step 37600/50000, Loss: 0.5014468815922737, Test Loss: 0.5399715825915337\n",
            "Step 37700/50000, Loss: 0.49771746158599856, Test Loss: 0.5399486199021339\n",
            "Step 37800/50000, Loss: 0.4979658380150795, Test Loss: 0.5401404649019241\n",
            "Step 37900/50000, Loss: 0.4978672149777412, Test Loss: 0.5400984436273575\n",
            "Step 38000/50000, Loss: 0.4970002031326294, Test Loss: 0.5400849655270576\n",
            "Step 38100/50000, Loss: 0.49319078475236894, Test Loss: 0.5402737855911255\n",
            "Step 38200/50000, Loss: 0.4919032683968544, Test Loss: 0.540255218744278\n",
            "Step 38300/50000, Loss: 0.49013256341218947, Test Loss: 0.5403067767620087\n",
            "Step 38400/50000, Loss: 0.49624019861221313, Test Loss: 0.5401993095874786\n",
            "Step 38500/50000, Loss: 0.5070158150792122, Test Loss: 0.539745643734932\n",
            "Step 38600/50000, Loss: 0.5106859561800957, Test Loss: 0.5397117808461189\n",
            "Step 38700/50000, Loss: 0.5072772273421288, Test Loss: 0.5395833551883698\n",
            "Step 38800/50000, Loss: 0.5087108266353607, Test Loss: 0.5395811051130295\n",
            "Step 38900/50000, Loss: 0.5086913874745369, Test Loss: 0.539511114358902\n",
            "Step 39000/50000, Loss: 0.5101708760857582, Test Loss: 0.5394201874732971\n",
            "Step 39100/50000, Loss: 0.5126127651333809, Test Loss: 0.5393865928053856\n",
            "Step 39200/50000, Loss: 0.5107463571429253, Test Loss: 0.5393551588058472\n",
            "Step 39300/50000, Loss: 0.5136496108770371, Test Loss: 0.5393457487225533\n",
            "Step 39400/50000, Loss: 0.5069820827245712, Test Loss: 0.539389118552208\n",
            "Step 39500/50000, Loss: 0.5111293032765388, Test Loss: 0.5393685102462769\n",
            "Step 39600/50000, Loss: 0.5073767012357712, Test Loss: 0.5394056066870689\n",
            "Step 39700/50000, Loss: 0.5081001633405685, Test Loss: 0.5394421964883804\n",
            "Step 39800/50000, Loss: 0.5096010833978653, Test Loss: 0.539382092654705\n",
            "Step 39900/50000, Loss: 0.5044914534687996, Test Loss: 0.5393995344638824\n",
            "Step 40000/50000, Loss: 0.5096197336912155, Test Loss: 0.5393679961562157\n",
            "Step 40100/50000, Loss: 0.5065052941441536, Test Loss: 0.5393980294466019\n",
            "Step 40200/50000, Loss: 0.5058723211288452, Test Loss: 0.5394647270441055\n",
            "Step 40300/50000, Loss: 0.5071479165554047, Test Loss: 0.5395227521657944\n",
            "Step 40400/50000, Loss: 0.5047757157683372, Test Loss: 0.5394424721598625\n",
            "Step 40500/50000, Loss: 0.5058568915724755, Test Loss: 0.5395368114113808\n",
            "Step 40600/50000, Loss: 0.5097119781374931, Test Loss: 0.5394527763128281\n",
            "Step 40700/50000, Loss: 0.505585141479969, Test Loss: 0.5394986569881439\n",
            "Step 40800/50000, Loss: 0.505511117875576, Test Loss: 0.5394658669829369\n",
            "Step 40900/50000, Loss: 0.5023908773064614, Test Loss: 0.5395566001534462\n",
            "Step 41000/50000, Loss: 0.506652555167675, Test Loss: 0.5394731014966965\n",
            "Step 41100/50000, Loss: 0.507455221414566, Test Loss: 0.5395409390330315\n",
            "Step 41200/50000, Loss: 0.5043680673837662, Test Loss: 0.5394297987222672\n",
            "Step 41300/50000, Loss: 0.5050949704647064, Test Loss: 0.5395004898309708\n",
            "Step 41400/50000, Loss: 0.501577168405056, Test Loss: 0.5394115447998047\n",
            "Step 41500/50000, Loss: 0.5013879150152206, Test Loss: 0.5393913984298706\n",
            "Step 41600/50000, Loss: 0.5003279143571854, Test Loss: 0.5394631996750832\n",
            "Step 41700/50000, Loss: 0.5016007670760154, Test Loss: 0.5393634587526321\n",
            "Step 41800/50000, Loss: 0.49823493242263794, Test Loss: 0.5395021811127663\n",
            "Step 41900/50000, Loss: 0.49926349729299546, Test Loss: 0.5396210700273514\n",
            "Step 42000/50000, Loss: 0.5054442447423935, Test Loss: 0.5396415814757347\n",
            "Step 42100/50000, Loss: 0.50202587723732, Test Loss: 0.539661668241024\n",
            "Step 42200/50000, Loss: 0.4960733976960182, Test Loss: 0.5397117212414742\n",
            "Step 42300/50000, Loss: 0.5030219155550003, Test Loss: 0.5396576225757599\n",
            "Step 42400/50000, Loss: 0.4974089801311493, Test Loss: 0.5397517904639244\n",
            "Step 42500/50000, Loss: 0.49898953795433043, Test Loss: 0.5396833717823029\n",
            "Step 42600/50000, Loss: 0.4989222511649132, Test Loss: 0.5398036763072014\n",
            "Step 42700/50000, Loss: 0.5000161120295524, Test Loss: 0.5398104265332222\n",
            "Step 42800/50000, Loss: 0.4932199516892433, Test Loss: 0.5398836359381676\n",
            "Step 42900/50000, Loss: 0.4919256231188774, Test Loss: 0.5398688986897469\n",
            "Step 43000/50000, Loss: 0.4985833743214607, Test Loss: 0.5398799479007721\n",
            "Step 43100/50000, Loss: 0.4971668937802315, Test Loss: 0.5400027558207512\n",
            "Step 43200/50000, Loss: 0.49428201764822005, Test Loss: 0.5399693325161934\n",
            "Step 43300/50000, Loss: 0.4935976222157478, Test Loss: 0.540062665939331\n",
            "Step 43400/50000, Loss: 0.5008321458101272, Test Loss: 0.5398197025060654\n",
            "Step 43500/50000, Loss: 0.5021645167469978, Test Loss: 0.5396685376763344\n",
            "Step 43600/50000, Loss: 0.49895587891340254, Test Loss: 0.53972277790308\n",
            "Step 43700/50000, Loss: 0.49847632586956026, Test Loss: 0.5396436974406242\n",
            "Step 43800/50000, Loss: 0.4981184619665146, Test Loss: 0.5397310703992844\n",
            "Step 43900/50000, Loss: 0.49537402898073196, Test Loss: 0.539734773337841\n",
            "Step 44000/50000, Loss: 0.498755077123642, Test Loss: 0.5396531820297241\n",
            "Step 44100/50000, Loss: 0.4986911591887474, Test Loss: 0.5397176146507263\n",
            "Step 44200/50000, Loss: 0.5000812029838562, Test Loss: 0.5396725684404373\n",
            "Step 44300/50000, Loss: 0.49518556147813797, Test Loss: 0.5398207157850266\n",
            "Step 44400/50000, Loss: 0.49464826852083205, Test Loss: 0.5398502722382545\n",
            "Step 44500/50000, Loss: 0.5006935778260231, Test Loss: 0.5397677645087242\n",
            "Step 44600/50000, Loss: 0.49818256229162217, Test Loss: 0.5398132503032684\n",
            "Step 44700/50000, Loss: 0.49620632976293566, Test Loss: 0.5399656072258949\n",
            "Step 44800/50000, Loss: 0.49781897008419035, Test Loss: 0.5399337857961655\n",
            "Step 44900/50000, Loss: 0.49707835257053373, Test Loss: 0.5399251356720924\n",
            "Step 45000/50000, Loss: 0.49177643567323687, Test Loss: 0.5400127246975899\n",
            "Step 45100/50000, Loss: 0.4867770627140999, Test Loss: 0.5401008650660515\n",
            "Step 45200/50000, Loss: 0.49645036190748215, Test Loss: 0.5400954708456993\n",
            "Step 45300/50000, Loss: 0.4933033186197281, Test Loss: 0.5400835126638412\n",
            "Step 45400/50000, Loss: 0.5118939998745918, Test Loss: 0.5395145788788795\n",
            "Step 45500/50000, Loss: 0.5083662039041519, Test Loss: 0.5394980236887932\n",
            "Step 45600/50000, Loss: 0.5080907043814659, Test Loss: 0.5394219979643822\n",
            "Step 45700/50000, Loss: 0.509898764193058, Test Loss: 0.5393904894590378\n",
            "Step 45800/50000, Loss: 0.5071603581309319, Test Loss: 0.5393557548522949\n",
            "Step 45900/50000, Loss: 0.5108535829186439, Test Loss: 0.5392103716731071\n",
            "Step 46000/50000, Loss: 0.5099755051732063, Test Loss: 0.5392666682600975\n",
            "Step 46100/50000, Loss: 0.5120785105228424, Test Loss: 0.5391957387328148\n",
            "Step 46200/50000, Loss: 0.5120513769984245, Test Loss: 0.5391103476285934\n",
            "Step 46300/50000, Loss: 0.5055394196510314, Test Loss: 0.539196103811264\n",
            "Step 46400/50000, Loss: 0.5103425335884094, Test Loss: 0.5391378477215767\n",
            "Step 46500/50000, Loss: 0.5065678125619888, Test Loss: 0.5392791852355003\n",
            "Step 46600/50000, Loss: 0.507869057059288, Test Loss: 0.5393331274390221\n",
            "Step 46700/50000, Loss: 0.5094516175985336, Test Loss: 0.5392355620861053\n",
            "Step 46800/50000, Loss: 0.5073900875449181, Test Loss: 0.5392070561647415\n",
            "Step 46900/50000, Loss: 0.5064733734726906, Test Loss: 0.5392192676663399\n",
            "Step 47000/50000, Loss: 0.5072105729579925, Test Loss: 0.5391889214515686\n",
            "Step 47100/50000, Loss: 0.5037602409720421, Test Loss: 0.539274089038372\n",
            "Step 47200/50000, Loss: 0.5082229962944984, Test Loss: 0.5393294245004654\n",
            "Step 47300/50000, Loss: 0.5041952931880951, Test Loss: 0.5392620787024498\n",
            "Step 47400/50000, Loss: 0.5062178316712379, Test Loss: 0.5393047109246254\n",
            "Step 47500/50000, Loss: 0.5090050908923149, Test Loss: 0.539303220808506\n",
            "Step 47600/50000, Loss: 0.5047084537148475, Test Loss: 0.5393324419856071\n",
            "Step 47700/50000, Loss: 0.5045905429124832, Test Loss: 0.5393098443746567\n",
            "Step 47800/50000, Loss: 0.5039082843065262, Test Loss: 0.5393583551049232\n",
            "Step 47900/50000, Loss: 0.5065311944484711, Test Loss: 0.5392666757106781\n",
            "Step 48000/50000, Loss: 0.5071438843011856, Test Loss: 0.5393249616026878\n",
            "Step 48100/50000, Loss: 0.5004611688852311, Test Loss: 0.5393214821815491\n",
            "Step 48200/50000, Loss: 0.5083751136064529, Test Loss: 0.5392718464136124\n",
            "Step 48300/50000, Loss: 0.4980458009243012, Test Loss: 0.539208821952343\n",
            "Step 48400/50000, Loss: 0.499648708999157, Test Loss: 0.5392308905720711\n",
            "Step 48500/50000, Loss: 0.5020219036936759, Test Loss: 0.539259284734726\n",
            "Step 48600/50000, Loss: 0.49979176551103593, Test Loss: 0.5391851142048836\n",
            "Step 48700/50000, Loss: 0.49803889751434327, Test Loss: 0.5393860414624214\n",
            "Step 48800/50000, Loss: 0.49939071893692016, Test Loss: 0.5394993796944618\n",
            "Step 48900/50000, Loss: 0.5076718246936798, Test Loss: 0.5394344180822372\n",
            "Step 49000/50000, Loss: 0.5004595616459846, Test Loss: 0.5394701510667801\n",
            "Step 49100/50000, Loss: 0.4938800597190857, Test Loss: 0.5395425483584404\n",
            "Step 49200/50000, Loss: 0.502016199529171, Test Loss: 0.5394943058490753\n",
            "Step 49300/50000, Loss: 0.49617588609457014, Test Loss: 0.5396153926849365\n",
            "Step 49400/50000, Loss: 0.49824530065059663, Test Loss: 0.5396235510706902\n",
            "Step 49500/50000, Loss: 0.5010240325331687, Test Loss: 0.5395853817462921\n",
            "Step 49600/50000, Loss: 0.49718677312135695, Test Loss: 0.5396591052412987\n",
            "Step 49700/50000, Loss: 0.49210875332355497, Test Loss: 0.5397243052721024\n",
            "Step 49800/50000, Loss: 0.49780592769384385, Test Loss: 0.5396812334656715\n",
            "Step 49900/50000, Loss: 0.4927149027585983, Test Loss: 0.5397619679570198\n",
            "Step 50000/50000, Loss: 0.4986410740017891, Test Loss: 0.5398522838950157\n"
          ]
        }
      ],
      "source": [
        "# Example config:\n",
        "batch_size = 64\n",
        "sequence_len = 128\n",
        "num_steps = 50000\n",
        "accumulation_steps = 100\n",
        "\n",
        "# Reload the train and test datasets\n",
        "train_ds = load_dataset(\"parquet\", data_files=\"fact_qa_train.parquet\", split=\"train\")\n",
        "test_ds = load_dataset(\"parquet\", data_files=\"fact_qa_test.parquet\", split=\"train\")\n",
        "\n",
        "# Convert dataset to PyTorch format\n",
        "train_ds.set_format(\"torch\", columns=[\"input\", \"target\"])\n",
        "test_ds.set_format(\"torch\", columns=[\"input\", \"target\"])\n",
        "\n",
        "# Create DataLoaders for training and testing\n",
        "train_dataloader = cycle(DataLoader(train_ds, batch_size=batch_size, shuffle=False))\n",
        "test_dataloader = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "\n",
        "use_existing_model = os.path.exists(\"./sft_final.pth\")\n",
        "# Check if pre-trained model exists\n",
        "if use_existing_model:\n",
        "    model = torch.load(\"./sft_final.pth\")\n",
        "    print(\"Loaded fine tuned model from ./sft_final.pth, skipping training loop.\")\n",
        "\n",
        "else:\n",
        "    # For SFT we start with the pretrained model\n",
        "    model = torch.load(\"./pretrain_final.pth\")\n",
        "    \n",
        "    # Define the optimizer\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
        "\n",
        "\n",
        "    # Scheduler with dynamic step size\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',factor=0.2, patience=10, min_lr=5e-6, threshold=1e-4)\n",
        "\n",
        "\n",
        "    # Training loop\n",
        "    losses = []\n",
        "    test_losses = []\n",
        "    accumulator = 0\n",
        "    accumulator_loss = 0\n",
        "    for i in range(num_steps):\n",
        "        model.train()\n",
        "        example = next(train_dataloader)\n",
        "        train_input = example[\"input\"].to(device)\n",
        "        train_target = example[\"target\"].to(device)\n",
        "        logits = model(train_input)\n",
        "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), train_target.view(-1))\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        # Update weights\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        accumulator += 1\n",
        "        accumulator_loss += loss.item()\n",
        "\n",
        "        \n",
        "\n",
        "        if accumulator >= accumulation_steps:\n",
        "            losses.append(accumulator_loss / accumulation_steps)\n",
        "            accumulator = 0\n",
        "            accumulator_loss = 0\n",
        "            model.eval()\n",
        "            test_loss = 0\n",
        "            test_accumulator = 0\n",
        "            with torch.no_grad():\n",
        "                for test_example in test_dataloader:\n",
        "                    test_input = test_example[\"input\"].to(device)\n",
        "                    test_target = test_example[\"target\"].to(device)\n",
        "                    test_logits = model(test_input)\n",
        "                    test_loss += F.cross_entropy(test_logits.view(-1, test_logits.size(-1)), test_target.view(-1)).item()\n",
        "                    test_accumulator += 1\n",
        "                test_losses.append(test_loss / test_accumulator)\n",
        "                print(f\"Step {i+1}/{num_steps}, Loss: {losses[-1]}, Test Loss: {test_losses[-1]}\")\n",
        "                test_dataloader = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
        "                scheduler.step(test_losses[-1])\n",
        "\n",
        "        if i+1 % 50000 == 0:\n",
        "            torch.save(model.state_dict(), f\"./sft_model_checkpoint_{i}.pt\")\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "if use_existing_model:\n",
        "    print(\"Existing model used, no loss curves shown.\")\n",
        "    plt.imshow(plt.imread(\"./sft_loss_curve.png\"))\n",
        "else:\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(losses, label=\"Train Loss\", color='blue')\n",
        "    plt.plot(test_losses, label=\"Test Loss\", color='red')\n",
        "    plt.xlabel('Checkpoint')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Supervised Fine Tuning - Training and Test Loss Over Time')\n",
        "    plt.grid(True, linestyle='--', alpha=0.7)\n",
        "    plt.legend()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not use_existing_model:\n",
        "    torch.save(model, f\"./sft_final.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Inferencia final"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ahora sí, podemos realizar una forma más natural de inferencia. En lugar de formatear todos nuestros prompts como predicción del siguiente token, podemos usar un formato de preguntas y respuestas o Q&A.\n",
        "\n",
        "Estamos usando un modelo muy pequeño y un conjunto de datos muy reducido en comparación con los LLM modernos, por lo que nuestro modelo no va a desempeñarse muy bien en la práctica.\n",
        "\n",
        "Sin embargo, se generan respuestas que al menos están relacionadas con el prompt y formateadas de manera correcta.\n",
        "\n",
        "A medida que escalemos el modelo, los datos, etc., las respuestas se volverán más reales, precisas y contextualmente adecuadas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sft_inference(prompt,torch_model, max_new_tokens):\n",
        "    torch_model.eval()\n",
        "    prompt = \"<Question>\" + prompt + \"</Question>\" + \"<Answer>\" # Wrap the prompt in <Question> and start inference with <Answer>\n",
        "    with torch.no_grad(): \n",
        "        tokens = hf_tokenizer.encode(prompt) # Tokenize the prompt\n",
        "        for _ in range(max_new_tokens):\n",
        "            if tokens[-1] == hf_tokenizer.eos_token_id: # Stop if we reach the end of the sequence\n",
        "                break\n",
        "            num_tokens = len(tokens) # \n",
        "            tokens_padded = tokens + [hf_tokenizer.eos_token_id] * (config.seq_len - num_tokens) # pad the sequence with eos token\n",
        "            tokens_padded = torch.tensor(tokens_padded).unsqueeze(0).to(device) \n",
        "            logits = torch_model(tokens_padded) # Forward pass through the model\n",
        "            probabilities = torch.softmax(logits[0, num_tokens-1, :], dim=-1) # Get the probabilities of the last token\n",
        "            predicted_token = torch.argmax(probabilities).item() # Greedy decoding, change to sampling for more diversity\n",
        "            tokens.append(predicted_token)\n",
        "        \n",
        "        # Strip the text to between the <Answer></Answer> tags\n",
        "        full_answer = hf_tokenizer.decode(tokens)\n",
        "        answer_start = full_answer.find(\"<Answer>\") + len(\"<Answer>\")\n",
        "        answer_end = full_answer.find(\"</Answer>\")\n",
        "        return full_answer[answer_start:answer_end]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted: The President of the Republic of the Republic of the Congo\n",
            "Predicted: Yellow\n",
            "Predicted: Red\n",
            "Predicted: Two\n",
            "Predicted: United States and Canada\n",
            "Predicted: Two-four-four-four-four-four-four-four-four-four\n"
          ]
        }
      ],
      "source": [
        "print(\"Predicted:\", sft_inference(\"Who is the most powerful leader in the west?\", model, max_new_tokens=20))\n",
        "print(\"Predicted:\", sft_inference(\"What color is the sun?\", model, max_new_tokens=20))\n",
        "print(\"Predicted:\", sft_inference(\"What color is the ocean\", model, max_new_tokens=20))\n",
        "print(\"Predicted:\", sft_inference(\"How many planets are in the solar system\", model, max_new_tokens=20))\n",
        "print(\"Predicted:\", sft_inference(\"What three countries are in north america?\", model, max_new_tokens=20))\n",
        "print(\"Predicted:\", sft_inference(\"How many eyes do humans have?\", model, max_new_tokens=20))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
